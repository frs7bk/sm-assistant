
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¶Ø®Ù…Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Dask
Ù†Ø¸Ø§Ù… Ù…ØªÙ‚Ø¯Ù… Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ© ÙˆØ§Ù„Ù…ÙˆØ²Ø¹Ø©
"""

import dask.dataframe as dd
import dask.array as da
import pandas as pd
import numpy as np
from dask.distributed import Client, as_completed
from dask.diagnostics import ProgressBar
import logging
from typing import Dict, List, Any, Optional
import asyncio
from pathlib import Path

class AdvancedDaskProcessor:
    """Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¶Ø®Ù…Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Dask"""
    
    def __init__(self, scheduler_address: Optional[str] = None):
        """ØªÙ‡ÙŠØ¦Ø© Ù…Ø¹Ø§Ù„Ø¬ Dask"""
        self.logger = logging.getLogger(__name__)
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø¹Ù…ÙŠÙ„ Dask
        if scheduler_address:
            self.client = Client(scheduler_address)
        else:
            self.client = Client(processes=True, threads_per_worker=2)
        
        self.logger.info(f"ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù…Ø¹Ø§Ù„Ø¬ Dask: {self.client.dashboard_link}")
    
    async def process_large_dataset(self, data_path: str, chunk_size: str = "100MB") -> Dict[str, Any]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø¶Ø®Ù…Ø©"""
        try:
            # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø´ÙƒÙ„ ÙƒØ³ÙˆÙ„
            if data_path.endswith('.csv'):
                df = dd.read_csv(data_path, blocksize=chunk_size)
            elif data_path.endswith('.parquet'):
                df = dd.read_parquet(data_path)
            elif data_path.endswith('.json'):
                df = dd.read_json(data_path, blocksize=chunk_size)
            else:
                raise ValueError(f"ØªÙ†Ø³ÙŠÙ‚ Ù…Ù„Ù ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…: {data_path}")
            
            # ØªØ­Ù„ÙŠÙ„ Ø£Ø³Ø§Ø³ÙŠ
            with ProgressBar():
                analysis_results = {
                    'total_rows': len(df),
                    'total_columns': len(df.columns),
                    'memory_usage': df.memory_usage(deep=True).sum().compute(),
                    'data_types': dict(df.dtypes),
                    'null_counts': df.isnull().sum().compute().to_dict(),
                    'basic_stats': df.describe().compute().to_dict()
                }
            
            self.logger.info(f"ØªÙ… ØªØ­Ù„ÙŠÙ„ {analysis_results['total_rows']} ØµÙ")
            return analysis_results
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
            return {}
    
    async def advanced_analytics(self, df: dd.DataFrame) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø© Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        try:
            results = {}
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø·Ø§Øª
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) > 1:
                with ProgressBar():
                    correlation_matrix = df[numeric_cols].corr().compute()
                    results['correlations'] = correlation_matrix.to_dict()
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©
            missing_data = df.isnull().sum().compute()
            results['missing_data_analysis'] = {
                'total_missing': int(missing_data.sum()),
                'missing_by_column': missing_data.to_dict(),
                'missing_percentage': (missing_data / len(df) * 100).to_dict()
            }
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙˆØ²ÙŠØ¹Ø§Øª
            if len(numeric_cols) > 0:
                distributions = {}
                for col in numeric_cols[:5]:  # Ø£ÙˆÙ„ 5 Ø£Ø¹Ù…Ø¯Ø© Ø±Ù‚Ù…ÙŠØ©
                    with ProgressBar():
                        col_data = df[col].compute()
                        distributions[col] = {
                            'mean': float(col_data.mean()),
                            'std': float(col_data.std()),
                            'skewness': float(col_data.skew()),
                            'kurtosis': float(col_data.kurtosis())
                        }
                results['distributions'] = distributions
            
            return results
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©: {e}")
            return {}
    
    async def machine_learning_pipeline(self, df: dd.DataFrame, target_column: str) -> Dict[str, Any]:
        """Ø®Ø· Ø¥Ù†ØªØ§Ø¬ ØªØ¹Ù„Ù… Ø¢Ù„ÙŠ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¶Ø®Ù…Ø©"""
        try:
            from dask_ml.model_selection import train_test_split
            from dask_ml.linear_model import LinearRegression
            from dask_ml.ensemble import RandomForestRegressor
            from dask_ml.preprocessing import StandardScaler
            
            # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            if target_column in numeric_cols:
                numeric_cols.remove(target_column)
            
            X = df[numeric_cols]
            y = df[target_column]
            
            # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42
            )
            
            # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            # ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø§Ø°Ø¬ Ù…ØªØ¹Ø¯Ø¯Ø©
            models = {
                'linear_regression': LinearRegression(),
                'random_forest': RandomForestRegressor(n_estimators=100, random_state=42)
            }
            
            results = {}
            
            for model_name, model in models.items():
                with ProgressBar():
                    # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
                    model.fit(X_train_scaled, y_train)
                    
                    # Ø§Ù„ØªÙ†Ø¨Ø¤ ÙˆØ§Ù„ØªÙ‚ÙŠÙŠÙ…
                    train_score = model.score(X_train_scaled, y_train).compute()
                    test_score = model.score(X_test_scaled, y_test).compute()
                    
                    results[model_name] = {
                        'train_score': float(train_score),
                        'test_score': float(test_score)
                    }
            
            return results
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø®Ø· Ø§Ù„Ø¥Ù†ØªØ§Ø¬: {e}")
            return {}
    
    async def time_series_analysis(self, df: dd.DataFrame, time_column: str, value_column: str) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ù„Ø§Ø³Ù„ Ø§Ù„Ø²Ù…Ù†ÙŠØ©"""
        try:
            # ØªØ­ÙˆÙŠÙ„ Ø¹Ù…ÙˆØ¯ Ø§Ù„ÙˆÙ‚Øª
            df[time_column] = dd.to_datetime(df[time_column])
            df = df.set_index(time_column)
            
            # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ø³Ø¨ ÙØªØ±Ø§Øª Ø²Ù…Ù†ÙŠØ©
            daily_data = df[value_column].resample('D').mean().compute()
            weekly_data = df[value_column].resample('W').mean().compute()
            monthly_data = df[value_column].resample('M').mean().compute()
            
            # Ø§ØªØ¬Ø§Ù‡Ø§Øª ÙˆØ£Ù†Ù…Ø§Ø·
            results = {
                'trend_analysis': {
                    'daily_trend': float(daily_data.pct_change().mean()),
                    'weekly_trend': float(weekly_data.pct_change().mean()),
                    'monthly_trend': float(monthly_data.pct_change().mean())
                },
                'seasonality': {
                    'daily_volatility': float(daily_data.std()),
                    'weekly_volatility': float(weekly_data.std()),
                    'monthly_volatility': float(monthly_data.std())
                },
                'summary_stats': {
                    'total_data_points': len(daily_data),
                    'date_range': {
                        'start': str(daily_data.index.min()),
                        'end': str(daily_data.index.max())
                    }
                }
            }
            
            return results
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ù„Ø§Ø³Ù„ Ø§Ù„Ø²Ù…Ù†ÙŠØ©: {e}")
            return {}
    
    async def cluster_analysis(self, df: dd.DataFrame, n_clusters: int = 5) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ¬Ù…ÙŠØ¹ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¶Ø®Ù…Ø©"""
        try:
            from dask_ml.cluster import KMeans
            from dask_ml.preprocessing import StandardScaler
            
            # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø±Ù‚Ù…ÙŠØ©
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            X = df[numeric_cols]
            
            # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)
            
            # ØªØ·Ø¨ÙŠÙ‚ K-Means
            kmeans = KMeans(n_clusters=n_clusters, random_state=42)
            
            with ProgressBar():
                clusters = kmeans.fit_predict(X_scaled)
                cluster_centers = kmeans.cluster_centers_
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª
            unique_clusters, counts = da.unique(clusters, return_counts=True)
            cluster_counts = dict(zip(unique_clusters.compute(), counts.compute()))
            
            results = {
                'n_clusters': n_clusters,
                'cluster_counts': cluster_counts,
                'cluster_centers': cluster_centers.tolist(),
                'total_samples': len(clusters)
            }
            
            return results
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ¬Ù…ÙŠØ¹: {e}")
            return {}
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        try:
            return {
                'dashboard_link': self.client.dashboard_link,
                'workers': len(self.client.scheduler_info()['workers']),
                'total_cores': sum(w['ncores'] for w in self.client.scheduler_info()['workers'].values()),
                'total_memory': sum(w['memory_limit'] for w in self.client.scheduler_info()['workers'].values()),
                'active_tasks': len(self.client.processing()),
                'completed_tasks': self.client.call_stack(),
            }
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡: {e}")
            return {}
    
    def __del__(self):
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯"""
        if hasattr(self, 'client'):
            self.client.close()

# Ù…Ø«Ø§Ù„ Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
async def main():
    """Ù…Ø«Ø§Ù„ Ø´Ø§Ù…Ù„ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¶Ø®Ù…Ø©"""
    processor = AdvancedDaskProcessor()
    
    try:
        # Ù…Ø«Ø§Ù„ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ù CSV Ø¶Ø®Ù…
        print("ğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¶Ø®Ù…Ø©...")
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ© Ø¶Ø®Ù…Ø©
        import pandas as pd
        large_data = pd.DataFrame({
            'id': range(1000000),
            'value1': np.random.randn(1000000),
            'value2': np.random.randn(1000000) * 100,
            'category': np.random.choice(['A', 'B', 'C'], 1000000),
            'timestamp': pd.date_range('2020-01-01', periods=1000000, freq='1min')
        })
        large_data.to_csv('big_data_sample.csv', index=False)
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        analysis = await processor.process_large_dataset('big_data_sample.csv')
        print(f"ğŸ“Š Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {analysis}")
        
        # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ù„Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
        df = dd.read_csv('big_data_sample.csv')
        
        # ØªØ­Ù„ÙŠÙ„Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø©
        advanced_results = await processor.advanced_analytics(df)
        print(f"ğŸ”¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©: {advanced_results}")
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ù„Ø§Ø³Ù„ Ø§Ù„Ø²Ù…Ù†ÙŠØ©
        ts_results = await processor.time_series_analysis(df, 'timestamp', 'value1')
        print(f"ğŸ“ˆ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ù„Ø§Ø³Ù„ Ø§Ù„Ø²Ù…Ù†ÙŠØ©: {ts_results}")
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡
        perf_stats = processor.get_performance_stats()
        print(f"âš¡ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡: {perf_stats}")
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£: {e}")
    
    finally:
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ØªØ¬Ø±ÙŠØ¨ÙŠØ©
        Path('big_data_sample.csv').unlink(missing_ok=True)

if __name__ == "__main__":
    asyncio.run(main())
