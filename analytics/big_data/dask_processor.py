
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¶Ø®Ù…Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Dask
ÙŠÙˆÙØ± Ù‚Ø¯Ø±Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø© Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¶Ø®Ù…Ø©
"""

import asyncio
import logging
import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, Any, List, Optional, Union
import json
from datetime import datetime, timedelta
import warnings

# ØªØ¬Ø§Ù‡Ù„ Ø§Ù„ØªØ­Ø°ÙŠØ±Ø§Øª ØºÙŠØ± Ø§Ù„Ù…Ù‡Ù…Ø©
warnings.filterwarnings('ignore')

try:
    import dask
    import dask.dataframe as dd
    import dask.array as da
    from dask.distributed import Client, as_completed
    from dask import delayed
    DASK_AVAILABLE = True
except ImportError:
    DASK_AVAILABLE = False
    print("âš ï¸ Dask ØºÙŠØ± Ù…ØªØ§Ø­ - Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Pandas Ø§Ù„Ø¹Ø§Ø¯ÙŠ")

class DaskProcessor:
    """Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¶Ø®Ù…Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…"""
    
    def __init__(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬"""
        self.logger = logging.getLogger(__name__)
        self.client: Optional[Client] = None
        self.data_dir = Path("data")
        self.cache_dir = self.data_dir / "cache"
        self.results_dir = self.data_dir / "analysis_results"
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        self.processing_stats = {
            "total_processed": 0,
            "processing_time": 0.0,
            "memory_used": 0,
            "cache_hits": 0
        }
        
        # ØªÙƒÙˆÙŠÙ† Dask
        if DASK_AVAILABLE:
            dask.config.set({
                'dataframe.query-planning': True,
                'array.slicing.split_large_chunks': True
            })
    
    async def initialize(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø¹Ù…ÙŠÙ„ Dask"""
        try:
            if DASK_AVAILABLE:
                self.client = Client(processes=False, silence_logs=False)
                self.logger.info(f"ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ø¹Ù…ÙŠÙ„ Dask: {self.client.dashboard_link}")
            else:
                self.logger.warning("Dask ØºÙŠØ± Ù…ØªØ§Ø­ - Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªØ³Ù„Ø³Ù„ÙŠØ©")
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªÙ‡ÙŠØ¦Ø© Dask: {e}")
    
    async def cleanup(self):
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯"""
        try:
            if self.client:
                await self.client.close()
                self.logger.info("ØªÙ… Ø¥ØºÙ„Ø§Ù‚ Ø¹Ù…ÙŠÙ„ Dask")
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªÙ†Ø¸ÙŠÙ Dask: {e}")
    
    async def process_large_dataset(
        self, 
        file_path: Union[str, Path], 
        chunk_size: int = 10000,
        operations: List[str] = None
    ) -> Dict[str, Any]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ¨ÙŠØ±Ø©"""
        start_time = datetime.now()
        
        try:
            file_path = Path(file_path)
            
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…Ù„Ù
            if not file_path.exists():
                return {"error": f"Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {file_path}"}
            
            # ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù
            if file_path.suffix.lower() == '.csv':
                result = await self._process_csv_file(file_path, chunk_size, operations)
            elif file_path.suffix.lower() in ['.json', '.jsonl']:
                result = await self._process_json_file(file_path, operations)
            elif file_path.suffix.lower() in ['.xlsx', '.xls']:
                result = await self._process_excel_file(file_path, operations)
            else:
                return {"error": f"Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…: {file_path.suffix}"}
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…Ø³ØªØºØ±Ù‚
            processing_time = (datetime.now() - start_time).total_seconds()
            result["processing_time"] = processing_time
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
            self.processing_stats["total_processed"] += 1
            self.processing_stats["processing_time"] += processing_time
            
            return result
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
            return {"error": str(e)}
    
    async def _process_csv_file(
        self, 
        file_path: Path, 
        chunk_size: int,
        operations: List[str]
    ) -> Dict[str, Any]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ù CSV"""
        try:
            if DASK_AVAILABLE:
                # Ø§Ø³ØªØ®Ø¯Ø§Ù… Dask Ù„Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©
                df = dd.read_csv(str(file_path), blocksize=f"{chunk_size}KB")
                
                # Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
                result = {
                    "file_info": {
                        "path": str(file_path),
                        "size_mb": file_path.stat().st_size / (1024 * 1024),
                        "partitions": df.npartitions
                    },
                    "basic_stats": {
                        "total_rows": len(df),
                        "total_columns": len(df.columns),
                        "columns": list(df.columns)
                    }
                }
                
                # ØªÙ†ÙÙŠØ° Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
                if operations:
                    for operation in operations:
                        if operation == "describe":
                            result["description"] = df.describe().compute().to_dict()
                        elif operation == "null_counts":
                            result["null_counts"] = df.isnull().sum().compute().to_dict()
                        elif operation == "data_types":
                            result["data_types"] = df.dtypes.to_dict()
                        elif operation == "memory_usage":
                            result["memory_usage"] = df.memory_usage(deep=True).compute().to_dict()
                
            else:
                # Ø§Ø³ØªØ®Ø¯Ø§Ù… Pandas Ø§Ù„Ø¹Ø§Ø¯ÙŠ
                df = pd.read_csv(file_path, chunksize=chunk_size)
                
                # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ù„Ù‰ Ø¯ÙØ¹Ø§Øª
                total_rows = 0
                columns = None
                
                for chunk in df:
                    total_rows += len(chunk)
                    if columns is None:
                        columns = list(chunk.columns)
                
                result = {
                    "file_info": {
                        "path": str(file_path),
                        "size_mb": file_path.stat().st_size / (1024 * 1024)
                    },
                    "basic_stats": {
                        "total_rows": total_rows,
                        "total_columns": len(columns),
                        "columns": columns
                    }
                }
            
            return result
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© CSV: {e}")
            return {"error": str(e)}
    
    async def _process_json_file(self, file_path: Path, operations: List[str]) -> Dict[str, Any]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ù JSON"""
        try:
            # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù
            with open(file_path, 'r', encoding='utf-8') as f:
                if file_path.suffix.lower() == '.jsonl':
                    # JSON Lines
                    data = [json.loads(line) for line in f]
                else:
                    # JSON Ø¹Ø§Ø¯ÙŠ
                    data = json.load(f)
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            if isinstance(data, list):
                result = {
                    "file_info": {
                        "path": str(file_path),
                        "size_mb": file_path.stat().st_size / (1024 * 1024),
                        "type": "array"
                    },
                    "basic_stats": {
                        "total_items": len(data),
                        "sample_keys": list(data[0].keys()) if data and isinstance(data[0], dict) else None
                    }
                }
            else:
                result = {
                    "file_info": {
                        "path": str(file_path),
                        "size_mb": file_path.stat().st_size / (1024 * 1024),
                        "type": "object"
                    },
                    "basic_stats": {
                        "keys": list(data.keys()) if isinstance(data, dict) else None
                    }
                }
            
            return result
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© JSON: {e}")
            return {"error": str(e)}
    
    async def _process_excel_file(self, file_path: Path, operations: List[str]) -> Dict[str, Any]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ù Excel"""
        try:
            # Ù‚Ø±Ø§Ø¡Ø© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø£ÙˆØ±Ø§Ù‚
            excel_file = pd.ExcelFile(file_path)
            sheets = excel_file.sheet_names
            
            result = {
                "file_info": {
                    "path": str(file_path),
                    "size_mb": file_path.stat().st_size / (1024 * 1024),
                    "sheets": sheets
                },
                "sheets_data": {}
            }
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒÙ„ ÙˆØ±Ù‚Ø©
            for sheet in sheets[:5]:  # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£ÙˆÙ„ 5 Ø£ÙˆØ±Ø§Ù‚ ÙÙ‚Ø·
                df = pd.read_excel(file_path, sheet_name=sheet)
                
                result["sheets_data"][sheet] = {
                    "rows": len(df),
                    "columns": len(df.columns),
                    "column_names": list(df.columns)
                }
            
            return result
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Excel: {e}")
            return {"error": str(e)}
    
    async def analyze_sample_data(self) -> str:
        """ØªØ­Ù„ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø¹ÙŠÙ†Ø© Ù„Ø£ØºØ±Ø§Ø¶ Ø§Ù„ØªØ¬Ø±Ø¨Ø©"""
        try:
            # Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ø¹ÙŠÙ†Ø©
            sample_data = await self._create_sample_data()
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            analysis_results = await self._analyze_dataframe(sample_data)
            
            # Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ±
            report = self._generate_analysis_report(analysis_results)
            
            return report
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¹ÙŠÙ†Ø©: {e}")
            return f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {e}"
    
    async def _create_sample_data(self) -> pd.DataFrame:
        """Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ø¹ÙŠÙ†Ø© Ù„Ù„ØªØ¬Ø±Ø¨Ø©"""
        np.random.seed(42)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ù…ØªÙ†ÙˆØ¹Ø©
        n_samples = 10000
        
        data = {
            'user_id': range(1, n_samples + 1),
            'age': np.random.randint(18, 80, n_samples),
            'income': np.random.normal(50000, 15000, n_samples),
            'education_level': np.random.choice(['Ø«Ø§Ù†ÙˆÙŠ', 'Ø¬Ø§Ù…Ø¹ÙŠ', 'Ø¯Ø±Ø§Ø³Ø§Øª Ø¹Ù„ÙŠØ§'], n_samples),
            'city': np.random.choice(['Ø§Ù„Ø±ÙŠØ§Ø¶', 'Ø¬Ø¯Ø©', 'Ø§Ù„Ø¯Ù…Ø§Ù…', 'Ù…ÙƒØ©', 'Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©'], n_samples),
            'satisfaction_score': np.random.uniform(1, 10, n_samples),
            'purchase_frequency': np.random.poisson(5, n_samples),
            'registration_date': pd.date_range('2020-01-01', periods=n_samples, freq='H')
        }
        
        return pd.DataFrame(data)
    
    async def _analyze_dataframe(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ù…ÙØµÙ„ Ù„Ø¥Ø·Ø§Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        analysis = {}
        
        # Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        analysis['basic_info'] = {
            'shape': df.shape,
            'memory_usage': df.memory_usage(deep=True).sum(),
            'dtypes': df.dtypes.to_dict()
        }
        
        # Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ÙˆØµÙÙŠØ©
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        if len(numeric_columns) > 0:
            analysis['descriptive_stats'] = df[numeric_columns].describe().to_dict()
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©
        analysis['missing_values'] = df.isnull().sum().to_dict()
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙØ¦ÙˆÙŠØ©
        categorical_columns = df.select_dtypes(include=['object']).columns
        analysis['categorical_analysis'] = {}
        
        for col in categorical_columns:
            analysis['categorical_analysis'][col] = {
                'unique_values': df[col].nunique(),
                'most_frequent': df[col].mode().iloc[0] if not df[col].empty else None,
                'value_counts': df[col].value_counts().head().to_dict()
            }
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙˆØ²ÙŠØ¹Ø§Øª
        analysis['distributions'] = {}
        for col in numeric_columns:
            analysis['distributions'][col] = {
                'mean': float(df[col].mean()),
                'median': float(df[col].median()),
                'std': float(df[col].std()),
                'skewness': float(df[col].skew()),
                'kurtosis': float(df[col].kurtosis())
            }
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø·Ø§Øª
        if len(numeric_columns) > 1:
            correlation_matrix = df[numeric_columns].corr()
            analysis['correlations'] = correlation_matrix.to_dict()
        
        return analysis
    
    def _generate_analysis_report(self, analysis: Dict[str, Any]) -> str:
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„"""
        report = f"""
ğŸ“Š ØªÙ‚Ø±ÙŠØ± ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¶Ø®Ù…Ø©
{'='*50}

ğŸ“ˆ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø£Ø³Ø§Ø³ÙŠØ©:
   â€¢ Ø¹Ø¯Ø¯ Ø§Ù„ØµÙÙˆÙ: {analysis['basic_info']['shape'][0]:,}
   â€¢ Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©: {analysis['basic_info']['shape'][1]}
   â€¢ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø©: {analysis['basic_info']['memory_usage'] / 1024 / 1024:.2f} Ù…ÙŠØ¬Ø§Ø¨Ø§ÙŠØª

ğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©:
"""
        
        missing_values = analysis.get('missing_values', {})
        for col, missing_count in missing_values.items():
            if missing_count > 0:
                report += f"   â€¢ {col}: {missing_count} Ù‚ÙŠÙ…Ø© Ù…ÙÙ‚ÙˆØ¯Ø©\n"
        
        if not any(missing_values.values()):
            report += "   âœ… Ù„Ø§ ØªÙˆØ¬Ø¯ Ù‚ÙŠÙ… Ù…ÙÙ‚ÙˆØ¯Ø©\n"
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø±Ù‚Ù…ÙŠØ©
        if 'descriptive_stats' in analysis:
            report += f"\nğŸ“Š Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ÙˆØµÙÙŠØ© Ù„Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø±Ù‚Ù…ÙŠØ©:\n"
            for col, stats in analysis['descriptive_stats'].items():
                report += f"""   â€¢ {col}:
     - Ø§Ù„Ù…ØªÙˆØ³Ø·: {stats['mean']:.2f}
     - Ø§Ù„ÙˆØ³ÙŠØ·: {stats['50%']:.2f}
     - Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ: {stats['std']:.2f}
"""
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙØ¦ÙˆÙŠØ©
        if 'categorical_analysis' in analysis:
            report += f"\nğŸ“‹ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙØ¦ÙˆÙŠØ©:\n"
            for col, cat_stats in analysis['categorical_analysis'].items():
                report += f"""   â€¢ {col}:
     - Ù‚ÙŠÙ… ÙØ±ÙŠØ¯Ø©: {cat_stats['unique_values']}
     - Ø§Ù„Ø£ÙƒØ«Ø± ØªÙƒØ±Ø§Ø±Ø§Ù‹: {cat_stats['most_frequent']}
"""
        
        # Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø·Ø§Øª
        if 'correlations' in analysis:
            report += f"\nğŸ”— Ø£Ù‚ÙˆÙ‰ Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø·Ø§Øª:\n"
            correlations = analysis['correlations']
            
            # Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£Ù‚ÙˆÙ‰ Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø·Ø§Øª
            strong_correlations = []
            for col1 in correlations:
                for col2 in correlations[col1]:
                    if col1 != col2:
                        corr_value = correlations[col1][col2]
                        if abs(corr_value) > 0.5:
                            strong_correlations.append((col1, col2, corr_value))
            
            for col1, col2, corr in sorted(strong_correlations, key=lambda x: abs(x[2]), reverse=True)[:5]:
                report += f"   â€¢ {col1} â†” {col2}: {corr:.3f}\n"
        
        # Ø§Ù„Ø®Ù„Ø§ØµØ© ÙˆØ§Ù„ØªÙˆØµÙŠØ§Øª
        report += f"""
ğŸ’¡ Ø®Ù„Ø§ØµØ© Ø§Ù„ØªØ­Ù„ÙŠÙ„:
   â€¢ Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {'Ù…Ù…ØªØ§Ø²Ø©' if not any(missing_values.values()) else 'Ø¬ÙŠØ¯Ø© Ù…Ø¹ Ø¨Ø¹Ø¶ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©'}
   â€¢ ØªÙ†ÙˆØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {'Ù…Ø±ØªÙØ¹' if len(analysis.get('categorical_analysis', {})) > 2 else 'Ù…ØªÙˆØ³Ø·'}
   â€¢ Ø¥Ù…ÙƒØ§Ù†ÙŠØ© Ø§Ù„ØªØ­Ù„ÙŠÙ„: Ø¹Ø§Ù„ÙŠØ© âœ…

ğŸ“Œ Ø§Ù„ØªÙˆØµÙŠØ§Øª:
   â€¢ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‡Ø°Ù‡ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ø¨Ù†Ø§Ø¡ Ù†Ù…Ø§Ø°Ø¬ ØªÙ†Ø¨Ø¤ÙŠØ©
   â€¢ Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø·Ø§Øª Ø§Ù„Ù‚ÙˆÙŠØ© ØªØ´ÙŠØ± Ø¥Ù„Ù‰ Ø¥Ù…ÙƒØ§Ù†ÙŠØ§Øª ØªØ­Ù„ÙŠÙ„ Ù…ØªÙ‚Ø¯Ù…Ø©
   â€¢ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¬Ø§Ù‡Ø²Ø© Ù„Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ© Ø§Ù„Ù…ØªØ·ÙˆØ±Ø©
"""
        
        return report
    
    async def process_user_behavior_data(self, user_data: Dict[str, Any]) -> Dict[str, Any]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø³Ù„ÙˆÙƒ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
        try:
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ DataFrame
            df = pd.DataFrame([user_data])
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            analysis = {
                "user_profile": {
                    "interactions": user_data.get("interactions", 0),
                    "session_duration": user_data.get("session_duration", 0),
                    "preferences": user_data.get("preferences", {})
                },
                "behavior_patterns": await self._analyze_behavior_patterns(user_data),
                "recommendations": await self._generate_recommendations(user_data)
            }
            
            return analysis
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {e}")
            return {"error": str(e)}
    
    async def _analyze_behavior_patterns(self, user_data: Dict[str, Any]) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø³Ù„ÙˆÙƒ"""
        patterns = {}
        
        # ØªØ­Ù„ÙŠÙ„ Ù†Ø´Ø§Ø· Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
        interactions = user_data.get("interactions", 0)
        if interactions > 0:
            patterns["activity_level"] = (
                "Ø¹Ø§Ù„ÙŠ" if interactions > 100 else
                "Ù…ØªÙˆØ³Ø·" if interactions > 50 else
                "Ù…Ù†Ø®ÙØ¶"
            )
        
        # ØªØ­Ù„ÙŠÙ„ Ø£ÙˆÙ‚Ø§Øª Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
        current_hour = datetime.now().hour
        if 6 <= current_hour < 12:
            patterns["usage_time"] = "ØµØ¨Ø§Ø­ÙŠ"
        elif 12 <= current_hour < 18:
            patterns["usage_time"] = "Ø¨Ø¹Ø¯ Ø§Ù„Ø¸Ù‡Ø±"
        elif 18 <= current_hour < 24:
            patterns["usage_time"] = "Ù…Ø³Ø§Ø¦ÙŠ"
        else:
            patterns["usage_time"] = "Ù„ÙŠÙ„ÙŠ"
        
        return patterns
    
    async def _generate_recommendations(self, user_data: Dict[str, Any]) -> List[str]:
        """ØªÙˆÙ„ÙŠØ¯ ØªÙˆØµÙŠØ§Øª Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
        recommendations = []
        
        interactions = user_data.get("interactions", 0)
        
        if interactions < 10:
            recommendations.append("Ø¬Ø±Ø¨ Ø§Ø³ØªÙƒØ´Ø§Ù Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©")
            recommendations.append("Ø§Ø·Ù„Ø¹ Ø¹Ù„Ù‰ Ø¯Ù„ÙŠÙ„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù„Ù„Ø§Ø³ØªÙØ§Ø¯Ø© Ø§Ù„Ù‚ØµÙˆÙ‰")
        
        if interactions > 50:
            recommendations.append("ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¢Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©")
            recommendations.append("Ø¬Ø±Ø¨ Ø£Ù†Ø¸Ù…Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ ÙˆØ§Ù„ØªÙˆÙ‚Ø¹")
        
        # Ø¥Ø¶Ø§ÙØ© ØªÙˆØµÙŠØ§Øª Ø¹Ø§Ù…Ø©
        recommendations.append("Ø§Ø­Ø±Øµ Ø¹Ù„Ù‰ ØªØ­Ø¯ÙŠØ« ØªÙØ¶ÙŠÙ„Ø§ØªÙƒ Ø¨Ø§Ù†ØªØ¸Ø§Ù…")
        recommendations.append("Ø§Ø³ØªØ®Ø¯Ù… Ù…ÙŠØ²Ø© Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù†Ø´Ø· Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡")
        
        return recommendations

# Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬
async def test_dask_processor():
    """Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¶Ø®Ù…Ø©"""
    processor = DaskProcessor()
    
    try:
        await processor.initialize()
        
        # Ø§Ø®ØªØ¨Ø§Ø± ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¹ÙŠÙ†Ø©
        result = await processor.analyze_sample_data()
        print(result)
        
    finally:
        await processor.cleanup()

if __name__ == "__main__":
    asyncio.run(test_dask_processor())
