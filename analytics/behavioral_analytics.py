
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ù…Ø­Ø±Ùƒ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ù„ÙˆÙƒ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
Advanced Behavioral Analytics Engine
"""

import asyncio
import logging
import numpy as np
import pandas as pd
import sqlite3
import json
from typing import Dict, List, Any, Optional, Tuple, Union
from datetime import datetime, timedelta
from pathlib import Path
from dataclasses import dataclass, asdict, field
from collections import defaultdict, deque
import threading
import queue
import hashlib
import warnings
warnings.filterwarnings('ignore')

# Machine Learning
try:
    from sklearn.cluster import DBSCAN, KMeans
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn.ensemble import IsolationForest
    from sklearn.decomposition import PCA
    from sklearn.metrics import silhouette_score
    from sklearn.model_selection import train_test_split
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False

# Deep Learning
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch.utils.data import DataLoader, TensorDataset
    PYTORCH_AVAILABLE = True
except ImportError:
    PYTORCH_AVAILABLE = False

# Visualization
try:
    import plotly.graph_objects as go
    import plotly.express as px
    from plotly.subplots import make_subplots
    import plotly.io as pio
    PLOTLY_AVAILABLE = True
except ImportError:
    PLOTLY_AVAILABLE = False

@dataclass
class BehaviorEvent:
    """Ø­Ø¯Ø« Ø³Ù„ÙˆÙƒÙŠ"""
    event_id: str
    user_id: str
    event_type: str
    event_data: Dict[str, Any]
    timestamp: datetime
    session_id: str = ""
    context: Dict[str, Any] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class BehaviorPattern:
    """Ù†Ù…Ø· Ø³Ù„ÙˆÙƒÙŠ"""
    pattern_id: str
    user_id: str
    pattern_type: str
    pattern_description: str
    frequency: float
    confidence: float
    start_time: datetime
    end_time: Optional[datetime] = None
    triggers: List[str] = field(default_factory=list)
    outcomes: List[str] = field(default_factory=list)
    context_factors: Dict[str, Any] = field(default_factory=dict)
    statistical_significance: float = 0.0

@dataclass
class UserBehaviorProfile:
    """Ù…Ù„Ù ØªØ¹Ø±ÙŠÙ Ø§Ù„Ø³Ù„ÙˆÙƒ Ø§Ù„Ø´Ø®ØµÙŠ"""
    user_id: str
    active_patterns: List[BehaviorPattern]
    behavior_trends: Dict[str, float]
    preferences: Dict[str, Any]
    interaction_style: str
    activity_periods: List[Tuple[int, int]]  # (start_hour, end_hour)
    engagement_score: float
    consistency_score: float
    adaptability_score: float
    last_updated: datetime = field(default_factory=datetime.now)

@dataclass
class BehaviorInsight:
    """Ø±Ø¤ÙŠØ© Ø³Ù„ÙˆÙƒÙŠØ©"""
    insight_id: str
    user_id: str
    insight_type: str
    description: str
    confidence: float
    impact_score: float
    recommendations: List[str]
    supporting_data: Dict[str, Any]
    timestamp: datetime = field(default_factory=datetime.now)

class BehaviorEmbeddingNetwork(nn.Module):
    """Ø´Ø¨ÙƒØ© ØªÙ…Ø«ÙŠÙ„ Ø§Ù„Ø³Ù„ÙˆÙƒ Ø§Ù„Ø¹ØµØ¨ÙŠØ©"""
    
    def __init__(self, input_dim: int = 128, embedding_dim: int = 64):
        super().__init__()
        self.input_dim = input_dim
        self.embedding_dim = embedding_dim
        
        # Ø·Ø¨Ù‚Ø§Øª Ø§Ù„ØªØ´ÙÙŠØ±
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.ReLU(),
            nn.BatchNorm1d(256),
            nn.Dropout(0.3),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.2),
            nn.Linear(128, embedding_dim),
            nn.Tanh()
        )
        
        # Ø·Ø¨Ù‚Ø§Øª ÙÙƒ Ø§Ù„ØªØ´ÙÙŠØ±
        self.decoder = nn.Sequential(
            nn.Linear(embedding_dim, 128),
            nn.ReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.2),
            nn.Linear(128, 256),
            nn.ReLU(),
            nn.BatchNorm1d(256),
            nn.Dropout(0.3),
            nn.Linear(256, input_dim),
            nn.Sigmoid()
        )
        
        # Ø·Ø¨Ù‚Ø© Ø§Ù„ØªØµÙ†ÙŠÙ
        self.classifier = nn.Sequential(
            nn.Linear(embedding_dim, 32),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(32, 16),
            nn.ReLU(),
            nn.Linear(16, 8),  # Ø¹Ø¯Ø¯ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø³Ù„ÙˆÙƒ
            nn.Softmax(dim=1)
        )
    
    def forward(self, x):
        # ØªØ´ÙÙŠØ±
        embedded = self.encoder(x)
        
        # ÙÙƒ Ø§Ù„ØªØ´ÙÙŠØ±
        reconstructed = self.decoder(embedded)
        
        # ØªØµÙ†ÙŠÙ
        classification = self.classifier(embedded)
        
        return embedded, reconstructed, classification

class AdvancedBehavioralAnalytics:
    """Ù…Ø­Ø±Ùƒ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ù„ÙˆÙƒ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
        # Ø§Ù„ØªÙƒÙˆÙŠÙ† Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
        self.is_initialized = False
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        self.db_path = Path("data/behavioral_analytics.db")
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        
        # Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆØ§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø§Øª
        self.embedding_network = None
        self.clustering_model = None
        self.anomaly_detector = None
        self.scaler = StandardScaler() if SKLEARN_AVAILABLE else None
        
        # Ù…Ø®Ø§Ø²Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        self.behavior_events = deque(maxlen=50000)
        self.user_profiles: Dict[str, UserBehaviorProfile] = {}
        self.active_patterns: Dict[str, List[BehaviorPattern]] = defaultdict(list)
        self.behavior_insights: Dict[str, List[BehaviorInsight]] = defaultdict(list)
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ø¸Ø§Ù…
        self.analytics_stats = {
            "total_events_processed": 0,
            "patterns_discovered": 0,
            "insights_generated": 0,
            "users_analyzed": 0,
            "model_accuracy": 0.0,
            "processing_time_avg": 0.0
        }
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø®Ù„ÙÙŠØ©
        self.processing_queue = queue.Queue()
        self.background_workers = []
        self.is_processing = False
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„
        self.analysis_config = {
            "min_pattern_frequency": 3,
            "pattern_confidence_threshold": 0.7,
            "anomaly_threshold": 0.1,
            "clustering_eps": 0.5,
            "clustering_min_samples": 5,
            "insight_confidence_threshold": 0.8
        }
    
    async def initialize(self):
        """ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ø±Ùƒ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ù„ÙˆÙƒÙŠ"""
        self.logger.info("ğŸ§  ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ø±Ùƒ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ù„ÙˆÙƒ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…...")
        
        try:
            # Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            await self._initialize_database()
            
            # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
            await self._load_models()
            
            # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¹Ù…Ø§Ù„ Ø§Ù„Ø®Ù„ÙÙŠÙŠÙ†
            self._start_background_workers()
            
            # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ©
            await self._load_historical_data()
            
            self.is_initialized = True
            self.logger.info("âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ø±Ùƒ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ù„ÙˆÙƒ Ø¨Ù†Ø¬Ø§Ø­")
            
        except Exception as e:
            self.logger.error(f"âŒ ÙØ´Ù„ ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ø±Ùƒ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ù„ÙˆÙƒ: {e}")
            # ØªÙ‡ÙŠØ¦Ø© Ø£Ø³Ø§Ø³ÙŠØ©
            self.is_initialized = True
    
    async def _initialize_database(self):
        """ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø£Ø­Ø¯Ø§Ø« Ø§Ù„Ø³Ù„ÙˆÙƒÙŠØ©
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS behavior_events (
                    event_id TEXT PRIMARY KEY,
                    user_id TEXT NOT NULL,
                    event_type TEXT NOT NULL,
                    event_data TEXT,
                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                    session_id TEXT,
                    context TEXT,
                    metadata TEXT,
                    INDEX(user_id),
                    INDEX(timestamp),
                    INDEX(event_type)
                )
            ''')
            
            # Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø³Ù„ÙˆÙƒÙŠØ©
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS behavior_patterns (
                    pattern_id TEXT PRIMARY KEY,
                    user_id TEXT NOT NULL,
                    pattern_type TEXT NOT NULL,
                    pattern_description TEXT,
                    frequency REAL,
                    confidence REAL,
                    start_time DATETIME,
                    end_time DATETIME,
                    triggers TEXT,
                    outcomes TEXT,
                    context_factors TEXT,
                    statistical_significance REAL,
                    INDEX(user_id),
                    INDEX(pattern_type)
                )
            ''')
            
            # Ø¬Ø¯ÙˆÙ„ Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS user_behavior_profiles (
                    user_id TEXT PRIMARY KEY,
                    active_patterns TEXT,
                    behavior_trends TEXT,
                    preferences TEXT,
                    interaction_style TEXT,
                    activity_periods TEXT,
                    engagement_score REAL,
                    consistency_score REAL,
                    adaptability_score REAL,
                    last_updated DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø±Ø¤Ù‰ Ø§Ù„Ø³Ù„ÙˆÙƒÙŠØ©
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS behavior_insights (
                    insight_id TEXT PRIMARY KEY,
                    user_id TEXT NOT NULL,
                    insight_type TEXT NOT NULL,
                    description TEXT,
                    confidence REAL,
                    impact_score REAL,
                    recommendations TEXT,
                    supporting_data TEXT,
                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                    INDEX(user_id),
                    INDEX(insight_type)
                )
            ''')
            
            conn.commit()
            conn.close()
            self.logger.info("âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø³Ù„ÙˆÙƒÙŠØ©")
            
        except Exception as e:
            self.logger.error(f"âŒ ÙØ´Ù„ ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
    
    async def _load_models(self):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬"""
        try:
            if PYTORCH_AVAILABLE:
                # ØªØ­Ù…ÙŠÙ„ Ø´Ø¨ÙƒØ© Ø§Ù„ØªÙ…Ø«ÙŠÙ„ Ø§Ù„Ø³Ù„ÙˆÙƒÙŠ
                self.embedding_network = BehaviorEmbeddingNetwork()
                self.embedding_network.to(self.device)
                
                # ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø­ÙÙˆØ¸ Ø¥Ù† ÙˆØ¬Ø¯
                model_path = Path("data/models/behavior_embedding.pth")
                if model_path.exists():
                    self.embedding_network.load_state_dict(
                        torch.load(model_path, map_location=self.device)
                    )
                    self.logger.info("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªÙ…Ø«ÙŠÙ„ Ø§Ù„Ø³Ù„ÙˆÙƒÙŠ Ø§Ù„Ù…Ø­ÙÙˆØ¸")
                else:
                    self.logger.info("ğŸ§  ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ØªÙ…Ø«ÙŠÙ„ Ø³Ù„ÙˆÙƒÙŠ Ø¬Ø¯ÙŠØ¯")
            
            if SKLEARN_AVAILABLE:
                # Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¬Ù…ÙŠØ¹
                self.clustering_model = DBSCAN(
                    eps=self.analysis_config["clustering_eps"],
                    min_samples=self.analysis_config["clustering_min_samples"]
                )
                
                # Ù†Ù…ÙˆØ°Ø¬ ÙƒØ´Ù Ø§Ù„Ø´Ø§Ø°Ø§Øª
                self.anomaly_detector = IsolationForest(
                    contamination=self.analysis_config["anomaly_threshold"],
                    random_state=42
                )
                
                self.logger.info("âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬: {e}")
    
    def _start_background_workers(self):
        """ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¹Ù…Ø§Ù„ Ø§Ù„Ø®Ù„ÙÙŠÙŠÙ†"""
        self.is_processing = True
        num_workers = 3
        
        for i in range(num_workers):
            worker = threading.Thread(
                target=self._background_processor,
                name=f"BehaviorWorker-{i}",
                daemon=True
            )
            worker.start()
            self.background_workers.append(worker)
        
        self.logger.info(f"ğŸ”„ ØªÙ… ØªØ´ØºÙŠÙ„ {num_workers} Ø¹Ø§Ù…Ù„ ØªØ­Ù„ÙŠÙ„ Ø³Ù„ÙˆÙƒÙŠ")
    
    def _background_processor(self):
        """Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø®Ù„ÙÙŠØ©"""
        while self.is_processing:
            try:
                task = self.processing_queue.get(timeout=1)
                asyncio.create_task(self._process_task(task))
                self.processing_queue.task_done()
                
            except queue.Empty:
                continue
            except Exception as e:
                self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø®Ù„ÙÙŠØ©: {e}")
    
    async def _process_task(self, task: Dict[str, Any]):
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù‡Ù…Ø©"""
        try:
            task_type = task.get("type")
            
            if task_type == "analyze_patterns":
                await self._analyze_behavior_patterns(task.get("user_id"))
            elif task_type == "generate_insights":
                await self._generate_behavior_insights(task.get("user_id"))
            elif task_type == "update_profile":
                await self._update_user_profile(task.get("user_id"))
            elif task_type == "detect_anomalies":
                await self._detect_behavior_anomalies(task.get("user_id"))
                
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù‡Ù…Ø©: {e}")
    
    async def track_behavior_event(self, event: BehaviorEvent) -> str:
        """ØªØªØ¨Ø¹ Ø­Ø¯Ø« Ø³Ù„ÙˆÙƒÙŠ"""
        try:
            # Ø¥Ø¶Ø§ÙØ© Ù„Ù„Ù…Ø®Ø²Ù† Ø§Ù„Ù…Ø­Ù„ÙŠ
            self.behavior_events.append(event)
            
            # Ø­ÙØ¸ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            await self._save_behavior_event(event)
            
            # Ø¥Ø¶Ø§ÙØ© Ù…Ù‡Ø§Ù… Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ù„Ù„Ø·Ø§Ø¨ÙˆØ±
            self.processing_queue.put({
                "type": "analyze_patterns",
                "user_id": event.user_id
            })
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
            self.analytics_stats["total_events_processed"] += 1
            
            self.logger.debug(f"ØªÙ… ØªØªØ¨Ø¹ Ø­Ø¯Ø« Ø³Ù„ÙˆÙƒÙŠ: {event.event_id}")
            return event.event_id
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØªØ¨Ø¹ Ø§Ù„Ø­Ø¯Ø« Ø§Ù„Ø³Ù„ÙˆÙƒÙŠ: {e}")
            raise
    
    async def _save_behavior_event(self, event: BehaviorEvent):
        """Ø­ÙØ¸ Ø­Ø¯Ø« Ø³Ù„ÙˆÙƒÙŠ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT OR REPLACE INTO behavior_events
                (event_id, user_id, event_type, event_data, timestamp, 
                 session_id, context, metadata)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                event.event_id,
                event.user_id,
                event.event_type,
                json.dumps(event.event_data, ensure_ascii=False),
                event.timestamp.isoformat(),
                event.session_id,
                json.dumps(event.context, ensure_ascii=False),
                json.dumps(event.metadata, ensure_ascii=False)
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø§Ù„Ø­Ø¯Ø« Ø§Ù„Ø³Ù„ÙˆÙƒÙŠ: {e}")
    
    async def _analyze_behavior_patterns(self, user_id: str):
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø³Ù„ÙˆÙƒÙŠØ© Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
        try:
            # Ø¬Ù…Ø¹ Ø£Ø­Ø¯Ø§Ø« Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
            user_events = [
                event for event in self.behavior_events 
                if event.user_id == user_id
            ]
            
            if len(user_events) < self.analysis_config["min_pattern_frequency"]:
                return
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø²Ù…Ù†ÙŠØ©
            temporal_patterns = await self._detect_temporal_patterns(user_events)
            
            # ØªØ­Ù„ÙŠÙ„ Ø£Ù†Ù…Ø§Ø· Ø§Ù„ØªØ³Ù„Ø³Ù„
            sequence_patterns = await self._detect_sequence_patterns(user_events)
            
            # ØªØ­Ù„ÙŠÙ„ Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø³ÙŠØ§Ù‚
            context_patterns = await self._detect_context_patterns(user_events)
            
            # Ø¯Ù…Ø¬ Ø§Ù„Ø£Ù†Ù…Ø§Ø·
            all_patterns = temporal_patterns + sequence_patterns + context_patterns
            
            # ØªÙ‚ÙŠÙŠÙ… ÙˆØªØµÙÙŠØ© Ø§Ù„Ø£Ù†Ù…Ø§Ø·
            validated_patterns = [
                pattern for pattern in all_patterns
                if pattern.confidence >= self.analysis_config["pattern_confidence_threshold"]
            ]
            
            # Ø­ÙØ¸ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
            for pattern in validated_patterns:
                await self._save_behavior_pattern(pattern)
                self.active_patterns[user_id].append(pattern)
            
            # ØªØ­Ø¯ÙŠØ« Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
            self.analytics_stats["patterns_discovered"] += len(validated_patterns)
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø³Ù„ÙˆÙƒÙŠØ©: {e}")
    
    async def _detect_temporal_patterns(self, events: List[BehaviorEvent]) -> List[BehaviorPattern]:
        """ÙƒØ´Ù Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø²Ù…Ù†ÙŠØ©"""
        patterns = []
        
        try:
            if not events:
                return patterns
            
            # ØªØ­Ù„ÙŠÙ„ Ø£ÙˆÙ‚Ø§Øª Ø§Ù„Ù†Ø´Ø§Ø·
            activity_hours = defaultdict(int)
            for event in events:
                hour = event.timestamp.hour
                activity_hours[hour] += 1
            
            # Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø³Ø§Ø¹Ø§Øª Ø§Ù„Ø°Ø±ÙˆØ©
            total_events = len(events)
            peak_hours = []
            
            for hour, count in activity_hours.items():
                frequency = count / total_events
                if frequency > 0.1:  # Ø£ÙƒØ«Ø± Ù…Ù† 10% Ù…Ù† Ø§Ù„Ù†Ø´Ø§Ø·
                    peak_hours.append(hour)
            
            if peak_hours:
                pattern_id = hashlib.md5(f"temporal_{events[0].user_id}_{'-'.join(map(str, peak_hours))}".encode()).hexdigest()[:12]
                
                pattern = BehaviorPattern(
                    pattern_id=pattern_id,
                    user_id=events[0].user_id,
                    pattern_type="temporal_activity",
                    pattern_description=f"Ù†Ø´Ø§Ø· Ù…ÙƒØ«Ù Ø®Ù„Ø§Ù„ Ø§Ù„Ø³Ø§Ø¹Ø§Øª: {', '.join(map(str, peak_hours))}",
                    frequency=len(peak_hours) / 24,
                    confidence=0.8,
                    start_time=min(event.timestamp for event in events),
                    triggers=["time_based"],
                    context_factors={"peak_hours": peak_hours}
                )
                patterns.append(pattern)
            
            # ØªØ­Ù„ÙŠÙ„ Ø£Ù†Ù…Ø§Ø· Ø£ÙŠØ§Ù… Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹
            weekday_activity = defaultdict(int)
            for event in events:
                weekday = event.timestamp.weekday()
                weekday_activity[weekday] += 1
            
            active_weekdays = []
            for weekday, count in weekday_activity.items():
                frequency = count / total_events
                if frequency > 0.15:  # Ø£ÙƒØ«Ø± Ù…Ù† 15% Ù…Ù† Ø§Ù„Ù†Ø´Ø§Ø·
                    active_weekdays.append(weekday)
            
            if active_weekdays:
                pattern_id = hashlib.md5(f"weekly_{events[0].user_id}_{'-'.join(map(str, active_weekdays))}".encode()).hexdigest()[:12]
                
                weekday_names = ["Ø§Ù„Ø§Ø«Ù†ÙŠÙ†", "Ø§Ù„Ø«Ù„Ø§Ø«Ø§Ø¡", "Ø§Ù„Ø£Ø±Ø¨Ø¹Ø§Ø¡", "Ø§Ù„Ø®Ù…ÙŠØ³", "Ø§Ù„Ø¬Ù…Ø¹Ø©", "Ø§Ù„Ø³Ø¨Øª", "Ø§Ù„Ø£Ø­Ø¯"]
                active_day_names = [weekday_names[day] for day in active_weekdays]
                
                pattern = BehaviorPattern(
                    pattern_id=pattern_id,
                    user_id=events[0].user_id,
                    pattern_type="weekly_activity",
                    pattern_description=f"Ù†Ø´Ø§Ø· Ù…Ù†ØªØ¸Ù… ÙÙŠ Ø£ÙŠØ§Ù…: {', '.join(active_day_names)}",
                    frequency=len(active_weekdays) / 7,
                    confidence=0.75,
                    start_time=min(event.timestamp for event in events),
                    triggers=["weekly_pattern"],
                    context_factors={"active_weekdays": active_weekdays}
                )
                patterns.append(pattern)
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ÙƒØ´Ù Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø²Ù…Ù†ÙŠØ©: {e}")
        
        return patterns
    
    async def _detect_sequence_patterns(self, events: List[BehaviorEvent]) -> List[BehaviorPattern]:
        """ÙƒØ´Ù Ø£Ù†Ù…Ø§Ø· Ø§Ù„ØªØ³Ù„Ø³Ù„"""
        patterns = []
        
        try:
            if len(events) < 3:
                return patterns
            
            # ØªØ±ØªÙŠØ¨ Ø§Ù„Ø£Ø­Ø¯Ø§Ø« Ø­Ø³Ø¨ Ø§Ù„ÙˆÙ‚Øª
            sorted_events = sorted(events, key=lambda e: e.timestamp)
            
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ØªØ³Ù„Ø³Ù„Ø§Øª Ù…ØªÙƒØ±Ø±Ø©
            sequence_counts = defaultdict(int)
            
            for i in range(len(sorted_events) - 2):
                sequence = tuple(
                    event.event_type for event in sorted_events[i:i+3]
                )
                sequence_counts[sequence] += 1
            
            # Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„ØªØ³Ù„Ø³Ù„Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø©
            total_sequences = len(sorted_events) - 2
            
            for sequence, count in sequence_counts.items():
                frequency = count / total_sequences
                
                if frequency >= 0.1 and count >= 3:  # ØªÙƒØ±Ø± 3 Ù…Ø±Ø§Øª Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„
                    pattern_id = hashlib.md5(f"sequence_{events[0].user_id}_{'-'.join(sequence)}".encode()).hexdigest()[:12]
                    
                    pattern = BehaviorPattern(
                        pattern_id=pattern_id,
                        user_id=events[0].user_id,
                        pattern_type="sequence_pattern",
                        pattern_description=f"ØªØ³Ù„Ø³Ù„ Ù…ØªÙƒØ±Ø±: {' â†’ '.join(sequence)}",
                        frequency=frequency,
                        confidence=min(frequency * 2, 0.9),
                        start_time=min(event.timestamp for event in events),
                        triggers=list(sequence[:-1]),
                        outcomes=[sequence[-1]],
                        context_factors={"sequence": sequence, "occurrences": count}
                    )
                    patterns.append(pattern)
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ÙƒØ´Ù Ø£Ù†Ù…Ø§Ø· Ø§Ù„ØªØ³Ù„Ø³Ù„: {e}")
        
        return patterns
    
    async def _detect_context_patterns(self, events: List[BehaviorEvent]) -> List[BehaviorPattern]:
        """ÙƒØ´Ù Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø³ÙŠØ§Ù‚"""
        patterns = []
        
        try:
            if not events:
                return patterns
            
            # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø­Ø¯Ø§Ø« Ø­Ø³Ø¨ Ø§Ù„Ø³ÙŠØ§Ù‚
            context_groups = defaultdict(list)
            
            for event in events:
                for key, value in event.context.items():
                    context_key = f"{key}:{value}"
                    context_groups[context_key].append(event)
            
            # ØªØ­Ù„ÙŠÙ„ ÙƒÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø³ÙŠØ§Ù‚
            for context_key, context_events in context_groups.items():
                if len(context_events) < 3:
                    continue
                
                frequency = len(context_events) / len(events)
                
                if frequency >= 0.15:  # Ø£ÙƒØ«Ø± Ù…Ù† 15% Ù…Ù† Ø§Ù„Ø£Ø­Ø¯Ø§Ø«
                    pattern_id = hashlib.md5(f"context_{events[0].user_id}_{context_key}".encode()).hexdigest()[:12]
                    
                    # ØªØ­Ù„ÙŠÙ„ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£Ø­Ø¯Ø§Ø« ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø³ÙŠØ§Ù‚
                    event_types = [event.event_type for event in context_events]
                    common_event_type = max(set(event_types), key=event_types.count)
                    
                    pattern = BehaviorPattern(
                        pattern_id=pattern_id,
                        user_id=events[0].user_id,
                        pattern_type="context_pattern",
                        pattern_description=f"Ù†Ù…Ø· Ø³Ù„ÙˆÙƒÙŠ ÙÙŠ Ø³ÙŠØ§Ù‚ {context_key}: ØºØ§Ù„Ø¨Ø§Ù‹ {common_event_type}",
                        frequency=frequency,
                        confidence=0.7,
                        start_time=min(event.timestamp for event in context_events),
                        triggers=[context_key],
                        outcomes=[common_event_type],
                        context_factors={"context": context_key, "dominant_behavior": common_event_type}
                    )
                    patterns.append(pattern)
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ÙƒØ´Ù Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø³ÙŠØ§Ù‚: {e}")
        
        return patterns
    
    async def _save_behavior_pattern(self, pattern: BehaviorPattern):
        """Ø­ÙØ¸ Ù†Ù…Ø· Ø³Ù„ÙˆÙƒÙŠ"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT OR REPLACE INTO behavior_patterns
                (pattern_id, user_id, pattern_type, pattern_description,
                 frequency, confidence, start_time, end_time, triggers,
                 outcomes, context_factors, statistical_significance)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                pattern.pattern_id,
                pattern.user_id,
                pattern.pattern_type,
                pattern.pattern_description,
                pattern.frequency,
                pattern.confidence,
                pattern.start_time.isoformat(),
                pattern.end_time.isoformat() if pattern.end_time else None,
                json.dumps(pattern.triggers, ensure_ascii=False),
                json.dumps(pattern.outcomes, ensure_ascii=False),
                json.dumps(pattern.context_factors, ensure_ascii=False),
                pattern.statistical_significance
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ø³Ù„ÙˆÙƒÙŠ: {e}")
    
    async def _generate_behavior_insights(self, user_id: str):
        """ØªÙˆÙ„ÙŠØ¯ Ø±Ø¤Ù‰ Ø³Ù„ÙˆÙƒÙŠØ©"""
        try:
            user_patterns = self.active_patterns.get(user_id, [])
            user_events = [
                event for event in self.behavior_events 
                if event.user_id == user_id
            ]
            
            if not user_patterns or not user_events:
                return
            
            insights = []
            
            # Ø±Ø¤Ù‰ Ø­ÙˆÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…ÙƒØªØ´ÙØ©
            pattern_insights = await self._analyze_pattern_insights(user_patterns, user_events)
            insights.extend(pattern_insights)
            
            # Ø±Ø¤Ù‰ Ø­ÙˆÙ„ Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª Ø§Ù„Ø³Ù„ÙˆÙƒÙŠØ©
            change_insights = await self._analyze_behavior_changes(user_events)
            insights.extend(change_insights)
            
            # Ø±Ø¤Ù‰ Ø­ÙˆÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙˆØ§Ù„ÙØ¹Ø§Ù„ÙŠØ©
            performance_insights = await self._analyze_performance_patterns(user_events)
            insights.extend(performance_insights)
            
            # Ø­ÙØ¸ Ø§Ù„Ø±Ø¤Ù‰
            for insight in insights:
                if insight.confidence >= self.analysis_config["insight_confidence_threshold"]:
                    await self._save_behavior_insight(insight)
                    self.behavior_insights[user_id].append(insight)
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
            self.analytics_stats["insights_generated"] += len(insights)
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±Ø¤Ù‰ Ø§Ù„Ø³Ù„ÙˆÙƒÙŠØ©: {e}")
    
    async def _analyze_pattern_insights(self, patterns: List[BehaviorPattern], events: List[BehaviorEvent]) -> List[BehaviorInsight]:
        """ØªØ­Ù„ÙŠÙ„ Ø±Ø¤Ù‰ Ø§Ù„Ø£Ù†Ù…Ø§Ø·"""
        insights = []
        
        try:
            # ØªØ­Ù„ÙŠÙ„ Ù‚ÙˆØ© Ø§Ù„Ø£Ù†Ù…Ø§Ø·
            strong_patterns = [p for p in patterns if p.confidence > 0.8]
            
            if strong_patterns:
                insight = BehaviorInsight(
                    insight_id=hashlib.md5(f"strong_patterns_{patterns[0].user_id}_{datetime.now()}".encode()).hexdigest()[:12],
                    user_id=patterns[0].user_id,
                    insight_type="pattern_strength",
                    description=f"ØªÙ… Ø§ÙƒØªØ´Ø§Ù {len(strong_patterns)} Ù†Ù…Ø· Ø³Ù„ÙˆÙƒÙŠ Ù‚ÙˆÙŠ",
                    confidence=0.9,
                    impact_score=0.8,
                    recommendations=[
                        "Ø§Ø³ØªÙØ¯ Ù…Ù† Ù‡Ø°Ù‡ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠØ©",
                        "Ù‚Ù… Ø¨ØªØ·ÙˆÙŠØ± Ø±ÙˆØªÙŠÙ† ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ù‡Ø°Ù‡ Ø§Ù„Ø£Ù†Ù…Ø§Ø·"
                    ],
                    supporting_data={"strong_patterns": [p.pattern_description for p in strong_patterns]}
                )
                insights.append(insight)
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø²Ù…Ù†ÙŠØ©
            temporal_patterns = [p for p in patterns if p.pattern_type == "temporal_activity"]
            
            if temporal_patterns:
                insight = BehaviorInsight(
                    insight_id=hashlib.md5(f"temporal_{patterns[0].user_id}_{datetime.now()}".encode()).hexdigest()[:12],
                    user_id=patterns[0].user_id,
                    insight_type="temporal_optimization",
                    description="Ù„Ø¯ÙŠÙƒ Ø£Ù†Ù…Ø§Ø· Ø²Ù…Ù†ÙŠØ© ÙˆØ§Ø¶Ø­Ø© ÙÙŠ Ø§Ù„Ù†Ø´Ø§Ø·",
                    confidence=0.85,
                    impact_score=0.7,
                    recommendations=[
                        "Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…Ù‡Ù…Ø© ÙÙŠ Ø£ÙˆÙ‚Ø§Øª Ø§Ù„Ù†Ø´Ø§Ø· Ø§Ù„Ø°Ø±ÙˆØ©",
                        "ØªØ¬Ù†Ø¨ Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø© ÙÙŠ Ø£ÙˆÙ‚Ø§Øª Ø§Ù„Ù†Ø´Ø§Ø· Ø§Ù„Ù…Ù†Ø®ÙØ¶"
                    ],
                    supporting_data={"temporal_patterns": [p.context_factors for p in temporal_patterns]}
                )
                insights.append(insight)
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø±Ø¤Ù‰ Ø§Ù„Ø£Ù†Ù…Ø§Ø·: {e}")
        
        return insights
    
    async def _analyze_behavior_changes(self, events: List[BehaviorEvent]) -> List[BehaviorInsight]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª Ø§Ù„Ø³Ù„ÙˆÙƒÙŠØ©"""
        insights = []
        
        try:
            if len(events) < 20:
                return insights
            
            # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø£Ø­Ø¯Ø§Ø« Ø¥Ù„Ù‰ ÙØªØ±Ø§Øª
            sorted_events = sorted(events, key=lambda e: e.timestamp)
            mid_point = len(sorted_events) // 2
            
            early_events = sorted_events[:mid_point]
            recent_events = sorted_events[mid_point:]
            
            # Ù…Ù‚Ø§Ø±Ù†Ø© Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£Ø­Ø¯Ø§Ø«
            early_types = set(event.event_type for event in early_events)
            recent_types = set(event.event_type for event in recent_events)
            
            new_behaviors = recent_types - early_types
            stopped_behaviors = early_types - recent_types
            
            if new_behaviors:
                insight = BehaviorInsight(
                    insight_id=hashlib.md5(f"new_behaviors_{events[0].user_id}_{datetime.now()}".encode()).hexdigest()[:12],
                    user_id=events[0].user_id,
                    insight_type="behavior_evolution",
                    description=f"ØªØ·ÙˆÙŠØ± Ø³Ù„ÙˆÙƒÙŠØ§Øª Ø¬Ø¯ÙŠØ¯Ø©: {', '.join(new_behaviors)}",
                    confidence=0.8,
                    impact_score=0.6,
                    recommendations=[
                        "Ø±Ø§Ù‚Ø¨ ÙØ¹Ø§Ù„ÙŠØ© Ø§Ù„Ø³Ù„ÙˆÙƒÙŠØ§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©",
                        "Ø§Ø¯Ù…Ø¬ Ø§Ù„Ø³Ù„ÙˆÙƒÙŠØ§Øª Ø§Ù„Ù…ÙÙŠØ¯Ø© ÙÙŠ Ø±ÙˆØªÙŠÙ†Ùƒ Ø§Ù„ÙŠÙˆÙ…ÙŠ"
                    ],
                    supporting_data={"new_behaviors": list(new_behaviors)}
                )
                insights.append(insight)
            
            if stopped_behaviors:
                insight = BehaviorInsight(
                    insight_id=hashlib.md5(f"stopped_behaviors_{events[0].user_id}_{datetime.now()}".encode()).hexdigest()[:12],
                    user_id=events[0].user_id,
                    insight_type="behavior_reduction",
                    description=f"ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø³Ù„ÙˆÙƒÙŠØ§Øª: {', '.join(stopped_behaviors)}",
                    confidence=0.75,
                    impact_score=0.5,
                    recommendations=[
                        "Ø±Ø§Ø¬Ø¹ Ø£Ø³Ø¨Ø§Ø¨ ØªÙ‚Ù„ÙŠÙ„ Ù‡Ø°Ù‡ Ø§Ù„Ø³Ù„ÙˆÙƒÙŠØ§Øª",
                        "ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„ØªØºÙŠÙŠØ± Ø¥ÙŠØ¬Ø§Ø¨ÙŠ"
                    ],
                    supporting_data={"stopped_behaviors": list(stopped_behaviors)}
                )
                insights.append(insight)
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª Ø§Ù„Ø³Ù„ÙˆÙƒÙŠØ©: {e}")
        
        return insights
    
    async def _analyze_performance_patterns(self, events: List[BehaviorEvent]) -> List[BehaviorInsight]:
        """ØªØ­Ù„ÙŠÙ„ Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        insights = []
        
        try:
            # ØªØ­Ù„ÙŠÙ„ Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø´Ø§Ø·
            if len(events) >= 10:
                # Ø­Ø³Ø§Ø¨ Ù…ØªÙˆØ³Ø· Ø§Ù„Ø£Ø­Ø¯Ø§Ø« ÙŠÙˆÙ…ÙŠØ§Ù‹
                start_date = min(event.timestamp for event in events).date()
                end_date = max(event.timestamp for event in events).date()
                days_span = (end_date - start_date).days + 1
                
                daily_average = len(events) / days_span
                
                if daily_average > 10:
                    performance_level = "Ø¹Ø§Ù„ÙŠ"
                    impact = 0.8
                elif daily_average > 5:
                    performance_level = "Ù…ØªÙˆØ³Ø·"
                    impact = 0.6
                else:
                    performance_level = "Ù…Ù†Ø®ÙØ¶"
                    impact = 0.4
                
                insight = BehaviorInsight(
                    insight_id=hashlib.md5(f"activity_level_{events[0].user_id}_{datetime.now()}".encode()).hexdigest()[:12],
                    user_id=events[0].user_id,
                    insight_type="activity_level",
                    description=f"Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù†Ø´Ø§Ø·: {performance_level} ({daily_average:.1f} Ø­Ø¯Ø«/ÙŠÙˆÙ…)",
                    confidence=0.85,
                    impact_score=impact,
                    recommendations=self._get_activity_recommendations(performance_level),
                    supporting_data={"daily_average": daily_average, "total_days": days_span}
                )
                insights.append(insight)
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø£Ø¯Ø§Ø¡: {e}")
        
        return insights
    
    def _get_activity_recommendations(self, performance_level: str) -> List[str]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙˆØµÙŠØ§Øª Ø§Ù„Ù†Ø´Ø§Ø·"""
        recommendations = {
            "Ø¹Ø§Ù„ÙŠ": [
                "Ø­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø³ØªÙˆÙ‰ Ù…Ù† Ø§Ù„Ù†Ø´Ø§Ø·",
                "ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ø®Ø° ÙØªØ±Ø§Øª Ø±Ø§Ø­Ø© ÙƒØ§ÙÙŠØ©",
                "Ø§Ø³ØªØºÙ„ Ù‡Ø°Ù‡ Ø§Ù„Ø·Ø§Ù‚Ø© ÙÙŠ Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…Ù‡Ù…Ø©"
            ],
            "Ù…ØªÙˆØ³Ø·": [
                "Ø­Ø§ÙˆÙ„ Ø²ÙŠØ§Ø¯Ø© Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù†Ø´Ø§Ø· ØªØ¯Ø±ÙŠØ¬ÙŠØ§Ù‹",
                "Ø±ÙƒØ² Ø¹Ù„Ù‰ Ø¬ÙˆØ¯Ø© Ø§Ù„Ø£Ù†Ø´Ø·Ø©",
                "Ø­Ø¯Ø¯ Ø£Ù‡Ø¯Ø§ÙØ§Ù‹ ÙˆØ§Ø¶Ø­Ø© Ù„Ø²ÙŠØ§Ø¯Ø© Ø§Ù„ØªÙØ§Ø¹Ù„"
            ],
            "Ù…Ù†Ø®ÙØ¶": [
                "Ø§Ø¨Ø¯Ø£ Ø¨Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ù†Ø´Ø§Ø· ØªØ¯Ø±ÙŠØ¬ÙŠØ§Ù‹",
                "Ø§Ø¨Ø­Ø« Ø¹Ù† Ù…Ø­ÙØ²Ø§Øª Ù„Ù„ØªÙØ§Ø¹Ù„ Ø£ÙƒØ«Ø±",
                "Ø­Ø¯Ø¯ Ø£ÙˆÙ‚Ø§ØªØ§Ù‹ Ø«Ø§Ø¨ØªØ© Ù„Ù„Ø£Ù†Ø´Ø·Ø©"
            ]
        }
        return recommendations.get(performance_level, ["Ø±Ø§Ù‚Ø¨ Ù…Ø³ØªÙˆÙ‰ Ù†Ø´Ø§Ø·Ùƒ Ø¨Ø§Ù†ØªØ¸Ø§Ù…"])
    
    async def _save_behavior_insight(self, insight: BehaviorInsight):
        """Ø­ÙØ¸ Ø±Ø¤ÙŠØ© Ø³Ù„ÙˆÙƒÙŠØ©"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT OR REPLACE INTO behavior_insights
                (insight_id, user_id, insight_type, description,
                 confidence, impact_score, recommendations, supporting_data, timestamp)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                insight.insight_id,
                insight.user_id,
                insight.insight_type,
                insight.description,
                insight.confidence,
                insight.impact_score,
                json.dumps(insight.recommendations, ensure_ascii=False),
                json.dumps(insight.supporting_data, ensure_ascii=False),
                insight.timestamp.isoformat()
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø³Ù„ÙˆÙƒÙŠØ©: {e}")
    
    async def get_user_behavior_analysis(self, user_id: str) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ Ø³Ù„ÙˆÙƒ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
        try:
            analysis = {
                "user_id": user_id,
                "patterns": [],
                "insights": [],
                "profile": None,
                "statistics": {},
                "recommendations": []
            }
            
            # Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù†Ø´Ø·Ø©
            user_patterns = self.active_patterns.get(user_id, [])
            analysis["patterns"] = [asdict(pattern) for pattern in user_patterns]
            
            # Ø§Ù„Ø±Ø¤Ù‰ Ø§Ù„Ø³Ù„ÙˆÙƒÙŠØ©
            user_insights = self.behavior_insights.get(user_id, [])
            analysis["insights"] = [asdict(insight) for insight in user_insights]
            
            # Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø´Ø®ØµÙŠ
            if user_id in self.user_profiles:
                analysis["profile"] = asdict(self.user_profiles[user_id])
            
            # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
            user_events = [
                event for event in self.behavior_events 
                if event.user_id == user_id
            ]
            
            analysis["statistics"] = {
                "total_events": len(user_events),
                "patterns_count": len(user_patterns),
                "insights_count": len(user_insights),
                "last_activity": max(event.timestamp for event in user_events).isoformat() if user_events else None,
                "activity_span_days": (
                    max(event.timestamp for event in user_events) - 
                    min(event.timestamp for event in user_events)
                ).days if len(user_events) > 1 else 0
            }
            
            # ØªÙˆØµÙŠØ§Øª Ù…Ø¬Ù…Ø¹Ø©
            all_recommendations = []
            for insight in user_insights:
                all_recommendations.extend(insight.recommendations)
            
            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø± ÙˆØ§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø£Ù‡Ù… Ø§Ù„ØªÙˆØµÙŠØ§Øª
            unique_recommendations = list(set(all_recommendations))
            analysis["recommendations"] = unique_recommendations[:5]  # Ø£ÙØ¶Ù„ 5 ØªÙˆØµÙŠØ§Øª
            
            return analysis
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ù„ÙˆÙƒ: {e}")
            return {"error": str(e)}
    
    async def create_behavior_visualization(self, user_id: str) -> Dict[str, Any]:
        """Ø¥Ù†Ø´Ø§Ø¡ ØªØµÙˆØ±Ø§Øª Ù„Ù„Ø³Ù„ÙˆÙƒ"""
        visualizations = {}
        
        try:
            if not PLOTLY_AVAILABLE:
                return {"error": "Ù…ÙƒØªØ¨Ø© Ø§Ù„ØªØµÙˆØ± ØºÙŠØ± Ù…ØªØ§Ø­Ø©"}
            
            user_events = [
                event for event in self.behavior_events 
                if event.user_id == user_id
            ]
            
            if not user_events:
                return {"error": "Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ©"}
            
            # Ù…Ø®Ø·Ø· Ø§Ù„Ù†Ø´Ø§Ø· Ø§Ù„Ø²Ù…Ù†ÙŠ
            activity_viz = await self._create_activity_timeline(user_events)
            if activity_viz:
                visualizations["activity_timeline"] = activity_viz
            
            # Ù…Ø®Ø·Ø· Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£Ø­Ø¯Ø§Ø«
            event_types_viz = await self._create_event_types_chart(user_events)
            if event_types_viz:
                visualizations["event_types"] = event_types_viz
            
            # Ù…Ø®Ø·Ø· Ø§Ù„Ø£Ù†Ù…Ø§Ø·
            patterns_viz = await self._create_patterns_chart(user_id)
            if patterns_viz:
                visualizations["patterns"] = patterns_viz
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªØµÙˆØ±Ø§Øª: {e}")
            visualizations = {"error": str(e)}
        
        return visualizations
    
    async def _create_activity_timeline(self, events: List[BehaviorEvent]) -> Optional[str]:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø·Ø· Ø²Ù…Ù†ÙŠ Ù„Ù„Ù†Ø´Ø§Ø·"""
        try:
            # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø­Ø¯Ø§Ø« Ø­Ø³Ø¨ Ø§Ù„ØªØ§Ø±ÙŠØ®
            daily_counts = defaultdict(int)
            for event in events:
                date = event.timestamp.date()
                daily_counts[date] += 1
            
            dates = list(daily_counts.keys())
            counts = list(daily_counts.values())
            
            fig = go.Figure()
            fig.add_trace(go.Scatter(
                x=dates,
                y=counts,
                mode='lines+markers',
                name='Ø§Ù„Ù†Ø´Ø§Ø· Ø§Ù„ÙŠÙˆÙ…ÙŠ',
                line=dict(color='#2E86AB', width=3),
                marker=dict(size=8)
            ))
            
            fig.update_layout(
                title='ØªØ·ÙˆØ± Ø§Ù„Ù†Ø´Ø§Ø· Ø¹Ø¨Ø± Ø§Ù„Ø²Ù…Ù†',
                xaxis_title='Ø§Ù„ØªØ§Ø±ÙŠØ®',
                yaxis_title='Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø­Ø¯Ø§Ø«',
                template='plotly_white',
                font=dict(family="Arial", size=12)
            )
            
            return fig.to_json()
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø·Ø· Ø§Ù„Ù†Ø´Ø§Ø· Ø§Ù„Ø²Ù…Ù†ÙŠ: {e}")
            return None
    
    async def _create_event_types_chart(self, events: List[BehaviorEvent]) -> Optional[str]:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø·Ø· Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£Ø­Ø¯Ø§Ø«"""
        try:
            # ØªØ¬Ù…ÙŠØ¹ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£Ø­Ø¯Ø§Ø«
            event_type_counts = defaultdict(int)
            for event in events:
                event_type_counts[event.event_type] += 1
            
            labels = list(event_type_counts.keys())
            values = list(event_type_counts.values())
            
            fig = go.Figure(data=go.Pie(
                labels=labels,
                values=values,
                hole=0.4,
                textinfo='label+percent',
                textposition='outside'
            ))
            
            fig.update_layout(
                title='ØªÙˆØ²ÙŠØ¹ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£Ø­Ø¯Ø§Ø«',
                template='plotly_white',
                font=dict(family="Arial", size=12)
            )
            
            return fig.to_json()
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø·Ø· Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£Ø­Ø¯Ø§Ø«: {e}")
            return None
    
    async def _create_patterns_chart(self, user_id: str) -> Optional[str]:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø·Ø· Ø§Ù„Ø£Ù†Ù…Ø§Ø·"""
        try:
            patterns = self.active_patterns.get(user_id, [])
            
            if not patterns:
                return None
            
            pattern_names = [pattern.pattern_type for pattern in patterns]
            pattern_confidence = [pattern.confidence for pattern in patterns]
            pattern_frequency = [pattern.frequency for pattern in patterns]
            
            fig = make_subplots(
                rows=1, cols=2,
                subplot_titles=('Ø«Ù‚Ø© Ø§Ù„Ø£Ù†Ù…Ø§Ø·', 'ØªÙƒØ±Ø§Ø± Ø§Ù„Ø£Ù†Ù…Ø§Ø·'),
                specs=[[{"type": "bar"}, {"type": "bar"}]]
            )
            
            fig.add_trace(
                go.Bar(x=pattern_names, y=pattern_confidence, name='Ø§Ù„Ø«Ù‚Ø©'),
                row=1, col=1
            )
            
            fig.add_trace(
                go.Bar(x=pattern_names, y=pattern_frequency, name='Ø§Ù„ØªÙƒØ±Ø§Ø±'),
                row=1, col=2
            )
            
            fig.update_layout(
                title='ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø³Ù„ÙˆÙƒÙŠØ©',
                template='plotly_white',
                font=dict(family="Arial", size=12)
            )
            
            return fig.to_json()
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø·Ø· Ø§Ù„Ø£Ù†Ù…Ø§Ø·: {e}")
            return None
    
    async def _load_historical_data(self):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ©"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£Ø­Ø¯Ø§Ø« Ø§Ù„Ø£Ø®ÙŠØ±Ø©
            cursor.execute('''
                SELECT * FROM behavior_events 
                WHERE timestamp > datetime('now', '-30 days')
                ORDER BY timestamp DESC
                LIMIT 1000
            ''')
            
            events_data = cursor.fetchall()
            
            for row in events_data:
                event = BehaviorEvent(
                    event_id=row[0],
                    user_id=row[1],
                    event_type=row[2],
                    event_data=json.loads(row[3]) if row[3] else {},
                    timestamp=datetime.fromisoformat(row[4]),
                    session_id=row[5] or "",
                    context=json.loads(row[6]) if row[6] else {},
                    metadata=json.loads(row[7]) if row[7] else {}
                )
                self.behavior_events.append(event)
            
            # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù†Ø´Ø·Ø©
            cursor.execute('''
                SELECT * FROM behavior_patterns
                WHERE end_time IS NULL OR end_time > datetime('now', '-7 days')
            ''')
            
            patterns_data = cursor.fetchall()
            
            for row in patterns_data:
                pattern = BehaviorPattern(
                    pattern_id=row[0],
                    user_id=row[1],
                    pattern_type=row[2],
                    pattern_description=row[3],
                    frequency=row[4],
                    confidence=row[5],
                    start_time=datetime.fromisoformat(row[6]),
                    end_time=datetime.fromisoformat(row[7]) if row[7] else None,
                    triggers=json.loads(row[8]) if row[8] else [],
                    outcomes=json.loads(row[9]) if row[9] else [],
                    context_factors=json.loads(row[10]) if row[10] else {},
                    statistical_significance=row[11] or 0.0
                )
                self.active_patterns[pattern.user_id].append(pattern)
            
            conn.close()
            
            self.analytics_stats["users_analyzed"] = len(set(event.user_id for event in self.behavior_events))
            
            self.logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(events_data)} Ø­Ø¯Ø« Ùˆ {len(patterns_data)} Ù†Ù…Ø·")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ©: {e}")
    
    async def get_system_analytics(self) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…"""
        try:
            return {
                "system_stats": self.analytics_stats,
                "active_users": len(set(event.user_id for event in self.behavior_events)),
                "total_patterns": sum(len(patterns) for patterns in self.active_patterns.values()),
                "total_insights": sum(len(insights) for insights in self.behavior_insights.values()),
                "processing_queue_size": self.processing_queue.qsize(),
                "models_loaded": {
                    "embedding_network": self.embedding_network is not None,
                    "clustering_model": self.clustering_model is not None,
                    "anomaly_detector": self.anomaly_detector is not None
                },
                "database_status": "connected" if self.db_path.exists() else "disconnected"
            }
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…: {e}")
            return {"error": str(e)}
    
    async def cleanup(self):
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯"""
        try:
            self.is_processing = False
            
            # Ø§Ù†ØªØ¸Ø§Ø± Ø§Ù†ØªÙ‡Ø§Ø¡ Ø§Ù„Ø¹Ù…Ø§Ù„
            for worker in self.background_workers:
                if worker.is_alive():
                    worker.join(timeout=2)
            
            self.logger.info("âœ… ØªÙ… ØªÙ†Ø¸ÙŠÙ Ù…ÙˆØ§Ø±Ø¯ Ù…Ø­Ø±Ùƒ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ù„ÙˆÙƒÙŠ")
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯: {e}")

# Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø«ÙŠÙ„ Ø¹Ø§Ù…
behavioral_analytics = AdvancedBehavioralAnalytics()

async def get_behavioral_analytics() -> AdvancedBehavioralAnalytics:
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø­Ø±Ùƒ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ù„ÙˆÙƒÙŠ"""
    if not behavioral_analytics.is_initialized:
        await behavioral_analytics.initialize()
    return behavioral_analytics

if __name__ == "__main__":
    async def test_behavioral_analytics():
        """Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø­Ø±Ùƒ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ù„ÙˆÙƒÙŠ"""
        print("ğŸ§  Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø­Ø±Ùƒ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ù„ÙˆÙƒ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…")
        print("=" * 50)
        
        analytics = await get_behavioral_analytics()
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø£Ø­Ø¯Ø§Ø« ØªØ¬Ø±ÙŠØ¨ÙŠØ©
        test_events = []
        base_time = datetime.now()
        
        for i in range(20):
            event = BehaviorEvent(
                event_id=f"test_event_{i}",
                user_id="test_user",
                event_type=["login", "search", "create", "share", "logout"][i % 5],
                event_data={"action": f"action_{i}", "value": i},
                timestamp=base_time + timedelta(hours=i),
                session_id=f"session_{i//5}",
                context={"location": "home" if i % 2 == 0 else "office"}
            )
            test_events.append(event)
        
        print(f"ğŸ“ Ø¥Ø¶Ø§ÙØ© {len(test_events)} Ø­Ø¯Ø« ØªØ¬Ø±ÙŠØ¨ÙŠ...")
        
        # ØªØªØ¨Ø¹ Ø§Ù„Ø£Ø­Ø¯Ø§Ø«
        for event in test_events:
            await analytics.track_behavior_event(event)
        
        # Ø§Ù†ØªØ¸Ø§Ø± Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        await asyncio.sleep(3)
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„
        analysis = await analytics.get_user_behavior_analysis("test_user")
        
        print(f"\nğŸ“Š Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„:")
        print(f"  â€¢ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø£Ø­Ø¯Ø§Ø«: {analysis['statistics']['total_events']}")
        print(f"  â€¢ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…ÙƒØªØ´ÙØ©: {analysis['statistics']['patterns_count']}")
        print(f"  â€¢ Ø§Ù„Ø±Ø¤Ù‰ Ø§Ù„Ù…ÙˆÙ„Ø¯Ø©: {analysis['statistics']['insights_count']}")
        
        if analysis['patterns']:
            print(f"\nğŸ” Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…ÙƒØªØ´ÙØ©:")
            for pattern in analysis['patterns']:
                print(f"  â€¢ {pattern['pattern_description']} (Ø«Ù‚Ø©: {pattern['confidence']:.1%})")
        
        if analysis['insights']:
            print(f"\nğŸ’¡ Ø§Ù„Ø±Ø¤Ù‰ Ø§Ù„Ø³Ù„ÙˆÙƒÙŠØ©:")
            for insight in analysis['insights']:
                print(f"  â€¢ {insight['description']} (Ø«Ù‚Ø©: {insight['confidence']:.1%})")
        
        if analysis['recommendations']:
            print(f"\nğŸ¯ Ø§Ù„ØªÙˆØµÙŠØ§Øª:")
            for recommendation in analysis['recommendations']:
                print(f"  â€¢ {recommendation}")
        
        # ØªØ­Ù„ÙŠÙ„Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…
        system_analytics = await analytics.get_system_analytics()
        print(f"\nğŸ–¥ï¸ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ø¸Ø§Ù…:")
        print(f"  â€¢ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙˆÙ† Ø§Ù„Ù†Ø´Ø·ÙˆÙ†: {system_analytics['active_users']}")
        print(f"  â€¢ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø£Ù†Ù…Ø§Ø·: {system_analytics['total_patterns']}")
        print(f"  â€¢ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø±Ø¤Ù‰: {system_analytics['total_insights']}")
        
        await analytics.cleanup()
        print("\nâœ¨ Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø¨Ù†Ø¬Ø§Ø­!")

    # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
    asyncio.run(test_behavioral_analytics())
