
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ù…Ø­Ø±Ùƒ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù„Ù„Ø£Ù†Ù…Ø§Ø· ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙ†Ø¨Ø¤ÙŠ
Advanced Smart Prediction Engine for Pattern Analysis
"""

import asyncio
import logging
import numpy as np
import pandas as pd
import json
import pickle
import warnings
from typing import Dict, List, Any, Optional, Tuple, Union, Callable
from pathlib import Path
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict, field
from enum import Enum
from collections import defaultdict, deque
import hashlib
import threading
import queue
warnings.filterwarnings('ignore')

# Machine Learning
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, IsolationForest
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, f1_score
from sklearn.pipeline import Pipeline
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

# Time Series Analysis
try:
    from statsmodels.tsa.arima.model import ARIMA
    from statsmodels.tsa.seasonal import seasonal_decompose
    from statsmodels.tsa.holtwinters import ExponentialSmoothing
    from statsmodels.tsa.statespace.sarimax import SARIMAX
    TIMESERIES_AVAILABLE = True
except ImportError:
    TIMESERIES_AVAILABLE = False
    logging.warning("Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ø³Ù„Ø§Ø³Ù„ Ø§Ù„Ø²Ù…Ù†ÙŠØ© ØºÙŠØ± Ù…ØªÙˆÙØ±Ø©")

# Deep Learning
try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import DataLoader, TensorDataset
    PYTORCH_AVAILABLE = True
except ImportError:
    PYTORCH_AVAILABLE = False
    logging.warning("PyTorch ØºÙŠØ± Ù…ØªÙˆÙØ± Ù„Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚")

# Advanced Analytics
try:
    import xgboost as xgb
    import lightgbm as lgb
    BOOSTING_AVAILABLE = True
except ImportError:
    BOOSTING_AVAILABLE = False
    logging.warning("Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„ØªØ¹Ø²ÙŠØ² Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© ØºÙŠØ± Ù…ØªÙˆÙØ±Ø©")

class PredictionType(Enum):
    """Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„ØªÙ†Ø¨Ø¤"""
    TIME_SERIES = "time_series"          # Ø³Ù„Ø§Ø³Ù„ Ø²Ù…Ù†ÙŠØ©
    CLASSIFICATION = "classification"    # ØªØµÙ†ÙŠÙ
    REGRESSION = "regression"           # Ø§Ù†Ø­Ø¯Ø§Ø±
    CLUSTERING = "clustering"           # ØªØ¬Ù…ÙŠØ¹
    ANOMALY_DETECTION = "anomaly"       # ÙƒØ´Ù Ø§Ù„Ø´Ø°ÙˆØ°
    PATTERN_RECOGNITION = "pattern"     # ØªÙ…ÙŠÙŠØ² Ø§Ù„Ø£Ù†Ù…Ø§Ø·
    TREND_ANALYSIS = "trend"            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª
    FORECASTING = "forecasting"         # ØªÙˆÙ‚Ø¹Ø§Øª Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ©

class ModelComplexity(Enum):
    """Ù…Ø³ØªÙˆÙŠØ§Øª ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
    SIMPLE = "simple"        # Ø¨Ø³ÙŠØ·
    MODERATE = "moderate"    # Ù…ØªÙˆØ³Ø·
    COMPLEX = "complex"      # Ù…Ø¹Ù‚Ø¯
    DEEP = "deep"           # Ø¹Ù…ÙŠÙ‚

class DataPattern(Enum):
    """Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
    LINEAR = "linear"                # Ø®Ø·ÙŠ
    NON_LINEAR = "non_linear"        # ØºÙŠØ± Ø®Ø·ÙŠ
    SEASONAL = "seasonal"            # Ù…ÙˆØ³Ù…ÙŠ
    CYCLIC = "cyclic"               # Ø¯ÙˆØ±ÙŠ
    TRENDING = "trending"           # Ø§ØªØ¬Ø§Ù‡ÙŠ
    RANDOM = "random"               # Ø¹Ø´ÙˆØ§Ø¦ÙŠ
    MIXED = "mixed"                 # Ù…Ø®ØªÙ„Ø·

@dataclass
class PredictionRequest:
    """Ø·Ù„Ø¨ Ø§Ù„ØªÙ†Ø¨Ø¤"""
    request_id: str
    prediction_type: PredictionType
    data: Dict[str, Any]
    target_variable: Optional[str] = None
    prediction_horizon: int = 10
    confidence_level: float = 0.95
    model_complexity: ModelComplexity = ModelComplexity.MODERATE
    features: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)
    created_at: datetime = field(default_factory=datetime.now)

@dataclass
class PredictionResult:
    """Ù†ØªÙŠØ¬Ø© Ø§Ù„ØªÙ†Ø¨Ø¤"""
    request_id: str
    predictions: List[float]
    confidence_intervals: Optional[List[Tuple[float, float]]] = None
    feature_importance: Optional[Dict[str, float]] = None
    model_performance: Dict[str, float] = field(default_factory=dict)
    detected_patterns: List[str] = field(default_factory=list)
    anomalies: List[int] = field(default_factory=list)
    recommendations: List[str] = field(default_factory=list)
    processing_time: float = 0.0
    created_at: datetime = field(default_factory=datetime.now)

@dataclass
class PatternAnalysis:
    """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø·"""
    pattern_type: DataPattern
    strength: float  # 0-1
    frequency: Optional[float] = None
    phase: Optional[float] = None
    trend_direction: Optional[str] = None
    seasonal_components: Optional[Dict[str, float]] = None
    description: str = ""

class AdvancedNeuralPredictor(nn.Module):
    """Ø´Ø¨ÙƒØ© Ø¹ØµØ¨ÙŠØ© Ù…ØªÙ‚Ø¯Ù…Ø© Ù„Ù„ØªÙ†Ø¨Ø¤"""
    
    def __init__(self, input_size: int, hidden_sizes: List[int], output_size: int, dropout_rate: float = 0.2):
        super().__init__()
        
        self.layers = nn.ModuleList()
        self.dropout_layers = nn.ModuleList()
        self.batch_norms = nn.ModuleList()
        
        # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª
        prev_size = input_size
        for hidden_size in hidden_sizes:
            self.layers.append(nn.Linear(prev_size, hidden_size))
            self.dropout_layers.append(nn.Dropout(dropout_rate))
            self.batch_norms.append(nn.BatchNorm1d(hidden_size))
            prev_size = hidden_size
        
        # Ø·Ø¨Ù‚Ø© Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬
        self.output_layer = nn.Linear(prev_size, output_size)
        
        # Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ (Attention)
        self.attention = nn.MultiheadAttention(embed_dim=prev_size, num_heads=4, batch_first=True)
        
    def forward(self, x):
        # Ø¥Ø¶Ø§ÙØ© Ø¨ÙØ¹Ø¯ Ù„Ù„ØªØ³Ù„Ø³Ù„ Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
        if len(x.shape) == 2:
            x = x.unsqueeze(1)  # (batch_size, 1, features)
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡
        attn_output, _ = self.attention(x, x, x)
        x = attn_output.squeeze(1)
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ù…Ø®ÙÙŠØ©
        for i, layer in enumerate(self.layers):
            x = layer(x)
            if x.size(0) > 1:  # ØªØ·Ø¨ÙŠÙ‚ BatchNorm ÙÙ‚Ø· Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ Ø£ÙƒØ«Ø± Ù…Ù† Ø¹ÙŠÙ†Ø©
                x = self.batch_norms[i](x)
            x = torch.relu(x)
            x = self.dropout_layers[i](x)
        
        x = self.output_layer(x)
        return x

class SmartPredictionEngine:
    """Ù…Ø­Ø±Ùƒ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.logger = logging.getLogger(__name__)
        self.config = config or {}
        
        # Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
        self.models: Dict[str, Any] = {}
        self.model_performance: Dict[str, Dict[str, float]] = {}
        self.pattern_analyzers: Dict[str, Any] = {}
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        self.scalers: Dict[str, StandardScaler] = {}
        self.encoders: Dict[str, LabelEncoder] = {}
        self.feature_selectors: Dict[str, Any] = {}
        
        # Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
        self.prediction_cache: Dict[str, PredictionResult] = {}
        self.pattern_cache: Dict[str, List[PatternAnalysis]] = {}
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª ÙˆÙ…Ù‚Ø§ÙŠÙŠØ³
        self.request_history: List[PredictionRequest] = []
        self.performance_metrics: Dict[str, float] = {}
        self.model_usage_stats: Dict[str, int] = defaultdict(int)
        
        # Ø®ÙŠÙˆØ· Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        self.prediction_queue = queue.Queue()
        self.processing_thread = None
        self.is_running = False
        
        # Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ø­ÙØ¸
        self.models_dir = Path("data/prediction_models")
        self.models_dir.mkdir(parents=True, exist_ok=True)
        
        # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
        self._initialize_models()
        
        self.logger.info("ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ø±Ùƒ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…")

    def _initialize_models(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©"""
        try:
            # Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠØ©
            self.models['linear_regression'] = LinearRegression()
            self.models['ridge_regression'] = Ridge(alpha=1.0)
            self.models['random_forest'] = RandomForestRegressor(n_estimators=100, random_state=42)
            self.models['gradient_boosting'] = GradientBoostingRegressor(n_estimators=100, random_state=42)
            
            # Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØ¬Ù…ÙŠØ¹
            self.models['kmeans'] = KMeans(n_clusters=5, random_state=42)
            self.models['dbscan'] = DBSCAN(eps=0.5, min_samples=5)
            
            # ÙƒØ´Ù Ø§Ù„Ø´Ø°ÙˆØ°
            self.models['isolation_forest'] = IsolationForest(contamination=0.1, random_state=42)
            
            # Ù†Ù…Ø§Ø°Ø¬ Ù…ØªÙ‚Ø¯Ù…Ø© Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…ØªÙˆÙØ±Ø©
            if BOOSTING_AVAILABLE:
                self.models['xgboost'] = xgb.XGBRegressor(random_state=42)
                self.models['lightgbm'] = lgb.LGBMRegressor(random_state=42)
            
            # Ù…Ø­Ù„Ù„Ø§Øª Ø§Ù„Ø£Ù†Ù…Ø§Ø·
            self.pattern_analyzers['trend'] = self._analyze_trend
            self.pattern_analyzers['seasonality'] = self._analyze_seasonality
            self.pattern_analyzers['cyclical'] = self._analyze_cyclical_patterns
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬: {e}")

    async def start_prediction_engine(self):
        """Ø¨Ø¯Ø¡ Ù…Ø­Ø±Ùƒ Ø§Ù„ØªÙ†Ø¨Ø¤"""
        try:
            self.is_running = True
            self.processing_thread = threading.Thread(target=self._prediction_processing_loop)
            self.processing_thread.start()
            self.logger.info("ØªÙ… Ø¨Ø¯Ø¡ Ù…Ø­Ø±Ùƒ Ø§Ù„ØªÙ†Ø¨Ø¤")
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¨Ø¯Ø¡ Ù…Ø­Ø±Ùƒ Ø§Ù„ØªÙ†Ø¨Ø¤: {e}")
            raise

    async def stop_prediction_engine(self):
        """Ø¥ÙŠÙ‚Ø§Ù Ù…Ø­Ø±Ùƒ Ø§Ù„ØªÙ†Ø¨Ø¤"""
        try:
            self.is_running = False
            if self.processing_thread and self.processing_thread.is_alive():
                self.processing_thread.join(timeout=5)
            
            await self.save_models()
            self.logger.info("ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ù…Ø­Ø±Ùƒ Ø§Ù„ØªÙ†Ø¨Ø¤")
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥ÙŠÙ‚Ø§Ù Ù…Ø­Ø±Ùƒ Ø§Ù„ØªÙ†Ø¨Ø¤: {e}")

    def _prediction_processing_loop(self):
        """Ø­Ù„Ù‚Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ù„Ø¨Ø§Øª Ø§Ù„ØªÙ†Ø¨Ø¤"""
        while self.is_running:
            try:
                try:
                    request = self.prediction_queue.get(timeout=1)
                    asyncio.create_task(self._process_prediction_request(request))
                except queue.Empty:
                    continue
                    
            except Exception as e:
                self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø­Ù„Ù‚Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªÙ†Ø¨Ø¤: {e}")

    async def predict(self, request: PredictionRequest) -> PredictionResult:
        """ØªÙ†ÙÙŠØ° Ø·Ù„Ø¨ Ø§Ù„ØªÙ†Ø¨Ø¤"""
        try:
            start_time = datetime.now()
            
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù†ØªÙŠØ¬Ø© Ù…Ø®Ø²Ù†Ø© Ù…Ø¤Ù‚ØªØ§Ù‹
            cache_key = self._generate_cache_key(request)
            if cache_key in self.prediction_cache:
                cached_result = self.prediction_cache[cache_key]
                if (datetime.now() - cached_result.created_at).total_seconds() < 3600:  # Ø³Ø§Ø¹Ø© ÙˆØ§Ø­Ø¯Ø©
                    return cached_result
            
            # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            processed_data = await self._prepare_data(request)
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø·
            patterns = await self._analyze_patterns(processed_data, request)
            
            # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨
            best_model = await self._select_best_model(processed_data, request, patterns)
            
            # ØªÙ†ÙÙŠØ° Ø§Ù„ØªÙ†Ø¨Ø¤
            predictions = await self._execute_prediction(best_model, processed_data, request)
            
            # Ø­Ø³Ø§Ø¨ ÙØªØ±Ø§Øª Ø§Ù„Ø«Ù‚Ø©
            confidence_intervals = await self._calculate_confidence_intervals(
                predictions, processed_data, request
            )
            
            # ÙƒØ´Ù Ø§Ù„Ø´Ø°ÙˆØ°
            anomalies = await self._detect_anomalies(processed_data, predictions)
            
            # ØªØ­Ù„ÙŠÙ„ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ù…ÙŠØ²Ø§Øª
            feature_importance = await self._analyze_feature_importance(best_model, processed_data)
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙˆØµÙŠØ§Øª
            recommendations = await self._generate_recommendations(
                predictions, patterns, anomalies, request
            )
            
            # Ø­Ø³Ø§Ø¨ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡
            performance = await self._evaluate_model_performance(best_model, processed_data, request)
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†ØªÙŠØ¬Ø©
            processing_time = (datetime.now() - start_time).total_seconds()
            
            result = PredictionResult(
                request_id=request.request_id,
                predictions=predictions.tolist() if isinstance(predictions, np.ndarray) else predictions,
                confidence_intervals=confidence_intervals,
                feature_importance=feature_importance,
                model_performance=performance,
                detected_patterns=[p.pattern_type.value for p in patterns],
                anomalies=anomalies,
                recommendations=recommendations,
                processing_time=processing_time
            )
            
            # Ø­ÙØ¸ ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ©
            self.prediction_cache[cache_key] = result
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
            self.request_history.append(request)
            self.model_usage_stats[best_model] += 1
            
            return result
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªÙ†ÙÙŠØ° Ø§Ù„ØªÙ†Ø¨Ø¤: {e}")
            raise

    async def _prepare_data(self, request: PredictionRequest) -> Dict[str, Any]:
        """ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªÙ†Ø¨Ø¤"""
        try:
            data = request.data
            
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ DataFrame
            if isinstance(data, dict):
                if 'dataframe' in data:
                    df = pd.DataFrame(data['dataframe'])
                elif 'series' in data:
                    df = pd.DataFrame({'value': data['series']})
                else:
                    df = pd.DataFrame([data])
            else:
                df = pd.DataFrame(data)
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©
            df = df.fillna(df.mean(numeric_only=True))
            
            # ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„ÙØ¦ÙˆÙŠØ©
            categorical_columns = df.select_dtypes(include=['object']).columns
            for col in categorical_columns:
                if col not in self.encoders:
                    self.encoders[col] = LabelEncoder()
                    df[col] = self.encoders[col].fit_transform(df[col].astype(str))
                else:
                    df[col] = self.encoders[col].transform(df[col].astype(str))
            
            # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø±Ù‚Ù…ÙŠØ©
            numeric_columns = df.select_dtypes(include=[np.number]).columns
            if len(numeric_columns) > 0:
                scaler_key = f"scaler_{request.prediction_type.value}"
                if scaler_key not in self.scalers:
                    self.scalers[scaler_key] = StandardScaler()
                    df[numeric_columns] = self.scalers[scaler_key].fit_transform(df[numeric_columns])
                else:
                    df[numeric_columns] = self.scalers[scaler_key].transform(df[numeric_columns])
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø²Ù…Ù†ÙŠØ© Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ Ø¹Ù…ÙˆØ¯ ØªØ§Ø±ÙŠØ®
            if 'timestamp' in df.columns or 'date' in df.columns:
                time_col = 'timestamp' if 'timestamp' in df.columns else 'date'
                df[time_col] = pd.to_datetime(df[time_col])
                df['hour'] = df[time_col].dt.hour
                df['day_of_week'] = df[time_col].dt.dayofweek
                df['month'] = df[time_col].dt.month
                df['quarter'] = df[time_col].dt.quarter
            
            return {
                'dataframe': df,
                'target_variable': request.target_variable,
                'features': request.features or list(df.columns),
                'original_data': data
            }
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
            raise

    async def _analyze_patterns(self, data: Dict[str, Any], request: PredictionRequest) -> List[PatternAnalysis]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø· ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        try:
            df = data['dataframe']
            patterns = []
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§ØªØ¬Ø§Ù‡ Ø§Ù„Ø¹Ø§Ù…
            if request.target_variable and request.target_variable in df.columns:
                target_series = df[request.target_variable]
                
                # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§ØªØ¬Ø§Ù‡
                trend_analysis = await self._analyze_trend(target_series)
                if trend_analysis:
                    patterns.append(trend_analysis)
                
                # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙˆØ³Ù…ÙŠØ©
                if len(target_series) >= 24:  # Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ© Ù„Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙˆØ³Ù…ÙŠ
                    seasonal_analysis = await self._analyze_seasonality(target_series)
                    if seasonal_analysis:
                        patterns.append(seasonal_analysis)
                
                # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¯ÙˆØ±ÙŠØ©
                cyclical_analysis = await self._analyze_cyclical_patterns(target_series)
                if cyclical_analysis:
                    patterns.append(cyclical_analysis)
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø¨ÙŠÙ† Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª
            correlation_patterns = await self._analyze_correlations(df)
            patterns.extend(correlation_patterns)
            
            return patterns
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø·: {e}")
            return []

    async def _analyze_trend(self, series: pd.Series) -> Optional[PatternAnalysis]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§ØªØ¬Ø§Ù‡ ÙÙŠ Ø§Ù„Ø³Ù„Ø³Ù„Ø© Ø§Ù„Ø²Ù…Ù†ÙŠØ©"""
        try:
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§ØªØ¬Ø§Ù‡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ
            x = np.arange(len(series))
            coeffs = np.polyfit(x, series, 1)
            slope = coeffs[0]
            
            # ØªØ­Ø¯ÙŠØ¯ Ù‚ÙˆØ© Ø§Ù„Ø§ØªØ¬Ø§Ù‡
            correlation = np.corrcoef(x, series)[0, 1]
            strength = abs(correlation)
            
            if strength > 0.5:  # Ø§ØªØ¬Ø§Ù‡ Ù‚ÙˆÙŠ
                direction = "ØµØ§Ø¹Ø¯" if slope > 0 else "Ù‡Ø§Ø¨Ø·"
                
                return PatternAnalysis(
                    pattern_type=DataPattern.TRENDING,
                    strength=strength,
                    trend_direction=direction,
                    description=f"Ø§ØªØ¬Ø§Ù‡ {direction} Ø¨Ù‚ÙˆØ© {strength:.2f}"
                )
            
            return None
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§ØªØ¬Ø§Ù‡: {e}")
            return None

    async def _analyze_seasonality(self, series: pd.Series) -> Optional[PatternAnalysis]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙˆØ³Ù…ÙŠØ© ÙÙŠ Ø§Ù„Ø³Ù„Ø³Ù„Ø© Ø§Ù„Ø²Ù…Ù†ÙŠØ©"""
        try:
            if not TIMESERIES_AVAILABLE or len(series) < 24:
                return None
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙˆØ³Ù…ÙŠØ©
            decomposition = seasonal_decompose(series, model='additive', period=12)
            seasonal_component = decomposition.seasonal
            
            # Ø­Ø³Ø§Ø¨ Ù‚ÙˆØ© Ø§Ù„Ù…ÙˆØ³Ù…ÙŠØ©
            seasonal_strength = np.var(seasonal_component) / np.var(series)
            
            if seasonal_strength > 0.1:  # Ù…ÙˆØ³Ù…ÙŠØ© ÙˆØ§Ø¶Ø­Ø©
                return PatternAnalysis(
                    pattern_type=DataPattern.SEASONAL,
                    strength=seasonal_strength,
                    seasonal_components={
                        'amplitude': np.max(seasonal_component) - np.min(seasonal_component),
                        'period': 12
                    },
                    description=f"Ù†Ù…Ø· Ù…ÙˆØ³Ù…ÙŠ Ø¨Ù‚ÙˆØ© {seasonal_strength:.2f}"
                )
            
            return None
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙˆØ³Ù…ÙŠØ©: {e}")
            return None

    async def _analyze_cyclical_patterns(self, series: pd.Series) -> Optional[PatternAnalysis]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø¯ÙˆØ±ÙŠØ©"""
        try:
            # ØªØ­Ù„ÙŠÙ„ FFT Ù„Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„ØªØ±Ø¯Ø¯Ø§Øª Ø§Ù„Ù…Ù‡ÙŠÙ…Ù†Ø©
            fft = np.fft.fft(series.values)
            freqs = np.fft.fftfreq(len(series))
            
            # Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£Ù‚ÙˆÙ‰ ØªØ±Ø¯Ø¯
            dominant_freq_idx = np.argmax(np.abs(fft[1:len(fft)//2])) + 1
            dominant_freq = freqs[dominant_freq_idx]
            
            if abs(dominant_freq) > 0.01:  # ØªØ±Ø¯Ø¯ ÙˆØ§Ø¶Ø­
                period = 1 / abs(dominant_freq)
                strength = np.abs(fft[dominant_freq_idx]) / np.sum(np.abs(fft))
                
                if strength > 0.1:
                    return PatternAnalysis(
                        pattern_type=DataPattern.CYCLIC,
                        strength=strength,
                        frequency=abs(dominant_freq),
                        description=f"Ù†Ù…Ø· Ø¯ÙˆØ±ÙŠ Ø¨ÙØªØ±Ø© {period:.1f} ÙˆÙ‚ÙˆØ© {strength:.2f}"
                    )
            
            return None
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø¯ÙˆØ±ÙŠØ©: {e}")
            return None

    async def _analyze_correlations(self, df: pd.DataFrame) -> List[PatternAnalysis]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø¨ÙŠÙ† Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª"""
        try:
            patterns = []
            numeric_columns = df.select_dtypes(include=[np.number]).columns
            
            if len(numeric_columns) > 1:
                correlation_matrix = df[numeric_columns].corr()
                
                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ø±ØªØ¨Ø§Ø·Ø§Øª Ù‚ÙˆÙŠØ©
                for i in range(len(correlation_matrix.columns)):
                    for j in range(i+1, len(correlation_matrix.columns)):
                        correlation = correlation_matrix.iloc[i, j]
                        
                        if abs(correlation) > 0.7:  # Ø§Ø±ØªØ¨Ø§Ø· Ù‚ÙˆÙŠ
                            col1 = correlation_matrix.columns[i]
                            col2 = correlation_matrix.columns[j]
                            
                            pattern = PatternAnalysis(
                                pattern_type=DataPattern.LINEAR if correlation > 0 else DataPattern.NON_LINEAR,
                                strength=abs(correlation),
                                description=f"Ø§Ø±ØªØ¨Ø§Ø· {'Ø¥ÙŠØ¬Ø§Ø¨ÙŠ' if correlation > 0 else 'Ø³Ù„Ø¨ÙŠ'} Ù‚ÙˆÙŠ Ø¨ÙŠÙ† {col1} Ùˆ {col2}"
                            )
                            patterns.append(pattern)
            
            return patterns
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø·Ø§Øª: {e}")
            return []

    async def _select_best_model(
        self, 
        data: Dict[str, Any], 
        request: PredictionRequest, 
        patterns: List[PatternAnalysis]
    ) -> str:
        """Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ØªÙ†Ø¨Ø¤"""
        try:
            df = data['dataframe']
            prediction_type = request.prediction_type
            complexity = request.model_complexity
            
            # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù†ÙˆØ¹ Ø§Ù„ØªÙ†Ø¨Ø¤
            if prediction_type == PredictionType.TIME_SERIES:
                # Ù„Ù„Ø³Ù„Ø§Ø³Ù„ Ø§Ù„Ø²Ù…Ù†ÙŠØ©
                if any(p.pattern_type == DataPattern.SEASONAL for p in patterns):
                    return 'arima' if TIMESERIES_AVAILABLE else 'gradient_boosting'
                elif any(p.pattern_type == DataPattern.TRENDING for p in patterns):
                    return 'linear_regression'
                else:
                    return 'random_forest'
                    
            elif prediction_type == PredictionType.CLASSIFICATION:
                if complexity == ModelComplexity.SIMPLE:
                    return 'linear_regression'  # Ù„Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø¨Ø³ÙŠØ·
                elif complexity == ModelComplexity.COMPLEX:
                    return 'xgboost' if BOOSTING_AVAILABLE else 'gradient_boosting'
                else:
                    return 'random_forest'
                    
            elif prediction_type == PredictionType.REGRESSION:
                if any(p.pattern_type == DataPattern.LINEAR for p in patterns):
                    return 'linear_regression'
                elif complexity == ModelComplexity.COMPLEX:
                    return 'xgboost' if BOOSTING_AVAILABLE else 'gradient_boosting'
                else:
                    return 'random_forest'
                    
            elif prediction_type == PredictionType.CLUSTERING:
                return 'kmeans'
                
            elif prediction_type == PredictionType.ANOMALY_DETECTION:
                return 'isolation_forest'
            
            # Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ
            return 'random_forest'
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {e}")
            return 'random_forest'

    async def _execute_prediction(
        self, 
        model_name: str, 
        data: Dict[str, Any], 
        request: PredictionRequest
    ) -> np.ndarray:
        """ØªÙ†ÙÙŠØ° Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø­Ø¯Ø¯"""
        try:
            df = data['dataframe']
            target_variable = data['target_variable']
            
            if model_name == 'arima' and TIMESERIES_AVAILABLE:
                return await self._predict_with_arima(df, target_variable, request)
            
            # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©
            if target_variable and target_variable in df.columns:
                X = df.drop(columns=[target_variable])
                y = df[target_variable]
            else:
                X = df
                y = None
            
            model = self.models[model_name]
            
            # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…Ø¯Ø±Ø¨Ø§Ù‹
            if hasattr(model, 'fit') and y is not None:
                model.fit(X, y)
            
            # Ø§Ù„ØªÙ†Ø¨Ø¤
            if request.prediction_type == PredictionType.CLUSTERING:
                predictions = model.fit_predict(X)
            elif hasattr(model, 'predict'):
                if request.prediction_horizon > len(X):
                    # Ù„Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØŒ Ù†Ø³ØªØ®Ø¯Ù… Ø¢Ø®Ø± Ø§Ù„Ù‚ÙŠÙ…
                    last_values = X.tail(1)
                    predictions = []
                    for _ in range(request.prediction_horizon):
                        pred = model.predict(last_values)[0]
                        predictions.append(pred)
                        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù‚ÙŠÙ… Ù„Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„ØªØ§Ù„ÙŠ (Ø¨Ø³ÙŠØ·)
                        last_values.iloc[0, -1] = pred
                    predictions = np.array(predictions)
                else:
                    predictions = model.predict(X[:request.prediction_horizon])
            else:
                predictions = np.zeros(request.prediction_horizon)
            
            return np.array(predictions)
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªÙ†ÙÙŠØ° Ø§Ù„ØªÙ†Ø¨Ø¤: {e}")
            return np.zeros(request.prediction_horizon)

    async def _predict_with_arima(
        self, 
        df: pd.DataFrame, 
        target_variable: str, 
        request: PredictionRequest
    ) -> np.ndarray:
        """Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ ARIMA"""
        try:
            series = df[target_variable]
            
            # ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ ARIMA
            model = ARIMA(series, order=(1, 1, 1))
            fitted_model = model.fit()
            
            # Ø§Ù„ØªÙ†Ø¨Ø¤
            forecast = fitted_model.forecast(steps=request.prediction_horizon)
            
            return np.array(forecast)
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ù€ ARIMA: {e}")
            return np.zeros(request.prediction_horizon)

    async def _calculate_confidence_intervals(
        self, 
        predictions: np.ndarray, 
        data: Dict[str, Any], 
        request: PredictionRequest
    ) -> List[Tuple[float, float]]:
        """Ø­Ø³Ø§Ø¨ ÙØªØ±Ø§Øª Ø§Ù„Ø«Ù‚Ø© Ù„Ù„ØªÙ†Ø¨Ø¤Ø§Øª"""
        try:
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ©
            df = data['dataframe']
            target_variable = data['target_variable']
            
            if target_variable and target_variable in df.columns:
                std = df[target_variable].std()
            else:
                std = np.std(predictions)
            
            # Ø­Ø³Ø§Ø¨ ÙØªØ±Ø§Øª Ø§Ù„Ø«Ù‚Ø©
            confidence_level = request.confidence_level
            z_score = 1.96 if confidence_level == 0.95 else 2.58  # Ù„Ù„Ø«Ù‚Ø© 95% Ø£Ùˆ 99%
            
            margin = z_score * std
            
            intervals = []
            for pred in predictions:
                lower = pred - margin
                upper = pred + margin
                intervals.append((lower, upper))
            
            return intervals
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø­Ø³Ø§Ø¨ ÙØªØ±Ø§Øª Ø§Ù„Ø«Ù‚Ø©: {e}")
            return [(p, p) for p in predictions]

    async def _detect_anomalies(self, data: Dict[str, Any], predictions: np.ndarray) -> List[int]:
        """ÙƒØ´Ù Ø§Ù„Ø´Ø°ÙˆØ° ÙÙŠ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª"""
        try:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… IQR Ù„ÙƒØ´Ù Ø§Ù„Ø´Ø°ÙˆØ°
            Q1 = np.percentile(predictions, 25)
            Q3 = np.percentile(predictions, 75)
            IQR = Q3 - Q1
            
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            anomalies = []
            for i, pred in enumerate(predictions):
                if pred < lower_bound or pred > upper_bound:
                    anomalies.append(i)
            
            return anomalies
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ÙƒØ´Ù Ø§Ù„Ø´Ø°ÙˆØ°: {e}")
            return []

    async def _analyze_feature_importance(
        self, 
        model_name: str, 
        data: Dict[str, Any]
    ) -> Optional[Dict[str, float]]:
        """ØªØ­Ù„ÙŠÙ„ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ù…ÙŠØ²Ø§Øª"""
        try:
            model = self.models[model_name]
            features = data['features']
            
            if hasattr(model, 'feature_importances_'):
                importances = model.feature_importances_
                return dict(zip(features, importances))
            elif hasattr(model, 'coef_'):
                coefficients = np.abs(model.coef_)
                return dict(zip(features, coefficients))
            
            return None
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ù…ÙŠØ²Ø§Øª: {e}")
            return None

    async def _generate_recommendations(
        self, 
        predictions: np.ndarray, 
        patterns: List[PatternAnalysis], 
        anomalies: List[int], 
        request: PredictionRequest
    ) -> List[str]:
        """Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª"""
        try:
            recommendations = []
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§ØªØ¬Ø§Ù‡ Ø§Ù„Ø¹Ø§Ù…
            if len(predictions) > 1:
                trend = np.mean(np.diff(predictions))
                if trend > 0:
                    recommendations.append("Ø§Ù„Ø§ØªØ¬Ø§Ù‡ Ø§Ù„Ø¹Ø§Ù… ØµØ§Ø¹Ø¯ - Ù‚Ø¯ ØªÙƒÙˆÙ† Ù‡Ù†Ø§Ùƒ ÙØ±ØµØ© Ù„Ù„Ù†Ù…Ùˆ")
                elif trend < 0:
                    recommendations.append("Ø§Ù„Ø§ØªØ¬Ø§Ù‡ Ø§Ù„Ø¹Ø§Ù… Ù‡Ø§Ø¨Ø· - ÙŠÙÙ†ØµØ­ Ø¨Ø§Ù„Ø­Ø°Ø± ÙˆØ§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©")
            
            # ØªÙˆØµÙŠØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…ÙƒØªØ´ÙØ©
            for pattern in patterns:
                if pattern.pattern_type == DataPattern.SEASONAL:
                    recommendations.append("ØªÙ… Ø§ÙƒØªØ´Ø§Ù Ù†Ù…Ø· Ù…ÙˆØ³Ù…ÙŠ - Ø®Ø·Ø· Ù„Ù„ØªØºÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ³Ù…ÙŠØ©")
                elif pattern.pattern_type == DataPattern.CYCLIC:
                    recommendations.append("Ù†Ù…Ø· Ø¯ÙˆØ±ÙŠ Ù…ÙƒØªØ´Ù - ØªÙˆÙ‚Ø¹ ØªÙƒØ±Ø§Ø± Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…Ø·")
                elif pattern.pattern_type == DataPattern.TRENDING:
                    if pattern.trend_direction == "ØµØ§Ø¹Ø¯":
                        recommendations.append("Ø§ØªØ¬Ø§Ù‡ Ù†Ù…Ùˆ Ù‚ÙˆÙŠ - Ø§Ø³ØªØ«Ù…Ø± ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„")
                    else:
                        recommendations.append("Ø§ØªØ¬Ø§Ù‡ Ù‡Ø¨ÙˆØ· - Ù‚Ø¯ ØªØ­ØªØ§Ø¬ Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© ØªØµØ­ÙŠØ­ÙŠØ©")
            
            # ØªÙˆØµÙŠØ§Øª Ù„Ù„Ø´Ø°ÙˆØ°
            if anomalies:
                recommendations.append(f"ØªÙ… Ø§ÙƒØªØ´Ø§Ù {len(anomalies)} Ù‚ÙŠÙ…Ø© Ø´Ø§Ø°Ø© - ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
            
            # ØªÙˆØµÙŠØ§Øª Ø¹Ø§Ù…Ø©
            if np.std(predictions) > np.mean(predictions) * 0.3:
                recommendations.append("ØªÙ‚Ù„Ø¨Ø§Øª Ø¹Ø§Ù„ÙŠØ© Ù…ØªÙˆÙ‚Ø¹Ø© - Ø®Ø·Ø· Ù„Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ø®Ø§Ø·Ø±")
            
            return recommendations[:5]  # Ø£Ù‚ØµÙ‰ 5 ØªÙˆØµÙŠØ§Øª
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙˆØµÙŠØ§Øª: {e}")
            return ["ØªØ¹Ø°Ø± Ø¥Ù†Ø´Ø§Ø¡ ØªÙˆØµÙŠØ§Øª"]

    async def _evaluate_model_performance(
        self, 
        model_name: str, 
        data: Dict[str, Any], 
        request: PredictionRequest
    ) -> Dict[str, float]:
        """ØªÙ‚ÙŠÙŠÙ… Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        try:
            performance = {}
            df = data['dataframe']
            target_variable = data['target_variable']
            
            if target_variable and target_variable in df.columns:
                model = self.models[model_name]
                X = df.drop(columns=[target_variable])
                y = df[target_variable]
                
                # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
                if len(X) > 10:
                    X_train, X_test, y_train, y_test = train_test_split(
                        X, y, test_size=0.2, random_state=42
                    )
                    
                    # ØªØ¯Ø±ÙŠØ¨ ÙˆØªÙ‚ÙŠÙŠÙ…
                    if hasattr(model, 'fit'):
                        model.fit(X_train, y_train)
                        y_pred = model.predict(X_test)
                        
                        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³
                        performance['mse'] = float(mean_squared_error(y_test, y_pred))
                        performance['rmse'] = float(np.sqrt(performance['mse']))
                        performance['r2'] = float(r2_score(y_test, y_pred))
                        
                        # Ø¯Ù‚Ø© Ø¥Ø¶Ø§ÙÙŠØ© Ù„Ù„ØªØµÙ†ÙŠÙ
                        if request.prediction_type == PredictionType.CLASSIFICATION:
                            y_pred_class = np.round(y_pred)
                            performance['accuracy'] = float(accuracy_score(y_test, y_pred_class))
            
            performance['model_name'] = model_name
            performance['evaluation_time'] = datetime.now().isoformat()
            
            return performance
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø£Ø¯Ø§Ø¡: {e}")
            return {'error': str(e)}

    def _generate_cache_key(self, request: PredictionRequest) -> str:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙØªØ§Ø­ Ù„Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ©"""
        try:
            # Ø¥Ù†Ø´Ø§Ø¡ hash Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø©
            key_data = {
                'prediction_type': request.prediction_type.value,
                'target_variable': request.target_variable,
                'prediction_horizon': request.prediction_horizon,
                'model_complexity': request.model_complexity.value,
                'features': sorted(request.features) if request.features else []
            }
            
            key_string = json.dumps(key_data, sort_keys=True)
            return hashlib.md5(key_string.encode()).hexdigest()
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙØªØ§Ø­ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ©: {e}")
            return f"key_{datetime.now().timestamp()}"

    async def get_prediction_statistics(self) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªÙ†Ø¨Ø¤"""
        try:
            stats = {
                'total_requests': len(self.request_history),
                'model_usage': dict(self.model_usage_stats),
                'cache_size': len(self.prediction_cache),
                'performance_metrics': self.performance_metrics,
                'prediction_types_distribution': {},
                'average_processing_time': 0.0,
                'engine_status': 'running' if self.is_running else 'stopped'
            }
            
            # ØªÙˆØ²ÙŠØ¹ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„ØªÙ†Ø¨Ø¤
            type_counts = defaultdict(int)
            total_time = 0
            
            for request in self.request_history:
                type_counts[request.prediction_type.value] += 1
            
            stats['prediction_types_distribution'] = dict(type_counts)
            
            # Ù…ØªÙˆØ³Ø· ÙˆÙ‚Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù† Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø®Ø²Ù†Ø©
            if self.prediction_cache:
                total_time = sum(result.processing_time for result in self.prediction_cache.values())
                stats['average_processing_time'] = total_time / len(self.prediction_cache)
            
            return stats
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª: {e}")
            return {'error': str(e)}

    async def save_models(self):
        """Ø­ÙØ¸ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆØ§Ù„Ø­Ø§Ù„Ø©"""
        try:
            # Ø­ÙØ¸ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
            models_file = self.models_dir / "models.pkl"
            with open(models_file, 'wb') as f:
                pickle.dump(self.models, f)
            
            # Ø­ÙØ¸ Ù…Ø¹Ø§Ù„Ø¬Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            scalers_file = self.models_dir / "scalers.pkl"
            with open(scalers_file, 'wb') as f:
                pickle.dump(self.scalers, f)
            
            encoders_file = self.models_dir / "encoders.pkl"
            with open(encoders_file, 'wb') as f:
                pickle.dump(self.encoders, f)
            
            # Ø­ÙØ¸ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
            stats_file = self.models_dir / "statistics.json"
            stats = await self.get_prediction_statistics()
            with open(stats_file, 'w', encoding='utf-8') as f:
                json.dump(stats, f, ensure_ascii=False, indent=2, default=str)
            
            self.logger.info("ØªÙ… Ø­ÙØ¸ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªÙ†Ø¨Ø¤ ÙˆØ§Ù„Ø­Ø§Ù„Ø©")
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬: {e}")

    async def load_models(self):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆØ§Ù„Ø­Ø§Ù„Ø©"""
        try:
            # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
            models_file = self.models_dir / "models.pkl"
            if models_file.exists():
                with open(models_file, 'rb') as f:
                    self.models.update(pickle.load(f))
            
            # ØªØ­Ù…ÙŠÙ„ Ù…Ø¹Ø§Ù„Ø¬Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            scalers_file = self.models_dir / "scalers.pkl"
            if scalers_file.exists():
                with open(scalers_file, 'rb') as f:
                    self.scalers.update(pickle.load(f))
            
            encoders_file = self.models_dir / "encoders.pkl"
            if encoders_file.exists():
                with open(encoders_file, 'rb') as f:
                    self.encoders.update(pickle.load(f))
            
            self.logger.info("ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªÙ†Ø¨Ø¤ ÙˆØ§Ù„Ø­Ø§Ù„Ø©")
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬: {e}")

# Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø«ÙŠÙ„ Ø¹Ø§Ù…
smart_prediction_engine = SmartPredictionEngine()

async def get_prediction_engine() -> SmartPredictionEngine:
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø­Ø±Ùƒ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„Ø°ÙƒÙŠ"""
    return smart_prediction_engine

if __name__ == "__main__":
    async def test_prediction_engine():
        """Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø­Ø±Ùƒ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„Ø°ÙƒÙŠ"""
        print("ğŸ”® Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø­Ø±Ùƒ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…")
        print("=" * 60)
        
        engine = await get_prediction_engine()
        await engine.start_prediction_engine()
        
        # Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ© Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
        sample_data = {
            'dataframe': {
                'value': [10, 12, 11, 13, 15, 14, 16, 18, 17, 19, 21, 20],
                'feature1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],
                'feature2': [0.5, 0.7, 0.6, 0.8, 1.0, 0.9, 1.1, 1.3, 1.2, 1.4, 1.6, 1.5]
            }
        }
        
        # Ø·Ù„Ø¨ ØªÙ†Ø¨Ø¤
        request = PredictionRequest(
            request_id="test_001",
            prediction_type=PredictionType.TIME_SERIES,
            data=sample_data,
            target_variable='value',
            prediction_horizon=5,
            confidence_level=0.95,
            model_complexity=ModelComplexity.MODERATE
        )
        
        print("ğŸ“Š Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±:")
        print(f"  â€¢ Ù†ÙˆØ¹ Ø§Ù„ØªÙ†Ø¨Ø¤: {request.prediction_type.value}")
        print(f"  â€¢ Ø§Ù„Ù…ØªØºÙŠØ± Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù: {request.target_variable}")
        print(f"  â€¢ Ø£ÙÙ‚ Ø§Ù„ØªÙ†Ø¨Ø¤: {request.prediction_horizon}")
        print(f"  â€¢ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø©: {request.confidence_level}")
        
        # ØªÙ†ÙÙŠØ° Ø§Ù„ØªÙ†Ø¨Ø¤
        print("\nğŸ” ØªÙ†ÙÙŠØ° Ø§Ù„ØªÙ†Ø¨Ø¤...")
        result = await engine.predict(request)
        
        print(f"\nğŸ“ˆ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙ†Ø¨Ø¤:")
        print(f"  â€¢ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª: {[f'{p:.2f}' for p in result.predictions]}")
        print(f"  â€¢ ÙˆÙ‚Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {result.processing_time:.3f} Ø«Ø§Ù†ÙŠØ©")
        print(f"  â€¢ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…ÙƒØªØ´ÙØ©: {result.detected_patterns}")
        
        if result.confidence_intervals:
            print(f"  â€¢ ÙØªØ±Ø§Øª Ø§Ù„Ø«Ù‚Ø©:")
            for i, (lower, upper) in enumerate(result.confidence_intervals):
                print(f"    - Ø§Ù„ØªÙ†Ø¨Ø¤ {i+1}: [{lower:.2f}, {upper:.2f}]")
        
        if result.feature_importance:
            print(f"  â€¢ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ù…ÙŠØ²Ø§Øª:")
            for feature, importance in result.feature_importance.items():
                print(f"    - {feature}: {importance:.3f}")
        
        if result.anomalies:
            print(f"  â€¢ Ø§Ù„Ø´Ø°ÙˆØ° Ø§Ù„Ù…ÙƒØªØ´Ù: {result.anomalies}")
        
        print(f"\nğŸ’¡ Ø§Ù„ØªÙˆØµÙŠØ§Øª:")
        for recommendation in result.recommendations:
            print(f"  â€¢ {recommendation}")
        
        print(f"\nğŸ“Š Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡:")
        for metric, value in result.model_performance.items():
            print(f"  â€¢ {metric}: {value}")
        
        # Ø§Ø®ØªØ¨Ø§Ø± Ø£Ù†ÙˆØ§Ø¹ ØªÙ†Ø¨Ø¤ Ù…Ø®ØªÙ„ÙØ©
        print(f"\nğŸ¯ Ø§Ø®ØªØ¨Ø§Ø± Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©:")
        
        # ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„ØªØµÙ†ÙŠÙ
        classification_request = PredictionRequest(
            request_id="test_002",
            prediction_type=PredictionType.CLASSIFICATION,
            data=sample_data,
            target_variable='value',
            prediction_horizon=3
        )
        
        classification_result = await engine.predict(classification_request)
        print(f"  â€¢ Ø§Ù„ØªØµÙ†ÙŠÙ: {[f'{p:.2f}' for p in classification_result.predictions]}")
        
        # ÙƒØ´Ù Ø§Ù„Ø´Ø°ÙˆØ°
        anomaly_request = PredictionRequest(
            request_id="test_003",
            prediction_type=PredictionType.ANOMALY_DETECTION,
            data=sample_data,
            prediction_horizon=len(sample_data['dataframe']['value'])
        )
        
        anomaly_result = await engine.predict(anomaly_request)
        print(f"  â€¢ ÙƒØ´Ù Ø§Ù„Ø´Ø°ÙˆØ°: {anomaly_result.anomalies}")
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        print(f"\nğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ø­Ø±Ùƒ:")
        stats = await engine.get_prediction_statistics()
        print(f"  â€¢ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø·Ù„Ø¨Ø§Øª: {stats['total_requests']}")
        print(f"  â€¢ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬: {stats['model_usage']}")
        print(f"  â€¢ Ø­Ø¬Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ©: {stats['cache_size']}")
        print(f"  â€¢ Ù…ØªÙˆØ³Ø· ÙˆÙ‚Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {stats['average_processing_time']:.3f} Ø«Ø§Ù†ÙŠØ©")
        
        # Ø­ÙØ¸ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
        await engine.save_models()
        print(f"\nğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆØ§Ù„Ø­Ø§Ù„Ø©")
        
        # Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ù…Ø­Ø±Ùƒ
        await engine.stop_prediction_engine()
        print(f"\nâ¹ï¸ ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ù…Ø­Ø±Ùƒ Ø§Ù„ØªÙ†Ø¨Ø¤")
        
        print("\nâœ¨ Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø¨Ù†Ø¬Ø§Ø­!")

    # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
    asyncio.run(test_prediction_engine())
