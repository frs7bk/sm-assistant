
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ù…Ø­Ø±Ùƒ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø³ØªÙ…Ø± Ø§Ù„Ù…ØªÙ‚Ø¯Ù… ÙˆØ§Ù„Ø°ÙƒÙŠ
Advanced Continuous Learning Engine
"""

import asyncio
import logging
import numpy as np
import pandas as pd
import pickle
import json
import hashlib
from typing import Dict, List, Any, Optional, Tuple, Union, Callable
from pathlib import Path
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict, field
from enum import Enum
from collections import deque, defaultdict
import threading
import queue
import warnings
warnings.filterwarnings('ignore')

# Machine Learning
from sklearn.metrics import accuracy_score, mean_squared_error, r2_score
from sklearn.base import clone
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.linear_model import SGDClassifier, SGDRegressor
from sklearn.neural_network import MLPClassifier, MLPRegressor

# Advanced Libraries
try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import DataLoader, TensorDataset
    PYTORCH_AVAILABLE = True
except ImportError:
    PYTORCH_AVAILABLE = False
    logging.warning("PyTorch ØºÙŠØ± Ù…ØªÙˆÙØ± Ù„Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚")

try:
    from river import linear_model, tree, ensemble, metrics, preprocessing
    from river.drift import ADWIN, PageHinkley, KSWIN
    RIVER_AVAILABLE = True
except ImportError:
    RIVER_AVAILABLE = False
    logging.warning("River ØºÙŠØ± Ù…ØªÙˆÙØ± Ù„Ù„ØªØ¹Ù„Ù… Ø§Ù„ØªØ¯ÙÙ‚ÙŠ")

class LearningMode(Enum):
    """Ø£Ù†Ù…Ø§Ø· Ø§Ù„ØªØ¹Ù„Ù…"""
    INCREMENTAL = "incremental"  # ØªØ¹Ù„Ù… ØªØ¯Ø±ÙŠØ¬ÙŠ
    ONLINE = "online"  # ØªØ¹Ù„Ù… ÙÙˆØ±ÙŠ
    BATCH = "batch"  # ØªØ¹Ù„Ù… Ø¨Ø§Ù„Ø¯ÙØ¹Ø§Øª
    FEDERATED = "federated"  # ØªØ¹Ù„Ù… ÙÙŠØ¯Ø±Ø§Ù„ÙŠ
    TRANSFER = "transfer"  # ØªØ¹Ù„Ù… Ù…Ù†Ù‚ÙˆÙ„
    META = "meta"  # ØªØ¹Ù„Ù… ÙÙˆÙ‚ÙŠ
    REINFORCEMENT = "reinforcement"  # ØªØ¹Ù„Ù… ØªØ¹Ø²ÙŠØ²ÙŠ

class ConceptDriftType(Enum):
    """Ø£Ù†ÙˆØ§Ø¹ Ø§Ù†Ø­Ø±Ø§Ù Ø§Ù„Ù…ÙÙ‡ÙˆÙ…"""
    SUDDEN = "sudden"  # Ù…ÙØ§Ø¬Ø¦
    GRADUAL = "gradual"  # ØªØ¯Ø±ÙŠØ¬ÙŠ
    RECURRING = "recurring"  # Ù…ØªÙƒØ±Ø±
    INCREMENTAL = "incremental"  # ØªØ¯Ø±ÙŠØ¬ÙŠ Ù…Ø³ØªÙ…Ø±

class LearningTask(Enum):
    """Ù…Ù‡Ø§Ù… Ø§Ù„ØªØ¹Ù„Ù…"""
    CLASSIFICATION = "classification"
    REGRESSION = "regression"
    CLUSTERING = "clustering"
    ANOMALY_DETECTION = "anomaly_detection"
    PATTERN_RECOGNITION = "pattern_recognition"
    BEHAVIOR_MODELING = "behavior_modeling"

@dataclass
class LearningExample:
    """Ù…Ø«Ø§Ù„ Ø§Ù„ØªØ¹Ù„Ù…"""
    example_id: str
    data: Dict[str, Any]
    target: Any
    timestamp: datetime
    source: str = "unknown"
    weight: float = 1.0
    metadata: Dict[str, Any] = field(default_factory=dict)
    feedback_score: Optional[float] = None
    is_validated: bool = False

@dataclass
class LearningSession:
    """Ø¬Ù„Ø³Ø© Ø§Ù„ØªØ¹Ù„Ù…"""
    session_id: str
    start_time: datetime
    end_time: Optional[datetime] = None
    examples_count: int = 0
    performance_before: Dict[str, float] = field(default_factory=dict)
    performance_after: Dict[str, float] = field(default_factory=dict)
    drift_detected: bool = False
    adaptation_applied: bool = False
    learning_mode: LearningMode = LearningMode.INCREMENTAL
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class ModelCheckpoint:
    """Ù†Ù‚Ø·Ø© ØªÙØªÙŠØ´ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
    checkpoint_id: str
    model_state: bytes
    performance_metrics: Dict[str, float]
    timestamp: datetime
    examples_seen: int
    drift_score: float = 0.0
    confidence_score: float = 0.0
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class DriftDetectionResult:
    """Ù†ØªÙŠØ¬Ø© ÙƒØ´Ù Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù"""
    drift_detected: bool
    drift_type: Optional[ConceptDriftType]
    confidence: float
    severity: float  # 0-1
    affected_features: List[str] = field(default_factory=list)
    recommendation: str = ""
    timestamp: datetime = field(default_factory=datetime.now)

class AdaptiveNeuralNetwork(nn.Module):
    """Ø´Ø¨ÙƒØ© Ø¹ØµØ¨ÙŠØ© ØªÙƒÙŠÙÙŠØ©"""
    
    def __init__(self, input_size: int, hidden_sizes: List[int], output_size: int, task_type: str = "classification"):
        super().__init__()
        self.task_type = task_type
        self.layers = nn.ModuleList()
        self.dropout_layers = nn.ModuleList()
        
        # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª
        prev_size = input_size
        for hidden_size in hidden_sizes:
            self.layers.append(nn.Linear(prev_size, hidden_size))
            self.dropout_layers.append(nn.Dropout(0.2))
            prev_size = hidden_size
        
        # Ø·Ø¨Ù‚Ø© Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬
        self.output_layer = nn.Linear(prev_size, output_size)
        
        # Ø·Ø¨Ù‚Ø§Øª Ø§Ù„ØªØ·Ø¨ÙŠØ¹
        self.batch_norms = nn.ModuleList([
            nn.BatchNorm1d(hidden_size) for hidden_size in hidden_sizes
        ])
        
    def forward(self, x):
        for i, layer in enumerate(self.layers):
            x = layer(x)
            if x.size(0) > 1:  # ØªØ·Ø¨ÙŠÙ‚ BatchNorm ÙÙ‚Ø· Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ Ø£ÙƒØ«Ø± Ù…Ù† Ø¹ÙŠÙ†Ø©
                x = self.batch_norms[i](x)
            x = torch.relu(x)
            x = self.dropout_layers[i](x)
        
        x = self.output_layer(x)
        
        if self.task_type == "classification":
            x = torch.softmax(x, dim=1)
        
        return x

class ContinuousLearningEngine:
    """Ù…Ø­Ø±Ùƒ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø³ØªÙ…Ø± Ø§Ù„Ù…ØªÙ‚Ø¯Ù…"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.logger = logging.getLogger(__name__)
        self.config = config or {}
        
        # Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        self.models: Dict[str, Any] = {}
        self.model_performance: Dict[str, Dict[str, float]] = {}
        self.model_checkpoints: Dict[str, List[ModelCheckpoint]] = {}
        
        # ÙƒØ´Ù Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù
        self.drift_detectors: Dict[str, Any] = {}
        self.drift_history: List[DriftDetectionResult] = []
        
        # Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        self.data_buffer = deque(maxlen=self.config.get('buffer_size', 10000))
        self.validation_buffer = deque(maxlen=self.config.get('validation_buffer_size', 1000))
        self.feedback_buffer = deque(maxlen=self.config.get('feedback_buffer_size', 5000))
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        self.scalers: Dict[str, StandardScaler] = {}
        self.encoders: Dict[str, LabelEncoder] = {}
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªØ¹Ù„Ù…
        self.learning_sessions: List[LearningSession] = []
        self.examples_processed = 0
        self.adaptations_count = 0
        self.last_adaptation_time = None
        
        # Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ø£Ø¯Ø§Ø¡
        self.performance_thresholds = {
            'accuracy_drop': self.config.get('accuracy_drop_threshold', 0.05),
            'drift_confidence': self.config.get('drift_confidence_threshold', 0.8),
            'adaptation_frequency': self.config.get('max_adaptations_per_hour', 5)
        }
        
        # Ø®ÙŠÙˆØ· Ø§Ù„ØªÙ†ÙÙŠØ°
        self.learning_queue = queue.Queue()
        self.processing_thread = None
        self.is_running = False
        
        # Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ø­ÙØ¸
        self.models_dir = Path("data/continuous_learning")
        self.models_dir.mkdir(parents=True, exist_ok=True)
        
        self._initialize_drift_detectors()
        self.logger.info("ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ø±Ùƒ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø³ØªÙ…Ø± Ø§Ù„Ù…ØªÙ‚Ø¯Ù…")

    def _initialize_drift_detectors(self):
        """ØªÙ‡ÙŠØ¦Ø© ÙƒØ§Ø´ÙØ§Øª Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù"""
        try:
            if RIVER_AVAILABLE:
                self.drift_detectors['adwin'] = ADWIN(delta=0.01)
                self.drift_detectors['page_hinkley'] = PageHinkley(min_instances=30, delta=0.005, threshold=50)
                self.drift_detectors['kswin'] = KSWIN(alpha=0.005, window_size=100, stat_size=30)
            else:
                # ÙƒØ§Ø´Ù Ø§Ù†Ø­Ø±Ø§Ù Ø¨Ø³ÙŠØ· Ù…Ø®ØµØµ
                self.drift_detectors['simple'] = SimpleDriftDetector()
                
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªÙ‡ÙŠØ¦Ø© ÙƒØ§Ø´ÙØ§Øª Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù: {e}")

    async def start_continuous_learning(self):
        """Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø³ØªÙ…Ø±"""
        try:
            self.is_running = True
            self.processing_thread = threading.Thread(target=self._continuous_learning_loop)
            self.processing_thread.start()
            self.logger.info("ØªÙ… Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø³ØªÙ…Ø±")
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø³ØªÙ…Ø±: {e}")
            raise

    async def stop_continuous_learning(self):
        """Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø³ØªÙ…Ø±"""
        try:
            self.is_running = False
            if self.processing_thread and self.processing_thread.is_alive():
                self.processing_thread.join(timeout=5)
            
            # Ø­ÙØ¸ Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©
            await self.save_learning_state()
            self.logger.info("ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø³ØªÙ…Ø±")
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø³ØªÙ…Ø±: {e}")

    def _continuous_learning_loop(self):
        """Ø­Ù„Ù‚Ø© Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø³ØªÙ…Ø±"""
        while self.is_running:
            try:
                # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø·Ø§Ø¨ÙˆØ±
                try:
                    example = self.learning_queue.get(timeout=1)
                    asyncio.create_task(self._process_learning_example(example))
                except queue.Empty:
                    continue
                
                # ÙØ­Øµ Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù Ø¨Ø´ÙƒÙ„ Ø¯ÙˆØ±ÙŠ
                if self.examples_processed % 100 == 0:
                    asyncio.create_task(self._check_concept_drift())
                
                # ØªØ­ÙÙŠØ¸ Ù†Ù‚Ø§Ø· Ø§Ù„ØªÙØªÙŠØ´
                if self.examples_processed % 1000 == 0:
                    asyncio.create_task(self._create_checkpoint())
                
            except Exception as e:
                self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø­Ù„Ù‚Ø© Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø³ØªÙ…Ø±: {e}")

    async def add_learning_example(
        self,
        data: Dict[str, Any],
        target: Any,
        source: str = "user_interaction",
        weight: float = 1.0,
        immediate_learning: bool = False
    ) -> str:
        """Ø¥Ø¶Ø§ÙØ© Ù…Ø«Ø§Ù„ ØªØ¹Ù„Ù… Ø¬Ø¯ÙŠØ¯"""
        try:
            example_id = hashlib.md5(f"{data}_{target}_{datetime.now()}".encode()).hexdigest()[:12]
            
            example = LearningExample(
                example_id=example_id,
                data=data,
                target=target,
                timestamp=datetime.now(),
                source=source,
                weight=weight
            )
            
            # Ø¥Ø¶Ø§ÙØ© Ù„Ù„Ù…Ø®Ø²Ù† Ø§Ù„Ù…Ø¤Ù‚Øª
            self.data_buffer.append(example)
            
            if immediate_learning:
                await self._process_learning_example(example)
            else:
                self.learning_queue.put(example)
            
            self.logger.debug(f"ØªÙ… Ø¥Ø¶Ø§ÙØ© Ù…Ø«Ø§Ù„ Ø§Ù„ØªØ¹Ù„Ù…: {example_id}")
            return example_id
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ø¶Ø§ÙØ© Ù…Ø«Ø§Ù„ Ø§Ù„ØªØ¹Ù„Ù…: {e}")
            raise

    async def _process_learning_example(self, example: LearningExample):
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø«Ø§Ù„ Ø§Ù„ØªØ¹Ù„Ù…"""
        try:
            # ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ù…Ù‡Ù…Ø©
            task_type = self._determine_task_type(example.target)
            model_key = f"continuous_{task_type}"
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
            if model_key not in self.models:
                await self._initialize_model(model_key, task_type, example.data)
            
            model = self.models[model_key]
            
            # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            X, y = await self._prepare_data([example])
            
            # Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¬ÙŠ
            if hasattr(model, 'partial_fit'):
                # Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªÙŠ ØªØ¯Ø¹Ù… Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¬ÙŠ
                if task_type == "classification" and hasattr(model, 'classes_'):
                    model.partial_fit(X, y, classes=model.classes_)
                else:
                    model.partial_fit(X, y)
            elif PYTORCH_AVAILABLE and isinstance(model, nn.Module):
                # Ù„Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ©
                await self._train_neural_network_online(model, X, y)
            else:
                # Ø¥Ø¹Ø§Ø¯Ø© ØªØ¯Ø±ÙŠØ¨ ÙƒØ§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø®Ø²Ù†Ø©
                await self._retrain_model_with_buffer(model_key, task_type)
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
            self.examples_processed += 1
            
            # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø£Ø¯Ø§Ø¡
            if self.examples_processed % 50 == 0:
                await self._evaluate_model_performance(model_key, task_type)
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø«Ø§Ù„ Ø§Ù„ØªØ¹Ù„Ù…: {e}")

    def _determine_task_type(self, target: Any) -> str:
        """ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ù…Ù‡Ù…Ø©"""
        if isinstance(target, (int, float)):
            if isinstance(target, int) and target in [0, 1]:
                return "classification"
            elif isinstance(target, int) and target < 10:
                return "classification"
            else:
                return "regression"
        elif isinstance(target, str):
            return "classification"
        else:
            return "classification"

    async def _initialize_model(self, model_key: str, task_type: str, sample_data: Dict[str, Any]):
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        try:
            input_size = len(sample_data)
            
            if PYTORCH_AVAILABLE and self.config.get('use_neural_networks', True):
                # Ø´Ø¨ÙƒØ© Ø¹ØµØ¨ÙŠØ© ØªÙƒÙŠÙÙŠØ©
                if task_type == "classification":
                    output_size = self.config.get('num_classes', 10)
                    model = AdaptiveNeuralNetwork(
                        input_size=input_size,
                        hidden_sizes=[64, 32],
                        output_size=output_size,
                        task_type=task_type
                    )
                else:
                    model = AdaptiveNeuralNetwork(
                        input_size=input_size,
                        hidden_sizes=[64, 32],
                        output_size=1,
                        task_type=task_type
                    )
                
                # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø­Ø³Ù†
                model.optimizer = optim.Adam(model.parameters(), lr=0.001)
                model.criterion = nn.CrossEntropyLoss() if task_type == "classification" else nn.MSELoss()
                
            else:
                # Ù†Ù…Ø§Ø°Ø¬ sklearn Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠØ©
                if task_type == "classification":
                    model = SGDClassifier(random_state=42, loss='log')
                else:
                    model = SGDRegressor(random_state=42)
            
            self.models[model_key] = model
            self.model_performance[model_key] = {}
            self.model_checkpoints[model_key] = []
            
            self.logger.info(f"ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {model_key}")
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {e}")
            raise

    async def _prepare_data(self, examples: List[LearningExample]) -> Tuple[np.ndarray, np.ndarray]:
        """ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªØ¯Ø±ÙŠØ¨"""
        try:
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            X_list = []
            y_list = []
            
            for example in examples:
                # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Ù…ØªØ¬Ù‡
                feature_vector = []
                for key, value in example.data.items():
                    if isinstance(value, (int, float)):
                        feature_vector.append(value)
                    elif isinstance(value, str):
                        # ØªØ±Ù…ÙŠØ² Ø§Ù„Ù†ØµÙˆØµ
                        if key not in self.encoders:
                            self.encoders[key] = LabelEncoder()
                            # ØªÙ‡ÙŠØ¦Ø© Ø¨Ù‚ÙŠÙ… ÙˆÙ‡Ù…ÙŠØ©
                            self.encoders[key].fit([value, "unknown"])
                        
                        try:
                            encoded_value = self.encoders[key].transform([value])[0]
                        except ValueError:
                            # Ù‚ÙŠÙ…Ø© Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù… Ù†Ø±Ù‡Ø§ Ù…Ù† Ù‚Ø¨Ù„
                            encoded_value = len(self.encoders[key].classes_)
                        
                        feature_vector.append(encoded_value)
                    else:
                        feature_vector.append(0)  # Ù‚ÙŠÙ…Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
                
                X_list.append(feature_vector)
                y_list.append(example.target)
            
            X = np.array(X_list)
            y = np.array(y_list)
            
            # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            scaler_key = "feature_scaler"
            if scaler_key not in self.scalers:
                self.scalers[scaler_key] = StandardScaler()
                X = self.scalers[scaler_key].fit_transform(X)
            else:
                X = self.scalers[scaler_key].transform(X)
            
            return X, y
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
            raise

    async def _train_neural_network_online(self, model: nn.Module, X: np.ndarray, y: np.ndarray):
        """ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ø¨Ø´ÙƒÙ„ ÙÙˆØ±ÙŠ"""
        try:
            model.train()
            
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ tensors
            X_tensor = torch.FloatTensor(X)
            
            if model.task_type == "classification":
                y_tensor = torch.LongTensor(y)
            else:
                y_tensor = torch.FloatTensor(y).unsqueeze(1)
            
            # Ø§Ù„ØªØ¯Ø±ÙŠØ¨
            model.optimizer.zero_grad()
            outputs = model(X_tensor)
            loss = model.criterion(outputs, y_tensor)
            loss.backward()
            model.optimizer.step()
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ©: {e}")

    async def _retrain_model_with_buffer(self, model_key: str, task_type: str):
        """Ø¥Ø¹Ø§Ø¯Ø© ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø®Ø²Ù†Ø©"""
        try:
            if len(self.data_buffer) < 10:
                return  # Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ©
            
            # Ø£Ø®Ø° Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø®Ø²Ù†Ø©
            recent_examples = list(self.data_buffer)[-1000:]  # Ø¢Ø®Ø± 1000 Ù…Ø«Ø§Ù„
            
            X, y = await self._prepare_data(recent_examples)
            
            # Ø¥Ø¹Ø§Ø¯Ø© ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            model = self.models[model_key]
            
            if PYTORCH_AVAILABLE and isinstance(model, nn.Module):
                # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ©
                dataset = TensorDataset(torch.FloatTensor(X), torch.LongTensor(y) if task_type == "classification" else torch.FloatTensor(y))
                dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
                
                model.train()
                for epoch in range(5):  # ØªØ¯Ø±ÙŠØ¨ Ø³Ø±ÙŠØ¹
                    for batch_X, batch_y in dataloader:
                        model.optimizer.zero_grad()
                        outputs = model(batch_X)
                        loss = model.criterion(outputs, batch_y)
                        loss.backward()
                        model.optimizer.step()
            else:
                # Ø¥Ø¹Ø§Ø¯Ø© ØªØ¯Ø±ÙŠØ¨ sklearn
                model.fit(X, y)
            
            self.logger.info(f"ØªÙ… Ø¥Ø¹Ø§Ø¯Ø© ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {model_key}")
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ø¹Ø§Ø¯Ø© ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {e}")

    async def _check_concept_drift(self) -> DriftDetectionResult:
        """ÙØ­Øµ Ø§Ù†Ø­Ø±Ø§Ù Ø§Ù„Ù…ÙÙ‡ÙˆÙ…"""
        try:
            if len(self.validation_buffer) < 50:
                return DriftDetectionResult(drift_detected=False, confidence=0.0, severity=0.0)
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø­Ø§Ù„ÙŠ
            recent_examples = list(self.validation_buffer)[-50:]
            performance_scores = []
            
            for example in recent_examples:
                # Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ù„ØªÙ†Ø¨Ø¤ ÙˆØ­Ø³Ø§Ø¨ Ø§Ù„Ø¯Ù‚Ø©
                predicted = await self._predict_single(example.data)
                actual = example.target
                
                if isinstance(actual, str) or isinstance(predicted, str):
                    score = 1.0 if predicted == actual else 0.0
                else:
                    score = 1.0 - abs(predicted - actual) / max(abs(actual), 1.0)
                
                performance_scores.append(score)
            
            current_performance = np.mean(performance_scores)
            
            # Ù…Ù‚Ø§Ø±Ù†Ø© Ù…Ø¹ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠ
            if hasattr(self, 'historical_performance'):
                performance_drop = self.historical_performance - current_performance
                
                drift_detected = performance_drop > self.performance_thresholds['accuracy_drop']
                confidence = min(performance_drop / self.performance_thresholds['accuracy_drop'], 1.0)
                severity = performance_drop
                
                if drift_detected:
                    # ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù
                    drift_type = self._classify_drift_type(performance_scores)
                    
                    result = DriftDetectionResult(
                        drift_detected=True,
                        drift_type=drift_type,
                        confidence=confidence,
                        severity=severity,
                        recommendation="ÙŠÙˆØµÙ‰ Ø¨Ø¥Ø¹Ø§Ø¯Ø© ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø£Ùˆ ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"
                    )
                    
                    self.drift_history.append(result)
                    
                    # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªÙƒÙŠÙ
                    await self._apply_adaptation(result)
                    
                    return result
            else:
                self.historical_performance = current_performance
            
            return DriftDetectionResult(drift_detected=False, confidence=0.0, severity=0.0)
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ÙØ­Øµ Ø§Ù†Ø­Ø±Ø§Ù Ø§Ù„Ù…ÙÙ‡ÙˆÙ…: {e}")
            return DriftDetectionResult(drift_detected=False, confidence=0.0, severity=0.0)

    def _classify_drift_type(self, performance_scores: List[float]) -> ConceptDriftType:
        """ØªØµÙ†ÙŠÙ Ù†ÙˆØ¹ Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù"""
        try:
            # ØªØ­Ù„ÙŠÙ„ Ù†Ù…Ø· Ø§Ù„Ø£Ø¯Ø§Ø¡
            scores_array = np.array(performance_scores)
            
            # ÙØ­Øµ Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù Ø§Ù„Ù…ÙØ§Ø¬Ø¦
            diff = np.diff(scores_array)
            if np.any(np.abs(diff) > 0.3):
                return ConceptDriftType.SUDDEN
            
            # ÙØ­Øµ Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù Ø§Ù„ØªØ¯Ø±ÙŠØ¬ÙŠ
            if len(scores_array) > 10:
                first_half = scores_array[:len(scores_array)//2]
                second_half = scores_array[len(scores_array)//2:]
                
                if np.mean(first_half) - np.mean(second_half) > 0.1:
                    return ConceptDriftType.GRADUAL
            
            # ÙØ­Øµ Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ù…ØªÙƒØ±Ø±
            if len(self.drift_history) > 3:
                recent_drifts = self.drift_history[-3:]
                if all(d.drift_detected for d in recent_drifts):
                    return ConceptDriftType.RECURRING
            
            return ConceptDriftType.INCREMENTAL
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØµÙ†ÙŠÙ Ù†ÙˆØ¹ Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù: {e}")
            return ConceptDriftType.GRADUAL

    async def _apply_adaptation(self, drift_result: DriftDetectionResult):
        """ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªÙƒÙŠÙ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù"""
        try:
            # ÙØ­Øµ Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªÙƒÙŠÙ
            if (self.last_adaptation_time and 
                (datetime.now() - self.last_adaptation_time).total_seconds() < 3600 / self.performance_thresholds['adaptation_frequency']):
                self.logger.info("ØªÙ… ØªØ®Ø·ÙŠ Ø§Ù„ØªÙƒÙŠÙ Ø¨Ø³Ø¨Ø¨ Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªÙƒÙŠÙ Ø§Ù„Ù…Ø±ØªÙØ¹")
                return
            
            # Ø§Ø®ØªÙŠØ§Ø± Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„ØªÙƒÙŠÙ
            if drift_result.severity > 0.3:
                # Ø§Ù†Ø­Ø±Ø§Ù Ø´Ø¯ÙŠØ¯ - Ø¥Ø¹Ø§Ø¯Ø© ØªØ¯Ø±ÙŠØ¨ ÙƒØ§Ù…Ù„
                await self._full_retrain_adaptation()
            elif drift_result.severity > 0.1:
                # Ø§Ù†Ø­Ø±Ø§Ù Ù…ØªÙˆØ³Ø· - ØªØ­Ø¯ÙŠØ« ØªØ¯Ø±ÙŠØ¬ÙŠ
                await self._incremental_adaptation()
            else:
                # Ø§Ù†Ø­Ø±Ø§Ù Ø®ÙÙŠÙ - Ø¶Ø¨Ø· Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
                await self._parameter_adjustment()
            
            self.adaptations_count += 1
            self.last_adaptation_time = datetime.now()
            
            self.logger.info(f"ØªÙ… ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªÙƒÙŠÙ Ù„Ù„Ø§Ù†Ø­Ø±Ø§Ù: {drift_result.drift_type}")
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªÙƒÙŠÙ: {e}")

    async def _full_retrain_adaptation(self):
        """Ø¥Ø¹Ø§Ø¯Ø© ØªØ¯Ø±ÙŠØ¨ ÙƒØ§Ù…Ù„ Ù„Ù„Ù†Ù…Ø§Ø°Ø¬"""
        try:
            for model_key in self.models.keys():
                # Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù‚Ø·Ø© ØªÙØªÙŠØ´ Ù‚Ø¨Ù„ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨
                await self._create_checkpoint(model_key)
                
                # ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ù…Ù‡Ù…Ø©
                task_type = model_key.split('_')[1]
                
                # Ø¥Ø¹Ø§Ø¯Ø© ØªØ¯Ø±ÙŠØ¨ Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ø¯ÙŠØ«Ø©
                await self._retrain_model_with_buffer(model_key, task_type)
                
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„ÙƒØ§Ù…Ù„: {e}")

    async def _incremental_adaptation(self):
        """ØªÙƒÙŠÙ ØªØ¯Ø±ÙŠØ¬ÙŠ"""
        try:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ø¯ÙŠØ«Ø© Ù„Ù„ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªØ¯Ø±ÙŠØ¬ÙŠ
            recent_examples = list(self.data_buffer)[-100:]
            
            for model_key, model in self.models.items():
                if hasattr(model, 'partial_fit') and recent_examples:
                    X, y = await self._prepare_data(recent_examples)
                    
                    if hasattr(model, 'classes_'):
                        model.partial_fit(X, y, classes=model.classes_)
                    else:
                        model.partial_fit(X, y)
                        
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªÙƒÙŠÙ Ø§Ù„ØªØ¯Ø±ÙŠØ¬ÙŠ: {e}")

    async def _parameter_adjustment(self):
        """Ø¶Ø¨Ø· Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª"""
        try:
            # Ø¶Ø¨Ø· Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„ØªØ¹Ù„Ù… Ù„Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ©
            for model in self.models.values():
                if PYTORCH_AVAILABLE and isinstance(model, nn.Module) and hasattr(model, 'optimizer'):
                    # ØªÙ‚Ù„ÙŠÙ„ Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù… Ù‚Ù„ÙŠÙ„Ø§Ù‹
                    for param_group in model.optimizer.param_groups:
                        param_group['lr'] *= 0.95
                        
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¶Ø¨Ø· Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª: {e}")

    async def _predict_single(self, data: Dict[str, Any]) -> Any:
        """ØªÙ†Ø¨Ø¤ Ù„Ø¹ÙŠÙ†Ø© ÙˆØ§Ø­Ø¯Ø©"""
        try:
            # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            example = LearningExample(
                example_id="temp",
                data=data,
                target=None,
                timestamp=datetime.now()
            )
            
            X, _ = await self._prepare_data([example])
            
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£ÙˆÙ„ Ù†Ù…ÙˆØ°Ø¬ Ù…ØªØ§Ø­
            if self.models:
                model_key = list(self.models.keys())[0]
                model = self.models[model_key]
                
                if PYTORCH_AVAILABLE and isinstance(model, nn.Module):
                    model.eval()
                    with torch.no_grad():
                        X_tensor = torch.FloatTensor(X)
                        output = model(X_tensor)
                        if model.task_type == "classification":
                            prediction = torch.argmax(output, dim=1).item()
                        else:
                            prediction = output.item()
                else:
                    prediction = model.predict(X)[0]
                
                return prediction
            
            return 0  # Ù‚ÙŠÙ…Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªÙ†Ø¨Ø¤: {e}")
            return 0

    async def _create_checkpoint(self, model_key: str = None):
        """Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù‚Ø·Ø© ØªÙØªÙŠØ´ Ù„Ù„Ù†Ù…Ø§Ø°Ø¬"""
        try:
            models_to_checkpoint = [model_key] if model_key else list(self.models.keys())
            
            for key in models_to_checkpoint:
                if key in self.models:
                    model = self.models[key]
                    
                    # Ø­ÙØ¸ Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
                    if PYTORCH_AVAILABLE and isinstance(model, nn.Module):
                        model_state = pickle.dumps(model.state_dict())
                    else:
                        model_state = pickle.dumps(model)
                    
                    # Ø­Ø³Ø§Ø¨ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡
                    performance_metrics = self.model_performance.get(key, {})
                    
                    checkpoint = ModelCheckpoint(
                        checkpoint_id=f"{key}_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                        model_state=model_state,
                        performance_metrics=performance_metrics,
                        timestamp=datetime.now(),
                        examples_seen=self.examples_processed
                    )
                    
                    self.model_checkpoints[key].append(checkpoint)
                    
                    # Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø¢Ø®Ø± 10 Ù†Ù‚Ø§Ø· ØªÙØªÙŠØ´ ÙÙ‚Ø·
                    if len(self.model_checkpoints[key]) > 10:
                        self.model_checkpoints[key] = self.model_checkpoints[key][-10:]
            
            self.logger.info("ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù‚Ø§Ø· Ø§Ù„ØªÙØªÙŠØ´")
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù‚Ø·Ø© Ø§Ù„ØªÙØªÙŠØ´: {e}")

    async def _evaluate_model_performance(self, model_key: str, task_type: str):
        """ØªÙ‚ÙŠÙŠÙ… Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        try:
            if len(self.validation_buffer) < 10:
                return
            
            model = self.models[model_key]
            validation_examples = list(self.validation_buffer)[-50:]
            
            predictions = []
            actual_values = []
            
            for example in validation_examples:
                try:
                    predicted = await self._predict_single(example.data)
                    predictions.append(predicted)
                    actual_values.append(example.target)
                except Exception:
                    continue
            
            if predictions and actual_values:
                # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³
                if task_type == "classification":
                    accuracy = accuracy_score(actual_values, predictions)
                    self.model_performance[model_key]['accuracy'] = accuracy
                else:
                    mse = mean_squared_error(actual_values, predictions)
                    r2 = r2_score(actual_values, predictions)
                    self.model_performance[model_key]['mse'] = mse
                    self.model_performance[model_key]['r2'] = r2
                
                self.model_performance[model_key]['last_evaluation'] = datetime.now().isoformat()
                
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø£Ø¯Ø§Ø¡: {e}")

    async def add_feedback(self, example_id: str, feedback_score: float, feedback_data: Dict[str, Any] = None):
        """Ø¥Ø¶Ø§ÙØ© Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ø¹Ù„Ù‰ Ø§Ù„ØªÙ†Ø¨Ø¤"""
        try:
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ø«Ø§Ù„ ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø®Ø²Ù†Ø©
            for example in self.data_buffer:
                if example.example_id == example_id:
                    example.feedback_score = feedback_score
                    example.is_validated = True
                    if feedback_data:
                        example.metadata.update(feedback_data)
                    
                    self.feedback_buffer.append(example)
                    break
            
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø§Øª ÙÙŠ Ø§Ù„ØªØ­Ø³ÙŠÙ†
            if len(self.feedback_buffer) >= 20:
                await self._apply_feedback_learning()
                
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø§Øª: {e}")

    async def _apply_feedback_learning(self):
        """ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø§Øª"""
        try:
            # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ø§Ù„Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ© ÙˆØ§Ù„Ø³Ù„Ø¨ÙŠØ©
            positive_examples = [ex for ex in self.feedback_buffer if ex.feedback_score >= 0.7]
            negative_examples = [ex for ex in self.feedback_buffer if ex.feedback_score <= 0.3]
            
            # Ø¥Ø¹Ø§Ø¯Ø© ØªØ¯Ø±ÙŠØ¨ Ù…Ø¹ Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø§Øª
            if positive_examples:
                for model_key in self.models.keys():
                    task_type = model_key.split('_')[1]
                    
                    # Ø¥Ø¹Ø·Ø§Ø¡ ÙˆØ²Ù† Ø£ÙƒØ¨Ø± Ù„Ù„Ø£Ù…Ø«Ù„Ø© Ø§Ù„Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ©
                    X_pos, y_pos = await self._prepare_data(positive_examples)
                    
                    model = self.models[model_key]
                    if hasattr(model, 'partial_fit'):
                        # ØªØ¯Ø±ÙŠØ¨ Ù…ØªØ¹Ø¯Ø¯ Ù„Ù„Ø£Ù…Ø«Ù„Ø© Ø§Ù„Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ©
                        for _ in range(3):
                            if hasattr(model, 'classes_'):
                                model.partial_fit(X_pos, y_pos, classes=model.classes_)
                            else:
                                model.partial_fit(X_pos, y_pos)
            
            # Ù…Ø³Ø­ Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
            self.feedback_buffer.clear()
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø§Øª: {e}")

    async def get_learning_statistics(self) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªØ¹Ù„Ù…"""
        try:
            stats = {
                "total_examples_processed": self.examples_processed,
                "total_adaptations": self.adaptations_count,
                "models_count": len(self.models),
                "drift_detections": len([d for d in self.drift_history if d.drift_detected]),
                "buffer_status": {
                    "data_buffer": len(self.data_buffer),
                    "validation_buffer": len(self.validation_buffer),
                    "feedback_buffer": len(self.feedback_buffer)
                },
                "model_performance": self.model_performance,
                "last_adaptation": self.last_adaptation_time.isoformat() if self.last_adaptation_time else None,
                "learning_sessions": len(self.learning_sessions),
                "is_running": self.is_running
            }
            
            return stats
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª: {e}")
            return {}

    async def save_learning_state(self):
        """Ø­ÙØ¸ Ø­Ø§Ù„Ø© Ø§Ù„ØªØ¹Ù„Ù…"""
        try:
            state_file = self.models_dir / "learning_state.json"
            
            state = {
                "examples_processed": self.examples_processed,
                "adaptations_count": self.adaptations_count,
                "last_adaptation_time": self.last_adaptation_time.isoformat() if self.last_adaptation_time else None,
                "model_performance": self.model_performance,
                "drift_history": [asdict(d) for d in self.drift_history],
                "config": self.config
            }
            
            with open(state_file, 'w', encoding='utf-8') as f:
                json.dump(state, f, ensure_ascii=False, indent=2, default=str)
            
            # Ø­ÙØ¸ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
            for model_key, model in self.models.items():
                model_file = self.models_dir / f"{model_key}.pkl"
                with open(model_file, 'wb') as f:
                    pickle.dump(model, f)
            
            self.logger.info("ØªÙ… Ø­ÙØ¸ Ø­Ø§Ù„Ø© Ø§Ù„ØªØ¹Ù„Ù…")
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø­Ø§Ù„Ø© Ø§Ù„ØªØ¹Ù„Ù…: {e}")

    async def load_learning_state(self):
        """ØªØ­Ù…ÙŠÙ„ Ø­Ø§Ù„Ø© Ø§Ù„ØªØ¹Ù„Ù…"""
        try:
            state_file = self.models_dir / "learning_state.json"
            
            if state_file.exists():
                with open(state_file, 'r', encoding='utf-8') as f:
                    state = json.load(f)
                
                self.examples_processed = state.get('examples_processed', 0)
                self.adaptations_count = state.get('adaptations_count', 0)
                
                if state.get('last_adaptation_time'):
                    self.last_adaptation_time = datetime.fromisoformat(state['last_adaptation_time'])
                
                self.model_performance = state.get('model_performance', {})
                
                # ØªØ­Ù…ÙŠÙ„ ØªØ§Ø±ÙŠØ® Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù
                drift_data = state.get('drift_history', [])
                self.drift_history = [DriftDetectionResult(**d) for d in drift_data]
            
            # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
            model_files = list(self.models_dir.glob("continuous_*.pkl"))
            for model_file in model_files:
                model_key = model_file.stem
                try:
                    with open(model_file, 'rb') as f:
                        self.models[model_key] = pickle.load(f)
                except Exception as e:
                    self.logger.warning(f"ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ {model_key}: {e}")
            
            self.logger.info("ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø­Ø§Ù„Ø© Ø§Ù„ØªØ¹Ù„Ù…")
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø­Ø§Ù„Ø© Ø§Ù„ØªØ¹Ù„Ù…: {e}")

class SimpleDriftDetector:
    """ÙƒØ§Ø´Ù Ø§Ù†Ø­Ø±Ø§Ù Ø¨Ø³ÙŠØ·"""
    
    def __init__(self, window_size: int = 100, threshold: float = 0.1):
        self.window_size = window_size
        self.threshold = threshold
        self.performance_window = deque(maxlen=window_size)
        self.baseline_performance = None
    
    def add_element(self, performance_score: float):
        """Ø¥Ø¶Ø§ÙØ© Ù†Ù‚Ø·Ø© Ø£Ø¯Ø§Ø¡ Ø¬Ø¯ÙŠØ¯Ø©"""
        self.performance_window.append(performance_score)
        
        if self.baseline_performance is None and len(self.performance_window) >= 20:
            self.baseline_performance = np.mean(list(self.performance_window)[:20])
    
    def detected_change(self) -> bool:
        """ÙØ­Øµ ÙˆØ¬ÙˆØ¯ Ø§Ù†Ø­Ø±Ø§Ù"""
        if len(self.performance_window) < 20 or self.baseline_performance is None:
            return False
        
        recent_performance = np.mean(list(self.performance_window)[-20:])
        return abs(self.baseline_performance - recent_performance) > self.threshold

# Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø«ÙŠÙ„ Ø¹Ø§Ù…
continuous_learning_engine = ContinuousLearningEngine()

async def get_continuous_learning_engine() -> ContinuousLearningEngine:
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø­Ø±Ùƒ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø³ØªÙ…Ø±"""
    return continuous_learning_engine

if __name__ == "__main__":
    async def test_continuous_learning():
        """Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø­Ø±Ùƒ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø³ØªÙ…Ø±"""
        print("ğŸ§  Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø­Ø±Ùƒ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø³ØªÙ…Ø± Ø§Ù„Ù…ØªÙ‚Ø¯Ù…")
        print("=" * 60)
        
        engine = await get_continuous_learning_engine()
        await engine.start_continuous_learning()
        
        print("ğŸ“š Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø³ØªÙ…Ø±...")
        
        # Ù…Ø­Ø§ÙƒØ§Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¹Ù„Ù…
        for i in range(100):
            data = {
                'feature1': np.random.randn(),
                'feature2': np.random.randn(),
                'feature3': np.random.choice(['A', 'B', 'C'])
            }
            target = np.random.choice([0, 1])
            
            example_id = await engine.add_learning_example(
                data=data,
                target=target,
                source="test_simulation"
            )
            
            # Ø¥Ø¶Ø§ÙØ© Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©
            if i % 10 == 0:
                feedback_score = np.random.uniform(0.3, 0.9)
                await engine.add_feedback(example_id, feedback_score)
            
            # Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ù†Ø­Ø±Ø§Ù ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            if i == 50:
                print("ğŸ”„ Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ù†Ø­Ø±Ø§Ù ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
        
        # Ø§Ù†ØªØ¸Ø§Ø± Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        await asyncio.sleep(2)
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        stats = await engine.get_learning_statistics()
        print("\nğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªØ¹Ù„Ù…:")
        print(f"  â€¢ Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {stats['total_examples_processed']}")
        print(f"  â€¢ Ø¹Ø¯Ø¯ Ø§Ù„ØªÙƒÙŠÙØ§Øª: {stats['total_adaptations']}")
        print(f"  â€¢ Ø¹Ø¯Ø¯ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬: {stats['models_count']}")
        print(f"  â€¢ ÙƒØ´Ù Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù: {stats['drift_detections']}")
        
        print(f"\nğŸ’¾ Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø®Ø§Ø²Ù† Ø§Ù„Ù…Ø¤Ù‚ØªØ©:")
        buffer_status = stats['buffer_status']
        print(f"  â€¢ Ù…Ø®Ø²Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {buffer_status['data_buffer']}")
        print(f"  â€¢ Ù…Ø®Ø²Ù† Ø§Ù„ØªØ­Ù‚Ù‚: {buffer_status['validation_buffer']}")
        print(f"  â€¢ Ù…Ø®Ø²Ù† Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø§Øª: {buffer_status['feedback_buffer']}")
        
        # Ø­ÙØ¸ Ø§Ù„Ø­Ø§Ù„Ø©
        await engine.save_learning_state()
        print("\nğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø­Ø§Ù„Ø© Ø§Ù„ØªØ¹Ù„Ù…")
        
        # Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ¹Ù„Ù…
        await engine.stop_continuous_learning()
        print("\nâ¹ï¸ ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø³ØªÙ…Ø±")
        
        print("\nâœ¨ Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø¨Ù†Ø¬Ø§Ø­!")

    # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
    asyncio.run(test_continuous_learning())
