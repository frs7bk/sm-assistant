
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ù…Ø­Ø±Ùƒ Ø§Ù„ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù…Ø®ØªÙ„Ø· Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
Advanced Mixed Reality Engine
"""

import asyncio
import logging
import numpy as np
import cv2
import json
from typing import Dict, List, Any, Optional, Tuple, Union
from pathlib import Path
from datetime import datetime
from dataclasses import dataclass, asdict
import threading
import queue
import math

try:
    import mediapipe as mp
    MEDIAPIPE_AVAILABLE = True
except ImportError:
    MEDIAPIPE_AVAILABLE = False

try:
    import open3d as o3d
    OPEN3D_AVAILABLE = True
except ImportError:
    OPEN3D_AVAILABLE = False

@dataclass
class SpatialObject:
    """ÙƒØ§Ø¦Ù† Ù…ÙƒØ§Ù†ÙŠ ÙÙŠ Ø§Ù„ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù…Ø®ØªÙ„Ø·"""
    object_id: str
    position: Tuple[float, float, float]
    rotation: Tuple[float, float, float]
    scale: Tuple[float, float, float]
    object_type: str
    properties: Dict[str, Any]
    is_virtual: bool
    confidence: float = 1.0
    last_updated: datetime = None
    
    def __post_init__(self):
        if self.last_updated is None:
            self.last_updated = datetime.now()

@dataclass
class SpatialAnchor:
    """Ù…Ø±Ø³Ø§Ø© Ù…ÙƒØ§Ù†ÙŠØ© Ù„Ù„ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù…Ø®ØªÙ„Ø·"""
    anchor_id: str
    world_position: Tuple[float, float, float]
    local_position: Tuple[float, float, float]
    orientation: Tuple[float, float, float, float]  # quaternion
    anchor_type: str
    confidence: float
    created_at: datetime
    attached_objects: List[str]

@dataclass
class GestureCommand:
    """Ø£Ù…Ø± Ø§Ù„Ø¥ÙŠÙ…Ø§Ø¡Ø©"""
    gesture_type: str
    confidence: float
    parameters: Dict[str, Any]
    timestamp: datetime
    hand_landmarks: Optional[List[Tuple[float, float, float]]] = None

class SpatialMappingEngine:
    """Ù…Ø­Ø±Ùƒ Ø±Ø³Ù… Ø§Ù„Ø®Ø±Ø§Ø¦Ø· Ø§Ù„Ù…ÙƒØ§Ù†ÙŠØ©"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
        # Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ù…ÙƒØ§Ù†ÙŠØ©
        self.spatial_map: Dict[str, SpatialObject] = {}
        self.spatial_anchors: Dict[str, SpatialAnchor] = {}
        
        # ÙƒØ§Ù…ÙŠØ±Ø§ ÙˆØ£Ø¬Ù‡Ø²Ø© Ø§Ù„Ø§Ø³ØªØ´Ø¹Ø§Ø±
        self.camera = None
        self.depth_camera = None
        
        # Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ÙƒØ´Ù
        if MEDIAPIPE_AVAILABLE:
            self.mp_hands = mp.solutions.hands
            self.mp_pose = mp.solutions.pose
            self.mp_face = mp.solutions.face_detection
            self.hands_detector = None
            self.pose_detector = None
            self.face_detector = None
        
        # Ù…Ø¹Ø§ÙŠØ±Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§
        self.camera_matrix = None
        self.distortion_coeffs = None
        
        # Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ø¸Ø§Ù…
        self.is_tracking = False
        self.tracking_quality = 0.0
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…
        self.config = {
            "max_tracking_distance": 5.0,  # Ù…ØªØ±
            "min_confidence": 0.7,
            "anchor_persistence_time": 300,  # Ø«Ø§Ù†ÙŠØ©
            "gesture_timeout": 2.0,
            "spatial_resolution": 0.01,  # Ù…ØªØ±
            "max_objects": 1000
        }

    async def initialize(self):
        """ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ø±Ùƒ Ø±Ø³Ù… Ø§Ù„Ø®Ø±Ø§Ø¦Ø· Ø§Ù„Ù…ÙƒØ§Ù†ÙŠØ©"""
        
        try:
            self.logger.info("ğŸ—ºï¸ ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ø±Ùƒ Ø±Ø³Ù… Ø§Ù„Ø®Ø±Ø§Ø¦Ø· Ø§Ù„Ù…ÙƒØ§Ù†ÙŠØ©...")
            
            # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§
            await self._initialize_camera()
            
            # ØªÙ‡ÙŠØ¦Ø© ÙƒØ§Ø´ÙØ§Øª MediaPipe
            if MEDIAPIPE_AVAILABLE:
                await self._initialize_detectors()
            
            # ØªØ­Ù…ÙŠÙ„ Ù…Ø¹Ø§ÙŠØ±Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§
            await self._load_camera_calibration()
            
            self.logger.info("âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ø±Ùƒ Ø±Ø³Ù… Ø§Ù„Ø®Ø±Ø§Ø¦Ø· Ø§Ù„Ù…ÙƒØ§Ù†ÙŠØ©")
            
        except Exception as e:
            self.logger.error(f"âŒ ÙØ´Ù„ ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ø±Ùƒ Ø±Ø³Ù… Ø§Ù„Ø®Ø±Ø§Ø¦Ø· Ø§Ù„Ù…ÙƒØ§Ù†ÙŠØ©: {e}")

    async def _initialize_camera(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§"""
        
        try:
            self.camera = cv2.VideoCapture(0)
            if not self.camera.isOpened():
                raise Exception("ÙØ´Ù„ ÙÙŠ ÙØªØ­ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§")
            
            # ØªØ¹ÙŠÙŠÙ† Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©
            self.camera.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
            self.camera.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
            self.camera.set(cv2.CAP_PROP_FPS, 30)
            
        except Exception as e:
            self.logger.warning(f"Ø®Ø·Ø£ ÙÙŠ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§: {e}")

    async def _initialize_detectors(self):
        """ØªÙ‡ÙŠØ¦Ø© ÙƒØ§Ø´ÙØ§Øª MediaPipe"""
        
        try:
            self.hands_detector = self.mp_hands.Hands(
                static_image_mode=False,
                max_num_hands=2,
                min_detection_confidence=0.7,
                min_tracking_confidence=0.5
            )
            
            self.pose_detector = self.mp_pose.Pose(
                static_image_mode=False,
                min_detection_confidence=0.7,
                min_tracking_confidence=0.5
            )
            
            self.face_detector = self.mp_face.FaceDetection(
                min_detection_confidence=0.7
            )
            
        except Exception as e:
            self.logger.warning(f"Ø®Ø·Ø£ ÙÙŠ ØªÙ‡ÙŠØ¦Ø© ÙƒØ§Ø´ÙØ§Øª MediaPipe: {e}")

    async def _load_camera_calibration(self):
        """ØªØ­Ù…ÙŠÙ„ Ù…Ø¹Ø§ÙŠØ±Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§"""
        
        try:
            calibration_file = Path("data/camera_calibration.json")
            
            if calibration_file.exists():
                with open(calibration_file, 'r') as f:
                    calibration_data = json.load(f)
                
                self.camera_matrix = np.array(calibration_data["camera_matrix"])
                self.distortion_coeffs = np.array(calibration_data["distortion_coeffs"])
            else:
                # Ù…Ø¹Ø§ÙŠØ±Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
                self.camera_matrix = np.array([
                    [800, 0, 320],
                    [0, 800, 240],
                    [0, 0, 1]
                ], dtype=np.float32)
                self.distortion_coeffs = np.zeros((4, 1))
                
        except Exception as e:
            self.logger.warning(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù…Ø¹Ø§ÙŠØ±Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§: {e}")

    async def start_spatial_tracking(self) -> bool:
        """Ø¨Ø¯Ø¡ ØªØªØ¨Ø¹ Ø§Ù„Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…ÙƒØ§Ù†ÙŠØ©"""
        
        try:
            if not self.camera or not self.camera.isOpened():
                await self._initialize_camera()
            
            self.is_tracking = True
            
            # Ø¨Ø¯Ø¡ Ø®ÙŠØ· Ø§Ù„ØªØªØ¨Ø¹
            tracking_thread = threading.Thread(
                target=self._tracking_loop,
                daemon=True
            )
            tracking_thread.start()
            
            self.logger.info("ğŸ¯ Ø¨Ø¯Ø¡ Ø§Ù„ØªØªØ¨Ø¹ Ø§Ù„Ù…ÙƒØ§Ù†ÙŠ")
            return True
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¨Ø¯Ø¡ Ø§Ù„ØªØªØ¨Ø¹ Ø§Ù„Ù…ÙƒØ§Ù†ÙŠ: {e}")
            return False

    def _tracking_loop(self):
        """Ø­Ù„Ù‚Ø© Ø§Ù„ØªØªØ¨Ø¹ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
        
        while self.is_tracking:
            try:
                if not self.camera or not self.camera.isOpened():
                    break
                
                ret, frame = self.camera.read()
                if not ret:
                    continue
                
                # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¥Ø·Ø§Ø±
                processed_frame = self._process_frame(frame)
                
                # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…ÙƒØ§Ù†ÙŠØ©
                self._update_spatial_map(processed_frame)
                
                # ØªÙ†Ø¸ÙŠÙ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
                self._cleanup_old_objects()
                
            except Exception as e:
                self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø­Ù„Ù‚Ø© Ø§Ù„ØªØªØ¨Ø¹: {e}")
                break

    def _process_frame(self, frame: np.ndarray) -> Dict[str, Any]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¥Ø·Ø§Ø± Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§"""
        
        results = {
            "frame": frame,
            "hands": [],
            "pose": None,
            "faces": [],
            "objects": [],
            "timestamp": datetime.now()
        }
        
        try:
            # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ RGB
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            
            if MEDIAPIPE_AVAILABLE and self.hands_detector:
                # ÙƒØ´Ù Ø§Ù„ÙŠØ¯ÙŠÙ†
                hands_results = self.hands_detector.process(rgb_frame)
                if hands_results.multi_hand_landmarks:
                    for hand_landmarks in hands_results.multi_hand_landmarks:
                        results["hands"].append(self._extract_hand_landmarks(hand_landmarks))
                
                # ÙƒØ´Ù Ø§Ù„ÙˆØ¶Ø¹ÙŠØ©
                if self.pose_detector:
                    pose_results = self.pose_detector.process(rgb_frame)
                    if pose_results.pose_landmarks:
                        results["pose"] = self._extract_pose_landmarks(pose_results.pose_landmarks)
                
                # ÙƒØ´Ù Ø§Ù„ÙˆØ¬ÙˆÙ‡
                if self.face_detector:
                    face_results = self.face_detector.process(rgb_frame)
                    if face_results.detections:
                        for detection in face_results.detections:
                            results["faces"].append(self._extract_face_detection(detection))
            
            # ÙƒØ´Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª Ø§Ù„Ø£Ø®Ø±Ù‰
            objects = self._detect_objects(frame)
            results["objects"] = objects
            
        except Exception as e:
            self.logger.warning(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¥Ø·Ø§Ø±: {e}")
        
        return results

    def _extract_hand_landmarks(self, hand_landmarks) -> List[Tuple[float, float, float]]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ù‚Ø§Ø· Ø§Ù„ÙŠØ¯"""
        
        landmarks = []
        for landmark in hand_landmarks.landmark:
            landmarks.append((landmark.x, landmark.y, landmark.z))
        return landmarks

    def _extract_pose_landmarks(self, pose_landmarks) -> List[Tuple[float, float, float]]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ù‚Ø§Ø· Ø§Ù„ÙˆØ¶Ø¹ÙŠØ©"""
        
        landmarks = []
        for landmark in pose_landmarks.landmark:
            landmarks.append((landmark.x, landmark.y, landmark.z))
        return landmarks

    def _extract_face_detection(self, detection) -> Dict[str, Any]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒØ´Ù Ø§Ù„ÙˆØ¬Ù‡"""
        
        bbox = detection.location_data.relative_bounding_box
        return {
            "bbox": [bbox.xmin, bbox.ymin, bbox.width, bbox.height],
            "confidence": detection.score[0]
        }

    def _detect_objects(self, frame: np.ndarray) -> List[Dict[str, Any]]:
        """ÙƒØ´Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¥Ø·Ø§Ø±"""
        
        objects = []
        
        try:
            # ÙƒØ´Ù Ø§Ù„Ø­ÙˆØ§Ù ÙˆØ§Ù„Ø£Ø´ÙƒØ§Ù„
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            edges = cv2.Canny(gray, 50, 150)
            
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙƒÙˆÙ†ØªÙˆØ±Ø§Øª
            contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            for i, contour in enumerate(contours):
                area = cv2.contourArea(contour)
                if area > 500:  # ØªØµÙÙŠØ© Ø§Ù„ÙƒÙˆÙ†ØªÙˆØ±Ø§Øª Ø§Ù„ØµØºÙŠØ±Ø©
                    # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø³ØªØ·ÙŠÙ„ Ø§Ù„Ù…Ø­ÙŠØ·
                    x, y, w, h = cv2.boundingRect(contour)
                    
                    # ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø«Ù„Ø§Ø«ÙŠ Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯
                    center_x = x + w // 2
                    center_y = y + h // 2
                    estimated_depth = self._estimate_depth(w, h)
                    
                    world_pos = self._screen_to_world(center_x, center_y, estimated_depth)
                    
                    objects.append({
                        "id": f"object_{i}",
                        "type": "unknown",
                        "bbox": [x, y, w, h],
                        "area": area,
                        "world_position": world_pos,
                        "confidence": 0.7
                    })
            
        except Exception as e:
            self.logger.warning(f"Ø®Ø·Ø£ ÙÙŠ ÙƒØ´Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª: {e}")
        
        return objects

    def _estimate_depth(self, width: int, height: int) -> float:
        """ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø­Ø¬Ù… Ø§Ù„ÙƒØ§Ø¦Ù†"""
        
        # ØªÙ‚Ø¯ÙŠØ± Ø¨Ø³ÙŠØ· Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø­Ø¬Ù… Ø§Ù„ÙƒØ§Ø¦Ù†
        # ÙŠÙ…ÙƒÙ† ØªØ­Ø³ÙŠÙ†Ù‡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙƒØ§Ù…ÙŠØ±Ø§ Ø§Ù„Ø¹Ù…Ù‚ Ø£Ùˆ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ
        
        object_size = math.sqrt(width * height)
        if object_size > 200:
            return 0.5  # Ù‚Ø±ÙŠØ¨
        elif object_size > 100:
            return 1.0  # Ù…ØªÙˆØ³Ø·
        else:
            return 2.0  # Ø¨Ø¹ÙŠØ¯

    def _screen_to_world(self, x: int, y: int, depth: float) -> Tuple[float, float, float]:
        """ØªØ­ÙˆÙŠÙ„ Ø¥Ø­Ø¯Ø§Ø«ÙŠØ§Øª Ø§Ù„Ø´Ø§Ø´Ø© Ø¥Ù„Ù‰ Ø¥Ø­Ø¯Ø§Ø«ÙŠØ§Øª Ø§Ù„Ø¹Ø§Ù„Ù…"""
        
        if self.camera_matrix is None:
            return (0.0, 0.0, depth)
        
        try:
            # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ø¥Ø­Ø¯Ø§Ø«ÙŠØ§Øª Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠØ©
            fx = self.camera_matrix[0, 0]
            fy = self.camera_matrix[1, 1]
            cx = self.camera_matrix[0, 2]
            cy = self.camera_matrix[1, 2]
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø«Ù„Ø§Ø«ÙŠ Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯
            world_x = (x - cx) * depth / fx
            world_y = (y - cy) * depth / fy
            world_z = depth
            
            return (world_x, world_y, world_z)
            
        except Exception as e:
            self.logger.warning(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¥Ø­Ø¯Ø§Ø«ÙŠØ§Øª: {e}")
            return (0.0, 0.0, depth)

    def _update_spatial_map(self, processed_frame: Dict[str, Any]):
        """ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…ÙƒØ§Ù†ÙŠØ©"""
        
        try:
            timestamp = processed_frame["timestamp"]
            
            # ØªØ­Ø¯ÙŠØ« ÙƒØ§Ø¦Ù†Ø§Øª Ø§Ù„ÙŠØ¯ÙŠÙ†
            for i, hand in enumerate(processed_frame["hands"]):
                hand_id = f"hand_{i}"
                if len(hand) > 8:  # Ù†Ù‚Ø·Ø© Ø§Ù„Ù…Ø¹ØµÙ…
                    wrist_pos = hand[0]  # Ù†Ù‚Ø·Ø© Ø§Ù„Ù…Ø¹ØµÙ…
                    world_pos = self._screen_to_world(
                        int(wrist_pos[0] * 1280),
                        int(wrist_pos[1] * 720),
                        1.0
                    )
                    
                    self._update_spatial_object(
                        hand_id,
                        world_pos,
                        "hand",
                        {"landmarks": hand},
                        is_virtual=False,
                        confidence=0.9
                    )
            
            # ØªØ­Ø¯ÙŠØ« ÙƒØ§Ø¦Ù†Ø§Øª Ø§Ù„ÙˆØ¬ÙˆÙ‡
            for i, face in enumerate(processed_frame["faces"]):
                face_id = f"face_{i}"
                bbox = face["bbox"]
                center_x = int((bbox[0] + bbox[2] / 2) * 1280)
                center_y = int((bbox[1] + bbox[3] / 2) * 720)
                
                world_pos = self._screen_to_world(center_x, center_y, 1.5)
                
                self._update_spatial_object(
                    face_id,
                    world_pos,
                    "face",
                    {"bbox": bbox},
                    is_virtual=False,
                    confidence=face["confidence"]
                )
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª Ø§Ù„Ù…ÙƒØªØ´ÙØ©
            for obj in processed_frame["objects"]:
                self._update_spatial_object(
                    obj["id"],
                    obj["world_position"],
                    obj["type"],
                    {"bbox": obj["bbox"], "area": obj["area"]},
                    is_virtual=False,
                    confidence=obj["confidence"]
                )
            
        except Exception as e:
            self.logger.warning(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…ÙƒØ§Ù†ÙŠØ©: {e}")

    def _update_spatial_object(
        self,
        object_id: str,
        position: Tuple[float, float, float],
        object_type: str,
        properties: Dict[str, Any],
        is_virtual: bool,
        confidence: float
    ):
        """ØªØ­Ø¯ÙŠØ« ÙƒØ§Ø¦Ù† Ù…ÙƒØ§Ù†ÙŠ"""
        
        if object_id in self.spatial_map:
            # ØªØ­Ø¯ÙŠØ« ÙƒØ§Ø¦Ù† Ù…ÙˆØ¬ÙˆØ¯
            obj = self.spatial_map[object_id]
            obj.position = position
            obj.properties.update(properties)
            obj.confidence = confidence
            obj.last_updated = datetime.now()
        else:
            # Ø¥Ù†Ø´Ø§Ø¡ ÙƒØ§Ø¦Ù† Ø¬Ø¯ÙŠØ¯
            if len(self.spatial_map) < self.config["max_objects"]:
                self.spatial_map[object_id] = SpatialObject(
                    object_id=object_id,
                    position=position,
                    rotation=(0.0, 0.0, 0.0),
                    scale=(1.0, 1.0, 1.0),
                    object_type=object_type,
                    properties=properties,
                    is_virtual=is_virtual,
                    confidence=confidence,
                    last_updated=datetime.now()
                )

    def _cleanup_old_objects(self):
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©"""
        
        current_time = datetime.now()
        objects_to_remove = []
        
        for object_id, obj in self.spatial_map.items():
            time_diff = (current_time - obj.last_updated).total_seconds()
            
            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© (ØºÙŠØ± Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©)
            if not obj.is_virtual and time_diff > 5.0:
                objects_to_remove.append(object_id)
        
        for object_id in objects_to_remove:
            del self.spatial_map[object_id]

class MixedRealityEngine:
    """Ù…Ø­Ø±Ùƒ Ø§Ù„ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù…Ø®ØªÙ„Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
        # Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        self.spatial_mapping = SpatialMappingEngine()
        
        # Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
        self.virtual_objects: Dict[str, SpatialObject] = {}
        
        # ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…ÙƒØ§Ù†ÙŠØ©
        self.spatial_ui_elements: Dict[str, Dict[str, Any]] = {}
        
        # Ù†Ø¸Ø§Ù… Ø§Ù„ØªÙØ§Ø¹Ù„
        self.interaction_zones: Dict[str, Dict[str, Any]] = {}
        self.active_gestures: Dict[str, GestureCommand] = {}
        
        # Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ø¸Ø§Ù…
        self.is_active = False
        self.current_mode = "tracking"  # tracking, interaction, creation
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¹Ø±Ø¶
        self.display_config = {
            "show_spatial_map": True,
            "show_virtual_objects": True,
            "show_ui_elements": True,
            "show_debug_info": False,
            "transparency": 0.7
        }

    async def initialize(self):
        """ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ø±Ùƒ Ø§Ù„ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù…Ø®ØªÙ„Ø·"""
        
        try:
            self.logger.info("ğŸ¥½ ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ø±Ùƒ Ø§Ù„ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù…Ø®ØªÙ„Ø·...")
            
            # ØªÙ‡ÙŠØ¦Ø© Ø±Ø³Ù… Ø§Ù„Ø®Ø±Ø§Ø¦Ø· Ø§Ù„Ù…ÙƒØ§Ù†ÙŠØ©
            await self.spatial_mapping.initialize()
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø¹Ù†Ø§ØµØ± ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
            await self._create_default_ui()
            
            # ØªÙ‡ÙŠØ¦Ø© Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„ØªÙØ§Ø¹Ù„
            await self._setup_interaction_zones()
            
            self.is_active = True
            self.logger.info("âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ø±Ùƒ Ø§Ù„ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù…Ø®ØªÙ„Ø·")
            
        except Exception as e:
            self.logger.error(f"âŒ ÙØ´Ù„ ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ø±Ùƒ Ø§Ù„ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù…Ø®ØªÙ„Ø·: {e}")

    async def _create_default_ui(self):
        """Ø¥Ù†Ø´Ø§Ø¡ ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©"""
        
        try:
            # Ù‚Ø§Ø¦Ù…Ø© Ø±Ø¦ÙŠØ³ÙŠØ© Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
            self.spatial_ui_elements["main_menu"] = {
                "type": "menu",
                "position": (0.0, 0.5, -1.0),
                "items": [
                    {"id": "create_object", "label": "Ø¥Ù†Ø´Ø§Ø¡ ÙƒØ§Ø¦Ù†", "action": "create_virtual_object"},
                    {"id": "settings", "label": "Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª", "action": "open_settings"},
                    {"id": "help", "label": "Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©", "action": "show_help"}
                ],
                "visible": False,
                "size": (0.3, 0.4)
            }
            
            # Ù„ÙˆØ­Ø© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª
            self.spatial_ui_elements["info_panel"] = {
                "type": "panel",
                "position": (0.5, 0.8, -0.8),
                "content": "Ù…Ø±Ø­Ø¨Ø§Ù‹ Ø¨Ùƒ ÙÙŠ Ø§Ù„ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù…Ø®ØªÙ„Ø·",
                "visible": True,
                "size": (0.4, 0.1),
                "background_color": (0, 0, 0, 0.7)
            }
            
        except Exception as e:
            self.logger.warning(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {e}")

    async def _setup_interaction_zones(self):
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„ØªÙØ§Ø¹Ù„"""
        
        try:
            # Ù…Ù†Ø·Ù‚Ø© ØªÙØ§Ø¹Ù„ Ø§Ù„ÙŠØ¯ Ø§Ù„ÙŠÙ…Ù†Ù‰
            self.interaction_zones["right_hand"] = {
                "type": "hand_interaction",
                "radius": 0.2,
                "actions": ["select", "grab", "point"],
                "sensitivity": 0.8
            }
            
            # Ù…Ù†Ø·Ù‚Ø© ØªÙØ§Ø¹Ù„ Ø§Ù„ØµÙˆØª
            self.interaction_zones["voice"] = {
                "type": "voice_interaction",
                "radius": 2.0,
                "actions": ["command", "dictation", "navigation"],
                "sensitivity": 0.7
            }
            
            # Ù…Ù†Ø·Ù‚Ø© ØªÙØ§Ø¹Ù„ Ø§Ù„Ù†Ø¸Ø±
            self.interaction_zones["gaze"] = {
                "type": "gaze_interaction",
                "radius": 0.1,
                "actions": ["focus", "select", "scroll"],
                "sensitivity": 0.9
            }
            
        except Exception as e:
            self.logger.warning(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ø¹Ø¯Ø§Ø¯ Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„ØªÙØ§Ø¹Ù„: {e}")

    async def start_mixed_reality_session(self) -> bool:
        """Ø¨Ø¯Ø¡ Ø¬Ù„Ø³Ø© Ø§Ù„ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù…Ø®ØªÙ„Ø·"""
        
        try:
            if not self.is_active:
                await self.initialize()
            
            # Ø¨Ø¯Ø¡ Ø§Ù„ØªØªØ¨Ø¹ Ø§Ù„Ù…ÙƒØ§Ù†ÙŠ
            success = await self.spatial_mapping.start_spatial_tracking()
            
            if success:
                # Ø¨Ø¯Ø¡ Ø­Ù„Ù‚Ø© Ø§Ù„Ø¹Ø±Ø¶
                display_thread = threading.Thread(
                    target=self._display_loop,
                    daemon=True
                )
                display_thread.start()
                
                self.logger.info("ğŸ® Ø¨Ø¯Ø¡ Ø¬Ù„Ø³Ø© Ø§Ù„ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù…Ø®ØªÙ„Ø·")
                return True
            
            return False
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¨Ø¯Ø¡ Ø¬Ù„Ø³Ø© Ø§Ù„ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù…Ø®ØªÙ„Ø·: {e}")
            return False

    def _display_loop(self):
        """Ø­Ù„Ù‚Ø© Ø§Ù„Ø¹Ø±Ø¶ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
        
        while self.is_active:
            try:
                if not self.spatial_mapping.camera or not self.spatial_mapping.camera.isOpened():
                    break
                
                ret, frame = self.spatial_mapping.camera.read()
                if not ret:
                    continue
                
                # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø®ØªÙ„Ø·
                mixed_frame = self._create_mixed_display(frame)
                
                # Ø¹Ø±Ø¶ Ø§Ù„Ø¥Ø·Ø§Ø±
                cv2.imshow("Mixed Reality", mixed_frame)
                
                # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£Ø­Ø¯Ø§Ø« Ù„ÙˆØ­Ø© Ø§Ù„Ù…ÙØ§ØªÙŠØ­
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q'):
                    break
                elif key == ord('m'):
                    await self._toggle_main_menu()
                elif key == ord('i'):
                    self.display_config["show_debug_info"] = not self.display_config["show_debug_info"]
                
            except Exception as e:
                self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø­Ù„Ù‚Ø© Ø§Ù„Ø¹Ø±Ø¶: {e}")
                break

    def _create_mixed_display(self, frame: np.ndarray) -> np.ndarray:
        """Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø®ØªÙ„Ø·"""
        
        mixed_frame = frame.copy()
        
        try:
            # Ø±Ø³Ù… Ø§Ù„Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…ÙƒØ§Ù†ÙŠØ©
            if self.display_config["show_spatial_map"]:
                mixed_frame = self._draw_spatial_map(mixed_frame)
            
            # Ø±Ø³Ù… Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
            if self.display_config["show_virtual_objects"]:
                mixed_frame = self._draw_virtual_objects(mixed_frame)
            
            # Ø±Ø³Ù… ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
            if self.display_config["show_ui_elements"]:
                mixed_frame = self._draw_ui_elements(mixed_frame)
            
            # Ø±Ø³Ù… Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØµØ­ÙŠØ­
            if self.display_config["show_debug_info"]:
                mixed_frame = self._draw_debug_info(mixed_frame)
            
        except Exception as e:
            self.logger.warning(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø®ØªÙ„Ø·: {e}")
        
        return mixed_frame

    def _draw_spatial_map(self, frame: np.ndarray) -> np.ndarray:
        """Ø±Ø³Ù… Ø§Ù„Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…ÙƒØ§Ù†ÙŠØ©"""
        
        try:
            for obj_id, obj in self.spatial_mapping.spatial_map.items():
                if not obj.is_virtual:
                    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠ Ø¥Ù„Ù‰ Ø¥Ø­Ø¯Ø§Ø«ÙŠØ§Øª Ø§Ù„Ø´Ø§Ø´Ø©
                    screen_pos = self._world_to_screen(obj.position)
                    
                    if screen_pos:
                        x, y = screen_pos
                        
                        # Ø±Ø³Ù… Ù†Ù‚Ø·Ø© Ù„Ù„ÙƒØ§Ø¦Ù†
                        color = self._get_object_color(obj.object_type)
                        cv2.circle(frame, (int(x), int(y)), 5, color, -1)
                        
                        # Ø±Ø³Ù… ØªØ³Ù…ÙŠØ©
                        cv2.putText(
                            frame,
                            f"{obj.object_type}_{obj_id[-2:]}",
                            (int(x) + 10, int(y) - 10),
                            cv2.FONT_HERSHEY_SIMPLEX,
                            0.5,
                            color,
                            1
                        )
            
        except Exception as e:
            self.logger.warning(f"Ø®Ø·Ø£ ÙÙŠ Ø±Ø³Ù… Ø§Ù„Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…ÙƒØ§Ù†ÙŠØ©: {e}")
        
        return frame

    def _world_to_screen(self, world_pos: Tuple[float, float, float]) -> Optional[Tuple[int, int]]:
        """ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠ Ø¥Ù„Ù‰ Ø¥Ø­Ø¯Ø§Ø«ÙŠØ§Øª Ø§Ù„Ø´Ø§Ø´Ø©"""
        
        try:
            if self.spatial_mapping.camera_matrix is None:
                return None
            
            x, y, z = world_pos
            
            if z <= 0:
                return None
            
            # Ø§Ù„Ø¥Ø³Ù‚Ø§Ø· Ø¹Ù„Ù‰ Ø§Ù„Ø´Ø§Ø´Ø©
            fx = self.spatial_mapping.camera_matrix[0, 0]
            fy = self.spatial_mapping.camera_matrix[1, 1]
            cx = self.spatial_mapping.camera_matrix[0, 2]
            cy = self.spatial_mapping.camera_matrix[1, 2]
            
            screen_x = int((x * fx / z) + cx)
            screen_y = int((y * fy / z) + cy)
            
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø­Ø¯ÙˆØ¯
            if 0 <= screen_x < 1280 and 0 <= screen_y < 720:
                return (screen_x, screen_y)
            
            return None
            
        except Exception as e:
            self.logger.warning(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¥Ø­Ø¯Ø§Ø«ÙŠØ§Øª: {e}")
            return None

    def _get_object_color(self, object_type: str) -> Tuple[int, int, int]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù„ÙˆÙ† Ø§Ù„ÙƒØ§Ø¦Ù† Ø­Ø³Ø¨ Ø§Ù„Ù†ÙˆØ¹"""
        
        colors = {
            "hand": (0, 255, 0),      # Ø£Ø®Ø¶Ø±
            "face": (255, 0, 0),      # Ø£Ø­Ù…Ø±
            "unknown": (0, 0, 255),   # Ø£Ø²Ø±Ù‚
            "virtual": (255, 255, 0), # Ø£ØµÙØ±
            "ui": (255, 0, 255)       # Ø£Ø±Ø¬ÙˆØ§Ù†ÙŠ
        }
        
        return colors.get(object_type, (128, 128, 128))

    def _draw_virtual_objects(self, frame: np.ndarray) -> np.ndarray:
        """Ø±Ø³Ù… Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©"""
        
        try:
            for obj_id, obj in self.virtual_objects.items():
                screen_pos = self._world_to_screen(obj.position)
                
                if screen_pos:
                    x, y = screen_pos
                    
                    # Ø±Ø³Ù… Ø§Ù„ÙƒØ§Ø¦Ù† Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ
                    if obj.object_type == "cube":
                        self._draw_virtual_cube(frame, (x, y), obj.scale[0] * 50)
                    elif obj.object_type == "sphere":
                        self._draw_virtual_sphere(frame, (x, y), obj.scale[0] * 30)
                    elif obj.object_type == "text":
                        self._draw_virtual_text(frame, (x, y), obj.properties.get("text", ""))
            
        except Exception as e:
            self.logger.warning(f"Ø®Ø·Ø£ ÙÙŠ Ø±Ø³Ù… Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©: {e}")
        
        return frame

    def _draw_virtual_cube(self, frame: np.ndarray, center: Tuple[int, int], size: int):
        """Ø±Ø³Ù… Ù…ÙƒØ¹Ø¨ Ø§ÙØªØ±Ø§Ø¶ÙŠ"""
        
        x, y = center
        half_size = size // 2
        
        # Ø±Ø³Ù… Ù…Ø³ØªØ·ÙŠÙ„ Ø¨Ø³ÙŠØ· Ù„ØªÙ…Ø«ÙŠÙ„ Ø§Ù„Ù…ÙƒØ¹Ø¨
        cv2.rectangle(
            frame,
            (x - half_size, y - half_size),
            (x + half_size, y + half_size),
            (255, 255, 0),
            2
        )
        
        # Ø¥Ø¶Ø§ÙØ© ØªØ£Ø«ÙŠØ± Ø«Ù„Ø§Ø«ÙŠ Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯
        offset = size // 4
        cv2.line(frame, (x - half_size, y - half_size), (x - half_size + offset, y - half_size - offset), (255, 255, 0), 2)
        cv2.line(frame, (x + half_size, y - half_size), (x + half_size + offset, y - half_size - offset), (255, 255, 0), 2)
        cv2.line(frame, (x + half_size, y + half_size), (x + half_size + offset, y + half_size - offset), (255, 255, 0), 2)
        cv2.line(frame, (x - half_size, y + half_size), (x - half_size + offset, y + half_size - offset), (255, 255, 0), 2)

    def _draw_virtual_sphere(self, frame: np.ndarray, center: Tuple[int, int], radius: int):
        """Ø±Ø³Ù… ÙƒØ±Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ©"""
        
        cv2.circle(frame, center, radius, (0, 255, 255), 2)
        cv2.circle(frame, center, radius // 2, (0, 255, 255), 1)

    def _draw_virtual_text(self, frame: np.ndarray, position: Tuple[int, int], text: str):
        """Ø±Ø³Ù… Ù†Øµ Ø§ÙØªØ±Ø§Ø¶ÙŠ"""
        
        cv2.putText(
            frame,
            text,
            position,
            cv2.FONT_HERSHEY_SIMPLEX,
            1.0,
            (0, 255, 255),
            2
        )

    def _draw_ui_elements(self, frame: np.ndarray) -> np.ndarray:
        """Ø±Ø³Ù… Ø¹Ù†Ø§ØµØ± ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
        
        try:
            for element_id, element in self.spatial_ui_elements.items():
                if not element.get("visible", True):
                    continue
                
                screen_pos = self._world_to_screen(element["position"])
                
                if screen_pos:
                    if element["type"] == "menu":
                        self._draw_menu(frame, screen_pos, element)
                    elif element["type"] == "panel":
                        self._draw_panel(frame, screen_pos, element)
            
        except Exception as e:
            self.logger.warning(f"Ø®Ø·Ø£ ÙÙŠ Ø±Ø³Ù… ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {e}")
        
        return frame

    def _draw_menu(self, frame: np.ndarray, position: Tuple[int, int], menu: Dict[str, Any]):
        """Ø±Ø³Ù… Ù‚Ø§Ø¦Ù…Ø©"""
        
        x, y = position
        item_height = 40
        
        for i, item in enumerate(menu.get("items", [])):
            item_y = y + i * item_height
            
            # Ø±Ø³Ù… Ø®Ù„ÙÙŠØ© Ø§Ù„Ø¹Ù†ØµØ±
            cv2.rectangle(
                frame,
                (x, item_y),
                (x + 200, item_y + item_height - 5),
                (50, 50, 50),
                -1
            )
            
            # Ø±Ø³Ù… Ø§Ù„Ù†Øµ
            cv2.putText(
                frame,
                item["label"],
                (x + 10, item_y + 25),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.6,
                (255, 255, 255),
                2
            )

    def _draw_panel(self, frame: np.ndarray, position: Tuple[int, int], panel: Dict[str, Any]):
        """Ø±Ø³Ù… Ù„ÙˆØ­Ø©"""
        
        x, y = position
        width, height = panel.get("size", (200, 50))
        width = int(width * 500)
        height = int(height * 500)
        
        # Ø±Ø³Ù… Ø®Ù„ÙÙŠØ© Ø§Ù„Ù„ÙˆØ­Ø©
        overlay = frame.copy()
        cv2.rectangle(overlay, (x, y), (x + width, y + height), (0, 0, 0), -1)
        
        alpha = panel.get("background_color", [0, 0, 0, 0.7])[3]
        cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0, frame)
        
        # Ø±Ø³Ù… Ø§Ù„Ù†Øµ
        cv2.putText(
            frame,
            panel.get("content", ""),
            (x + 10, y + 30),
            cv2.FONT_HERSHEY_SIMPLEX,
            0.7,
            (255, 255, 255),
            2
        )

    def _draw_debug_info(self, frame: np.ndarray) -> np.ndarray:
        """Ø±Ø³Ù… Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØµØ­ÙŠØ­"""
        
        try:
            debug_info = [
                f"Objects: {len(self.spatial_mapping.spatial_map)}",
                f"Virtual: {len(self.virtual_objects)}",
                f"Mode: {self.current_mode}",
                f"Tracking: {self.spatial_mapping.is_tracking}",
                f"Quality: {self.spatial_mapping.tracking_quality:.1f}"
            ]
            
            for i, info in enumerate(debug_info):
                cv2.putText(
                    frame,
                    info,
                    (10, 30 + i * 25),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    0.6,
                    (0, 255, 0),
                    1
                )
            
        except Exception as e:
            self.logger.warning(f"Ø®Ø·Ø£ ÙÙŠ Ø±Ø³Ù… Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØµØ­ÙŠØ­: {e}")
        
        return frame

    async def create_virtual_object(
        self,
        object_type: str,
        position: Tuple[float, float, float],
        properties: Dict[str, Any] = None
    ) -> str:
        """Ø¥Ù†Ø´Ø§Ø¡ ÙƒØ§Ø¦Ù† Ø§ÙØªØ±Ø§Ø¶ÙŠ"""
        
        try:
            object_id = f"virtual_{len(self.virtual_objects)}"
            
            virtual_object = SpatialObject(
                object_id=object_id,
                position=position,
                rotation=(0.0, 0.0, 0.0),
                scale=(1.0, 1.0, 1.0),
                object_type=object_type,
                properties=properties or {},
                is_virtual=True,
                confidence=1.0
            )
            
            self.virtual_objects[object_id] = virtual_object
            
            self.logger.info(f"ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ ÙƒØ§Ø¦Ù† Ø§ÙØªØ±Ø§Ø¶ÙŠ: {object_id}")
            return object_id
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙƒØ§Ø¦Ù† Ø§ÙØªØ±Ø§Ø¶ÙŠ: {e}")
            return ""

    async def detect_gesture(self, hand_landmarks: List[Tuple[float, float, float]]) -> Optional[GestureCommand]:
        """ÙƒØ´Ù Ø§Ù„Ø¥ÙŠÙ…Ø§Ø¡Ø©"""
        
        try:
            if len(hand_landmarks) < 21:  # ÙŠØ¯ MediaPipe Ù„Ù‡Ø§ 21 Ù†Ù‚Ø·Ø©
                return None
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¥ÙŠÙ…Ø§Ø¡Ø§Øª Ø§Ù„Ø¨Ø³ÙŠØ·Ø©
            gesture = self._analyze_hand_gesture(hand_landmarks)
            
            if gesture:
                return GestureCommand(
                    gesture_type=gesture["type"],
                    confidence=gesture["confidence"],
                    parameters=gesture.get("parameters", {}),
                    timestamp=datetime.now(),
                    hand_landmarks=hand_landmarks
                )
            
            return None
            
        except Exception as e:
            self.logger.warning(f"Ø®Ø·Ø£ ÙÙŠ ÙƒØ´Ù Ø§Ù„Ø¥ÙŠÙ…Ø§Ø¡Ø©: {e}")
            return None

    def _analyze_hand_gesture(self, landmarks: List[Tuple[float, float, float]]) -> Optional[Dict[str, Any]]:
        """ØªØ­Ù„ÙŠÙ„ Ø¥ÙŠÙ…Ø§Ø¡Ø© Ø§Ù„ÙŠØ¯"""
        
        try:
            # Ù†Ù‚Ø§Ø· Ø§Ù„Ø£ØµØ§Ø¨Ø¹ (Ø£Ø·Ø±Ø§Ù Ø§Ù„Ø£ØµØ§Ø¨Ø¹)
            finger_tips = [4, 8, 12, 16, 20]  # Ø§Ù„Ø¥Ø¨Ù‡Ø§Ù…ØŒ Ø§Ù„Ø³Ø¨Ø§Ø¨Ø©ØŒ Ø§Ù„ÙˆØ³Ø·Ù‰ØŒ Ø§Ù„Ø¨Ù†ØµØ±ØŒ Ø§Ù„Ø®Ù†ØµØ±
            finger_pips = [3, 6, 10, 14, 18]  # Ø§Ù„Ù…ÙØ§ØµÙ„ Ø§Ù„ÙˆØ³Ø·Ù‰
            
            # Ø­Ø³Ø§Ø¨ Ø­Ø§Ù„Ø© ÙƒÙ„ Ø¥ØµØ¨Ø¹ (Ù…ÙØªÙˆØ­/Ù…ØºÙ„Ù‚)
            fingers_up = []
            
            # Ø§Ù„Ø¥Ø¨Ù‡Ø§Ù… (Ù…Ù‚Ø§Ø±Ù†Ø© x)
            if landmarks[finger_tips[0]][0] > landmarks[finger_pips[0]][0]:
                fingers_up.append(1)
            else:
                fingers_up.append(0)
            
            # Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ø£ØµØ§Ø¨Ø¹ (Ù…Ù‚Ø§Ø±Ù†Ø© y)
            for i in range(1, 5):
                if landmarks[finger_tips[i]][1] < landmarks[finger_pips[i]][1]:
                    fingers_up.append(1)
                else:
                    fingers_up.append(0)
            
            # ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ø¥ÙŠÙ…Ø§Ø¡Ø©
            total_fingers = sum(fingers_up)
            
            if total_fingers == 0:
                return {"type": "fist", "confidence": 0.9}
            elif total_fingers == 1 and fingers_up[1] == 1:  # Ø§Ù„Ø³Ø¨Ø§Ø¨Ø© ÙÙ‚Ø·
                return {"type": "point", "confidence": 0.8}
            elif total_fingers == 2 and fingers_up[1] == 1 and fingers_up[2] == 1:  # Ø³Ø¨Ø§Ø¨Ø© ÙˆÙˆØ³Ø·Ù‰
                return {"type": "peace", "confidence": 0.8}
            elif total_fingers == 5:
                return {"type": "open_hand", "confidence": 0.9}
            elif total_fingers == 1 and fingers_up[0] == 1:  # Ø§Ù„Ø¥Ø¨Ù‡Ø§Ù… ÙÙ‚Ø·
                return {"type": "thumbs_up", "confidence": 0.8}
            
            return {"type": "unknown", "confidence": 0.5}
            
        except Exception as e:
            self.logger.warning(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø¥ÙŠÙ…Ø§Ø¡Ø© Ø§Ù„ÙŠØ¯: {e}")
            return None

    async def process_voice_command(self, command: str) -> Dict[str, Any]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£Ù…Ø± ØµÙˆØªÙŠ ÙÙŠ Ø§Ù„ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù…Ø®ØªÙ„Ø·"""
        
        try:
            command_lower = command.lower().strip()
            
            if "Ø£Ù†Ø´Ø¦" in command_lower or "create" in command_lower:
                # Ø£Ù…Ø± Ø¥Ù†Ø´Ø§Ø¡ ÙƒØ§Ø¦Ù†
                if "Ù…ÙƒØ¹Ø¨" in command_lower or "cube" in command_lower:
                    object_id = await self.create_virtual_object("cube", (0.0, 0.0, -1.0))
                    return {"success": True, "action": "create_cube", "object_id": object_id}
                
                elif "ÙƒØ±Ø©" in command_lower or "sphere" in command_lower:
                    object_id = await self.create_virtual_object("sphere", (0.0, 0.0, -1.0))
                    return {"success": True, "action": "create_sphere", "object_id": object_id}
                
                elif "Ù†Øµ" in command_lower or "text" in command_lower:
                    object_id = await self.create_virtual_object(
                        "text", 
                        (0.0, 0.0, -1.0),
                        {"text": "Ù…Ø±Ø­Ø¨Ø§Ù‹ Ø¨Ùƒ!"}
                    )
                    return {"success": True, "action": "create_text", "object_id": object_id}
            
            elif "Ø§Ø¸Ù‡Ø± Ø§Ù„Ù‚Ø§Ø¦Ù…Ø©" in command_lower or "show menu" in command_lower:
                await self._toggle_main_menu()
                return {"success": True, "action": "toggle_menu"}
            
            elif "Ø§Ù…Ø³Ø­" in command_lower or "clear" in command_lower:
                self.virtual_objects.clear()
                return {"success": True, "action": "clear_objects"}
            
            elif "Ù…Ø¹Ù„ÙˆÙ…Ø§Øª" in command_lower or "info" in command_lower:
                self.display_config["show_debug_info"] = not self.display_config["show_debug_info"]
                return {"success": True, "action": "toggle_debug"}
            
            return {"success": False, "error": "Ø£Ù…Ø± ØºÙŠØ± Ù…ÙÙ‡ÙˆÙ…"}
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ù…Ø± Ø§Ù„ØµÙˆØªÙŠ: {e}")
            return {"success": False, "error": str(e)}

    async def _toggle_main_menu(self):
        """ØªØ¨Ø¯ÙŠÙ„ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
        
        if "main_menu" in self.spatial_ui_elements:
            current_state = self.spatial_ui_elements["main_menu"]["visible"]
            self.spatial_ui_elements["main_menu"]["visible"] = not current_state

    async def get_spatial_analytics(self) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„Ø§Øª Ø§Ù„Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…ÙƒØ§Ù†ÙŠØ©"""
        
        try:
            # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª
            object_stats = {}
            for obj in self.spatial_mapping.spatial_map.values():
                obj_type = obj.object_type
                if obj_type not in object_stats:
                    object_stats[obj_type] = {"count": 0, "avg_confidence": 0.0}
                
                object_stats[obj_type]["count"] += 1
                object_stats[obj_type]["avg_confidence"] += obj.confidence
            
            # Ø­Ø³Ø§Ø¨ Ù…ØªÙˆØ³Ø· Ø§Ù„Ø«Ù‚Ø©
            for stats in object_stats.values():
                stats["avg_confidence"] /= stats["count"]
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…ÙƒØ§Ù†ÙŠ
            positions = [obj.position for obj in self.spatial_mapping.spatial_map.values()]
            spatial_analysis = self._analyze_spatial_distribution(positions)
            
            return {
                "total_objects": len(self.spatial_mapping.spatial_map),
                "virtual_objects": len(self.virtual_objects),
                "object_statistics": object_stats,
                "spatial_distribution": spatial_analysis,
                "tracking_quality": self.spatial_mapping.tracking_quality,
                "system_status": {
                    "is_active": self.is_active,
                    "current_mode": self.current_mode,
                    "camera_status": self.spatial_mapping.camera is not None
                }
            }
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª Ø§Ù„Ù…ÙƒØ§Ù†ÙŠØ©: {e}")
            return {"error": str(e)}

    def _analyze_spatial_distribution(self, positions: List[Tuple[float, float, float]]) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…ÙƒØ§Ù†ÙŠ"""
        
        if not positions:
            return {"center": (0, 0, 0), "spread": 0.0, "density": 0.0}
        
        try:
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø±ÙƒØ²
            center_x = sum(pos[0] for pos in positions) / len(positions)
            center_y = sum(pos[1] for pos in positions) / len(positions)
            center_z = sum(pos[2] for pos in positions) / len(positions)
            center = (center_x, center_y, center_z)
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§Ù†ØªØ´Ø§Ø±
            distances = [
                math.sqrt((pos[0] - center_x)**2 + (pos[1] - center_y)**2 + (pos[2] - center_z)**2)
                for pos in positions
            ]
            spread = max(distances) if distances else 0.0
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„ÙƒØ«Ø§ÙØ©
            volume = (4/3) * math.pi * (spread**3) if spread > 0 else 1.0
            density = len(positions) / volume
            
            return {
                "center": center,
                "spread": spread,
                "density": density,
                "object_count": len(positions)
            }
            
        except Exception as e:
            self.logger.warning(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…ÙƒØ§Ù†ÙŠ: {e}")
            return {"center": (0, 0, 0), "spread": 0.0, "density": 0.0}

    async def stop_mixed_reality_session(self):
        """Ø¥ÙŠÙ‚Ø§Ù Ø¬Ù„Ø³Ø© Ø§Ù„ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù…Ø®ØªÙ„Ø·"""
        
        try:
            self.is_active = False
            self.spatial_mapping.is_tracking = False
            
            if self.spatial_mapping.camera:
                self.spatial_mapping.camera.release()
            
            cv2.destroyAllWindows()
            
            self.logger.info("ğŸ”´ ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø¬Ù„Ø³Ø© Ø§Ù„ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù…Ø®ØªÙ„Ø·")
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥ÙŠÙ‚Ø§Ù Ø¬Ù„Ø³Ø© Ø§Ù„ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù…Ø®ØªÙ„Ø·: {e}")

# Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø«ÙŠÙ„ Ø¹Ø§Ù…
mixed_reality_engine = MixedRealityEngine()

async def get_mixed_reality_engine() -> MixedRealityEngine:
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø­Ø±Ùƒ Ø§Ù„ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù…Ø®ØªÙ„Ø·"""
    return mixed_reality_engine

if __name__ == "__main__":
    async def test_mixed_reality():
        """Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø­Ø±Ùƒ Ø§Ù„ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù…Ø®ØªÙ„Ø·"""
        print("ğŸ¥½ Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø­Ø±Ùƒ Ø§Ù„ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù…Ø®ØªÙ„Ø·")
        print("=" * 50)
        
        engine = await get_mixed_reality_engine()
        
        # Ø¨Ø¯Ø¡ Ø§Ù„Ø¬Ù„Ø³Ø©
        success = await engine.start_mixed_reality_session()
        
        if success:
            print("âœ… ØªÙ… Ø¨Ø¯Ø¡ Ø¬Ù„Ø³Ø© Ø§Ù„ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù…Ø®ØªÙ„Ø·")
            print("ğŸ® Ø§Ù„Ø£ÙˆØ§Ù…Ø± Ø§Ù„Ù…ØªØ§Ø­Ø©:")
            print("   â€¢ 'q' - Ø®Ø±ÙˆØ¬")
            print("   â€¢ 'm' - ØªØ¨Ø¯ÙŠÙ„ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø©")
            print("   â€¢ 'i' - Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØµØ­ÙŠØ­")
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø¨Ø¹Ø¶ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª Ø§Ù„ØªØ¬Ø±ÙŠØ¨ÙŠØ©
            await engine.create_virtual_object("cube", (0.0, 0.0, -1.0))
            await engine.create_virtual_object("sphere", (0.5, 0.0, -1.5))
            await engine.create_virtual_object("text", (-0.5, 0.0, -1.2), {"text": "Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Øµ"})
            
            # Ø§Ù†ØªØ¸Ø§Ø± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
            print("â³ Ø§Ø¶ØºØ· 'q' ÙÙŠ Ù†Ø§ÙØ°Ø© Ø§Ù„Ø¹Ø±Ø¶ Ù„Ù„Ø®Ø±ÙˆØ¬...")
            
            try:
                while engine.is_active:
                    await asyncio.sleep(1)
            except KeyboardInterrupt:
                pass
            
            await engine.stop_mixed_reality_session()
        else:
            print("âŒ ÙØ´Ù„ ÙÙŠ Ø¨Ø¯Ø¡ Ø¬Ù„Ø³Ø© Ø§Ù„ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù…Ø®ØªÙ„Ø·")
    
    asyncio.run(test_mixed_reality())
