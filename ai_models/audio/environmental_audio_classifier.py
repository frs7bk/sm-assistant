
import librosa
import numpy as np
import torch
import torch.nn as nn
from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor
import json
from datetime import datetime
import sqlite3

class EnvironmentalAudioClassifier:
    """
    Ù†Ø¸Ø§Ù… ØªØµÙ†ÙŠÙ Ø§Ù„Ø£ØµÙˆØ§Øª Ø§Ù„Ø¨ÙŠØ¦ÙŠØ© ÙˆØ§Ù„ØªÙ…ÙŠÙŠØ² Ø¨ÙŠÙ†Ù‡Ø§
    ÙŠÙ…ÙƒÙ†Ù‡ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰:
    - Ø£ØµÙˆØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹Ø© (Ù…Ø·Ø±ØŒ Ø±ÙŠØ§Ø­ØŒ Ø·ÙŠÙˆØ±)
    - Ø£ØµÙˆØ§Øª Ø§Ù„Ù…Ù†Ø²Ù„ (ØªÙ„ÙØ²ÙŠÙˆÙ†ØŒ Ù…ÙˆØ³ÙŠÙ‚Ù‰ØŒ Ø£Ø¬Ù‡Ø²Ø©)
    - Ø£ØµÙˆØ§Øª Ø§Ù„Ø´Ø§Ø±Ø¹ (Ø³ÙŠØ§Ø±Ø§ØªØŒ Ø£Ø¬Ø±Ø§Ø³ØŒ Ø¥Ù†Ø´Ø§Ø¡Ø§Øª)
    - Ø£ØµÙˆØ§Øª Ø¨Ø´Ø±ÙŠØ© (ÙƒÙ„Ø§Ù…ØŒ Ø¶Ø­ÙƒØŒ Ø¨ÙƒØ§Ø¡)
    - Ø£ØµÙˆØ§Øª Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª
    """
    
    def __init__(self):
        print("ğŸ”Š ØªØ­Ù…ÙŠÙ„ Ù†Ø¸Ø§Ù… ØªØµÙ†ÙŠÙ Ø§Ù„Ø£ØµÙˆØ§Øª Ø§Ù„Ø¨ÙŠØ¦ÙŠØ©...")
        
        # ÙØ¦Ø§Øª Ø§Ù„Ø£ØµÙˆØ§Øª
        self.sound_categories = {
            'human_voices': {
                'speech': 'ÙƒÙ„Ø§Ù… Ø¨Ø´Ø±ÙŠ',
                'laughter': 'Ø¶Ø­Ùƒ',
                'crying': 'Ø¨ÙƒØ§Ø¡',
                'singing': 'ØºÙ†Ø§Ø¡',
                'whispering': 'Ù‡Ù…Ø³',
                'shouting': 'ØµØ±Ø§Ø®'
            },
            'nature_sounds': {
                'rain': 'Ù…Ø·Ø±',
                'wind': 'Ø±ÙŠØ§Ø­',
                'thunder': 'Ø±Ø¹Ø¯',
                'birds': 'Ø¹ØµØ§ÙÙŠØ±',
                'water_flow': 'ØªØ¯ÙÙ‚ Ø§Ù„Ù…Ø§Ø¡',
                'ocean_waves': 'Ø£Ù…ÙˆØ§Ø¬ Ø§Ù„Ø¨Ø­Ø±'
            },
            'household_sounds': {
                'tv': 'ØªÙ„ÙØ²ÙŠÙˆÙ†',
                'music': 'Ù…ÙˆØ³ÙŠÙ‚Ù‰',
                'washing_machine': 'ØºØ³Ø§Ù„Ø©',
                'microwave': 'Ù…ÙŠÙƒØ±ÙˆÙˆÙŠÙ',
                'vacuum_cleaner': 'Ù…ÙƒÙ†Ø³Ø© ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ©',
                'door_bell': 'Ø¬Ø±Ø³ Ø§Ù„Ø¨Ø§Ø¨',
                'phone_ring': 'Ø±Ù†ÙŠÙ† Ù‡Ø§ØªÙ'
            },
            'street_sounds': {
                'car_engine': 'Ù…Ø­Ø±Ùƒ Ø³ÙŠØ§Ø±Ø©',
                'motorcycle': 'Ø¯Ø±Ø§Ø¬Ø© Ù†Ø§Ø±ÙŠØ©',
                'horn': 'Ø¨ÙˆÙ‚ Ø³ÙŠØ§Ø±Ø©',
                'construction': 'Ø£Ø¹Ù…Ø§Ù„ Ø¥Ù†Ø´Ø§Ø¡',
                'siren': 'ØµÙØ§Ø±Ø© Ø¥Ù†Ø°Ø§Ø±',
                'footsteps': 'Ø®Ø·ÙˆØ§Øª Ø£Ù‚Ø¯Ø§Ù…'
            },
            'animal_sounds': {
                'dog_bark': 'Ù†Ø¨Ø§Ø­ ÙƒÙ„Ø¨',
                'cat_meow': 'Ù…ÙˆØ§Ø¡ Ù‚Ø·Ø©',
                'bird_chirp': 'ØªØºØ±ÙŠØ¯ Ø·Ø§Ø¦Ø±',
                'horse_neigh': 'ØµÙ‡ÙŠÙ„ Ø­ØµØ§Ù†',
                'cow_moo': 'Ø®ÙˆØ§Ø± Ø¨Ù‚Ø±Ø©'
            },
            'mechanical_sounds': {
                'fan': 'Ù…Ø±ÙˆØ­Ø©',
                'air_conditioner': 'Ù…ÙƒÙŠÙ Ù‡ÙˆØ§Ø¡',
                'computer_fan': 'Ù…Ø±ÙˆØ­Ø© ÙƒÙ…Ø¨ÙŠÙˆØªØ±',
                'drill': 'Ù…Ø«Ù‚Ø§Ø¨',
                'saw': 'Ù…Ù†Ø´Ø§Ø±'
            }
        }
        
        # Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø£ØµÙˆØ§Øª Ø§Ù„Ù…ÙƒØªØ´ÙØ©
        self.db_path = "detected_sounds.db"
        self._init_database()
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        self.sample_rate = 22050
        self.window_size = 2.0  # Ø«Ø§Ù†ÙŠØªØ§Ù† Ù„ÙƒÙ„ Ù†Ø§ÙØ°Ø©
        self.overlap = 0.5  # ØªØ¯Ø§Ø®Ù„ 50%
        
        # ØªØ­Ù…ÙŠÙ„ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØµÙ†ÙŠÙ
        self._load_models()
        
        print("âœ… Ù†Ø¸Ø§Ù… ØªØµÙ†ÙŠÙ Ø§Ù„Ø£ØµÙˆØ§Øª Ø¬Ø§Ù‡Ø²!")
    
    def _init_database(self):
        """Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS detected_sounds (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT,
                sound_category TEXT,
                sound_type TEXT,
                sound_name TEXT,
                confidence REAL,
                duration REAL,
                audio_features TEXT,
                context_info TEXT
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS sound_sessions (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                session_id TEXT,
                start_time TEXT,
                end_time TEXT,
                total_sounds INTEGER,
                dominant_category TEXT,
                environment_type TEXT,
                summary TEXT
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def _load_models(self):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬"""
        try:
            # Ù†Ù…ÙˆØ°Ø¬ Wav2Vec2 Ù„Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø¹Ø§Ù…
            self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
                "facebook/wav2vec2-base-960h"
            )
            
            # Ù†Ù…Ø§Ø°Ø¬ Ù…Ø®ØµØµØ© Ù„ÙƒÙ„ ÙØ¦Ø© (Ø³ÙŠØªÙ… Ø§Ø³ØªØ¨Ø¯Ø§Ù„Ù‡Ø§ Ø¨Ù†Ù…Ø§Ø°Ø¬ Ù…Ø¯Ø±Ø¨Ø©)
            self.classifiers = {}
            
            print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¨Ù†Ø¬Ø§Ø­")
            
        except Exception as e:
            print(f"âš ï¸ ØªØ­Ø°ÙŠØ±: {e}")
            print("ğŸ“ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ø¨Ù†ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„ØµÙˆØªÙŠØ©")
    
    def extract_audio_features(self, audio_path, start_time=0, duration=None):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„ØµÙˆØªÙŠØ©"""
        try:
            # ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØª
            y, sr = librosa.load(audio_path, sr=self.sample_rate, 
                               offset=start_time, duration=duration)
            
            if len(y) < sr * 0.1:  # Ø£Ù‚Ù„ Ù…Ù† 100ms
                return None
            
            features = {}
            
            # Ø§Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
            features['duration'] = len(y) / sr
            features['rms_energy'] = np.sqrt(np.mean(y**2))
            features['zero_crossing_rate'] = np.mean(librosa.feature.zero_crossing_rate(y)[0])
            
            # MFCC
            mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
            features['mfcc_mean'] = np.mean(mfcc, axis=1)
            features['mfcc_std'] = np.std(mfcc, axis=1)
            
            # Spectral features
            spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]
            features['spectral_centroid'] = np.mean(spectral_centroids)
            features['spectral_centroid_std'] = np.std(spectral_centroids)
            
            spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)[0]
            features['spectral_rolloff'] = np.mean(spectral_rolloff)
            
            spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)[0]
            features['spectral_bandwidth'] = np.mean(spectral_bandwidth)
            
            # Chroma
            chroma = librosa.feature.chroma_stft(y=y, sr=sr)
            features['chroma_mean'] = np.mean(chroma, axis=1)
            
            # Tempo
            tempo, _ = librosa.beat.beat_track(y=y, sr=sr)
            features['tempo'] = tempo
            
            # Pitch
            pitches, magnitudes = librosa.core.piptrack(y=y, sr=sr)
            pitch_values = []
            for t in range(pitches.shape[1]):
                index = magnitudes[:, t].argmax()
                pitch = pitches[index, t]
                if pitch > 0:
                    pitch_values.append(pitch)
            
            if pitch_values:
                features['pitch_mean'] = np.mean(pitch_values)
                features['pitch_std'] = np.std(pitch_values)
            else:
                features['pitch_mean'] = 0
                features['pitch_std'] = 0
            
            # Harmonic and percussive components
            y_harmonic, y_percussive = librosa.effects.hpss(y)
            features['harmonic_ratio'] = np.sum(y_harmonic**2) / np.sum(y**2)
            features['percussive_ratio'] = np.sum(y_percussive**2) / np.sum(y**2)
            
            return features
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø®ØµØ§Ø¦Øµ: {e}")
            return None
    
    def classify_sound_by_features(self, features):
        """ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø®ØµØ§Ø¦Øµ"""
        
        classifications = []
        
        # ØªØµÙ†ÙŠÙ Ø§Ù„Ø£ØµÙˆØ§Øª Ø§Ù„Ø¨Ø´Ø±ÙŠØ©
        if self._is_human_voice(features):
            voice_type = self._classify_human_voice(features)
            classifications.append({
                'category': 'human_voices',
                'type': voice_type,
                'name': self.sound_categories['human_voices'].get(voice_type, voice_type),
                'confidence': 0.8
            })
        
        # ØªØµÙ†ÙŠÙ Ø£ØµÙˆØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹Ø©
        nature_type = self._classify_nature_sound(features)
        if nature_type:
            classifications.append({
                'category': 'nature_sounds',
                'type': nature_type,
                'name': self.sound_categories['nature_sounds'].get(nature_type, nature_type),
                'confidence': 0.7
            })
        
        # ØªØµÙ†ÙŠÙ Ø§Ù„Ø£ØµÙˆØ§Øª Ø§Ù„Ù…Ù†Ø²Ù„ÙŠØ©
        household_type = self._classify_household_sound(features)
        if household_type:
            classifications.append({
                'category': 'household_sounds',
                'type': household_type,
                'name': self.sound_categories['household_sounds'].get(household_type, household_type),
                'confidence': 0.6
            })
        
        # ØªØµÙ†ÙŠÙ Ø£ØµÙˆØ§Øª Ø§Ù„Ø´Ø§Ø±Ø¹
        street_type = self._classify_street_sound(features)
        if street_type:
            classifications.append({
                'category': 'street_sounds',
                'type': street_type,
                'name': self.sound_categories['street_sounds'].get(street_type, street_type),
                'confidence': 0.6
            })
        
        # ØªØµÙ†ÙŠÙ Ø£ØµÙˆØ§Øª Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª
        animal_type = self._classify_animal_sound(features)
        if animal_type:
            classifications.append({
                'category': 'animal_sounds',
                'type': animal_type,
                'name': self.sound_categories['animal_sounds'].get(animal_type, animal_type),
                'confidence': 0.7
            })
        
        # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„Ø«Ù‚Ø©
        classifications.sort(key=lambda x: x['confidence'], reverse=True)
        
        return classifications
    
    def _is_human_voice(self, features):
        """ÙƒØ´Ù Ø§Ù„ØµÙˆØª Ø§Ù„Ø¨Ø´Ø±ÙŠ"""
        # Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„ØµÙˆØª Ø§Ù„Ø¨Ø´Ø±ÙŠ
        pitch_in_range = 80 <= features['pitch_mean'] <= 500
        has_formants = features['spectral_centroid'] > 500
        harmonic_content = features['harmonic_ratio'] > 0.3
        
        return pitch_in_range and has_formants and harmonic_content
    
    def _classify_human_voice(self, features):
        """ØªØµÙ†ÙŠÙ Ù†ÙˆØ¹ Ø§Ù„ØµÙˆØª Ø§Ù„Ø¨Ø´Ø±ÙŠ"""
        
        energy = features['rms_energy']
        pitch_std = features['pitch_std']
        spectral_bandwidth = features['spectral_bandwidth']
        
        if energy > 0.1 and pitch_std > 50:
            return 'shouting'
        elif energy < 0.02:
            return 'whispering'
        elif spectral_bandwidth > 2000 and pitch_std > 30:
            return 'laughter'
        elif pitch_std > 40:
            return 'singing'
        else:
            return 'speech'
    
    def _classify_nature_sound(self, features):
        """ØªØµÙ†ÙŠÙ Ø£ØµÙˆØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹Ø©"""
        
        energy = features['rms_energy']
        zcr = features['zero_crossing_rate']
        spectral_centroid = features['spectral_centroid']
        
        if zcr > 0.15 and energy < 0.05:
            return 'wind'
        elif features['percussive_ratio'] > 0.7 and energy > 0.1:
            return 'thunder'
        elif spectral_centroid > 2000 and zcr > 0.1:
            return 'birds'
        elif zcr > 0.2 and spectral_centroid < 1000:
            return 'rain'
        elif features['harmonic_ratio'] > 0.5 and spectral_centroid < 500:
            return 'water_flow'
        
        return None
    
    def _classify_household_sound(self, features):
        """ØªØµÙ†ÙŠÙ Ø§Ù„Ø£ØµÙˆØ§Øª Ø§Ù„Ù…Ù†Ø²Ù„ÙŠØ©"""
        
        tempo = features['tempo']
        energy = features['rms_energy']
        spectral_rolloff = features['spectral_rolloff']
        
        if features['harmonic_ratio'] > 0.6 and spectral_rolloff > 3000:
            return 'music'
        elif tempo > 0 and features['percussive_ratio'] > 0.5:
            return 'washing_machine'
        elif energy < 0.03 and spectral_rolloff < 2000:
            return 'tv'
        elif features['pitch_mean'] > 1000:
            return 'microwave'
        
        return None
    
    def _classify_street_sound(self, features):
        """ØªØµÙ†ÙŠÙ Ø£ØµÙˆØ§Øª Ø§Ù„Ø´Ø§Ø±Ø¹"""
        
        energy = features['rms_energy']
        spectral_centroid = features['spectral_centroid']
        
        if energy > 0.15 and spectral_centroid < 1000:
            return 'car_engine'
        elif energy > 0.2 and spectral_centroid > 2000:
            return 'motorcycle'
        elif features['pitch_mean'] > 500 and energy > 0.1:
            return 'horn'
        elif features['percussive_ratio'] > 0.8:
            return 'construction'
        
        return None
    
    def _classify_animal_sound(self, features):
        """ØªØµÙ†ÙŠÙ Ø£ØµÙˆØ§Øª Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª"""
        
        pitch_mean = features['pitch_mean']
        energy = features['rms_energy']
        
        if 200 <= pitch_mean <= 800 and energy > 0.05:
            return 'dog_bark'
        elif pitch_mean > 300 and features['harmonic_ratio'] > 0.4:
            return 'cat_meow'
        elif pitch_mean > 1000:
            return 'bird_chirp'
        
        return None
    
    def analyze_audio_file(self, audio_path):
        """ØªØ­Ù„ÙŠÙ„ Ù…Ù„Ù ØµÙˆØªÙŠ ÙƒØ§Ù…Ù„"""
        
        print(f"ğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù„Ù: {audio_path}")
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¯Ø© Ø§Ù„Ù…Ù„Ù
        duration = librosa.get_duration(filename=audio_path)
        
        # ØªÙ‚Ø³ÙŠÙ… Ø¥Ù„Ù‰ Ù†ÙˆØ§ÙØ°
        window_duration = self.window_size
        overlap_duration = window_duration * self.overlap
        step = window_duration - overlap_duration
        
        detected_sounds = []
        
        for start_time in np.arange(0, duration - window_duration, step):
            features = self.extract_audio_features(
                audio_path, start_time, window_duration
            )
            
            if features:
                classifications = self.classify_sound_by_features(features)
                
                for classification in classifications:
                    sound_info = {
                        'start_time': start_time,
                        'end_time': start_time + window_duration,
                        'duration': window_duration,
                        'timestamp': datetime.now().isoformat(),
                        **classification
                    }
                    detected_sounds.append(sound_info)
                    
                    # Ø­ÙØ¸ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                    self._save_detected_sound(sound_info, features)
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        analysis = self._analyze_session(detected_sounds)
        
        return {
            'total_duration': duration,
            'detected_sounds': detected_sounds,
            'analysis': analysis
        }
    
    def _save_detected_sound(self, sound_info, features):
        """Ø­ÙØ¸ Ø§Ù„ØµÙˆØª Ø§Ù„Ù…ÙƒØªØ´Ù"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO detected_sounds 
            (timestamp, sound_category, sound_type, sound_name, confidence, 
             duration, audio_features, context_info)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            sound_info['timestamp'],
            sound_info['category'],
            sound_info['type'],
            sound_info['name'],
            sound_info['confidence'],
            sound_info['duration'],
            json.dumps(features),
            json.dumps({'start_time': sound_info['start_time']})
        ))
        
        conn.commit()
        conn.close()
    
    def _analyze_session(self, detected_sounds):
        """ØªØ­Ù„ÙŠÙ„ Ø¬Ù„Ø³Ø© Ø§Ù„Ø£ØµÙˆØ§Øª"""
        
        if not detected_sounds:
            return {'summary': 'Ù„Ù… ÙŠØªÙ… Ø§ÙƒØªØ´Ø§Ù Ø£ØµÙˆØ§Øª'}
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ÙØ¦Ø§Øª
        categories = {}
        for sound in detected_sounds:
            category = sound['category']
            categories[category] = categories.get(category, 0) + 1
        
        # Ø§Ù„ÙØ¦Ø© Ø§Ù„Ù…Ù‡ÙŠÙ…Ù†Ø©
        dominant_category = max(categories.items(), key=lambda x: x[1])[0]
        
        # ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ø¨ÙŠØ¦Ø©
        environment_type = self._determine_environment(categories)
        
        # Ù…Ù„Ø®Øµ
        summary = f"ØªÙ… Ø§ÙƒØªØ´Ø§Ù {len(detected_sounds)} ØµÙˆØª. "
        summary += f"Ø§Ù„Ø¨ÙŠØ¦Ø©: {environment_type}. "
        summary += f"Ø§Ù„ÙØ¦Ø© Ø§Ù„Ù…Ù‡ÙŠÙ…Ù†Ø©: {self._translate_category(dominant_category)}."
        
        return {
            'total_sounds': len(detected_sounds),
            'categories': categories,
            'dominant_category': dominant_category,
            'environment_type': environment_type,
            'summary': summary
        }
    
    def _determine_environment(self, categories):
        """ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ø¨ÙŠØ¦Ø©"""
        
        if 'nature_sounds' in categories and categories['nature_sounds'] > 2:
            return 'Ø¨ÙŠØ¦Ø© Ø·Ø¨ÙŠØ¹ÙŠØ©'
        elif 'household_sounds' in categories and categories['household_sounds'] > 2:
            return 'Ø¨ÙŠØ¦Ø© Ù…Ù†Ø²Ù„ÙŠØ©'
        elif 'street_sounds' in categories and categories['street_sounds'] > 2:
            return 'Ø¨ÙŠØ¦Ø© Ø­Ø¶Ø±ÙŠØ©'
        elif 'human_voices' in categories and categories['human_voices'] > 2:
            return 'Ø¨ÙŠØ¦Ø© Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ©'
        else:
            return 'Ø¨ÙŠØ¦Ø© Ù…Ø®ØªÙ„Ø·Ø©'
    
    def _translate_category(self, category):
        """ØªØ±Ø¬Ù…Ø© Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„ÙØ¦Ø§Øª"""
        translations = {
            'human_voices': 'Ø£ØµÙˆØ§Øª Ø¨Ø´Ø±ÙŠØ©',
            'nature_sounds': 'Ø£ØµÙˆØ§Øª Ø·Ø¨ÙŠØ¹Ø©',
            'household_sounds': 'Ø£ØµÙˆØ§Øª Ù…Ù†Ø²Ù„ÙŠØ©',
            'street_sounds': 'Ø£ØµÙˆØ§Øª Ø´Ø§Ø±Ø¹',
            'animal_sounds': 'Ø£ØµÙˆØ§Øª Ø­ÙŠÙˆØ§Ù†Ø§Øª',
            'mechanical_sounds': 'Ø£ØµÙˆØ§Øª Ø¢Ù„ÙŠØ©'
        }
        return translations.get(category, category)
    
    def get_sound_statistics(self):
        """Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£ØµÙˆØ§Øª"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø¹Ø§Ù…Ø©
        cursor.execute('SELECT COUNT(*) FROM detected_sounds')
        total_sounds = cursor.fetchone()[0]
        
        # Ø£ÙƒØ«Ø± Ø§Ù„ÙØ¦Ø§Øª Ø´ÙŠÙˆØ¹Ø§Ù‹
        cursor.execute('''
            SELECT sound_category, COUNT(*) as count 
            FROM detected_sounds 
            GROUP BY sound_category 
            ORDER BY count DESC
        ''')
        category_stats = cursor.fetchall()
        
        # Ø£Ø­Ø¯Ø« Ø§Ù„Ø£ØµÙˆØ§Øª
        cursor.execute('''
            SELECT sound_name, confidence, timestamp 
            FROM detected_sounds 
            ORDER BY timestamp DESC 
            LIMIT 10
        ''')
        recent_sounds = cursor.fetchall()
        
        conn.close()
        
        return {
            'total_detected': total_sounds,
            'category_distribution': category_stats,
            'recent_detections': recent_sounds
        }

if __name__ == "__main__":
    classifier = EnvironmentalAudioClassifier()
    
    print("ğŸ”Š Ù†Ø¸Ø§Ù… ØªØµÙ†ÙŠÙ Ø§Ù„Ø£ØµÙˆØ§Øª Ø§Ù„Ø¨ÙŠØ¦ÙŠØ©")
    print("=" * 50)
    
    # Ø¹Ø±Ø¶ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
    stats = classifier.get_sound_statistics()
    print(f"ğŸ“Š Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª:")
    print(f"   - Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø£ØµÙˆØ§Øª Ø§Ù„Ù…ÙƒØªØ´ÙØ©: {stats['total_detected']}")
    
    if stats['category_distribution']:
        print("   - ØªÙˆØ²ÙŠØ¹ Ø§Ù„ÙØ¦Ø§Øª:")
        for category, count in stats['category_distribution'][:5]:
            translated = classifier._translate_category(category)
            print(f"     â€¢ {translated}: {count}")
