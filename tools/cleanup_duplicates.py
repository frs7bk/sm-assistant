
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ§¹ Ø£Ø¯Ø§Ø© Ø§Ù„ØªÙ†Ø¸ÙŠÙ ÙˆØ§Ù„ØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© Ù„Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒÙŠ
Advanced Cleanup and Organization Tool
"""

import os
import shutil
import hashlib
import json
import time
from pathlib import Path
from typing import Dict, List, Set, Tuple, Any, Optional
from dataclasses import dataclass, asdict
from datetime import datetime
import logging

@dataclass
class FileInfo:
    """Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù„Ù"""
    path: str
    size: int
    modified_time: float
    hash_md5: str
    extension: str
    is_duplicate: bool = False
    keep_file: bool = True
    reason_for_removal: str = ""

@dataclass
class CleanupReport:
    """ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªÙ†Ø¸ÙŠÙ"""
    total_files_scanned: int
    duplicates_found: int
    files_removed: int
    space_freed: int
    deprecated_files: int
    backup_created: bool
    cleanup_time: float
    errors: List[str]

class AdvancedProjectCleaner:
    """Ù…Ù†Ø¸Ù Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…"""
    
    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root)
        self.logger = self._setup_logging()
        
        # Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ
        self.cleanup_rules = {
            # Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø© Ø§Ù„ÙˆØ§Ø¶Ø­Ø©
            "obvious_duplicates": [
                r"assistant_updated.*\.py$",
                r"assistant.*_v\d+\.py$",
                r".*_copy\.py$",
                r".*_backup\.py$",
                r".*_old\.py$"
            ],
            
            # Ù…Ù„ÙØ§Øª Ù…Ø¤Ù‚ØªØ©
            "temp_files": [
                r".*\.tmp$",
                r".*\.temp$", 
                r".*~$",
                r"\.DS_Store$",
                r"Thumbs\.db$"
            ],
            
            # Ù…Ù„ÙØ§Øª Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø¤Ù‚ØªØ©
            "system_temp": [
                r"__pycache__",
                r"\.pyc$",
                r"\.pyo$",
                r"\.coverage$"
            ],
            
            # Ù…Ø¬Ù„Ø¯Ø§Øª Ù…Ø¤Ù‚ØªØ©
            "temp_directories": [
                "__pycache__",
                ".pytest_cache",
                ".coverage",
                "node_modules",
                ".git" # Ù„Ø§ Ù†Ø­Ø°ÙÙ‡ØŒ ÙÙ‚Ø· Ù†ØªØ¬Ø§Ù‡Ù„Ù‡
            ]
        }
        
        # Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ù‡Ù…Ø© Ø§Ù„ØªÙŠ ÙŠØ¬Ø¨ Ø¹Ø¯Ù… Ø­Ø°ÙÙ‡Ø§
        self.protected_files = {
            "main.py",
            "main_unified.py", 
            "requirements.txt",
            "pyproject.toml",
            ".replit",
            ".env",
            ".env.example",
            "README.md"
        }
        
        # Ù…Ø¬Ù„Ø¯Ø§Øª Ù…Ø­Ù…ÙŠØ©
        self.protected_dirs = {
            ".git",
            "data",
            "frontend",
            "api"
        }
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        self.stats = {
            "files_scanned": 0,
            "duplicates_found": 0,
            "files_removed": 0,
            "space_freed": 0,
            "errors": []
        }
    
    def _setup_logging(self) -> logging.Logger:
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„Ø³Ø¬Ù„Ø§Øª"""
        logger = logging.getLogger("project_cleaner")
        logger.setLevel(logging.INFO)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø³Ø¬Ù„Ø§Øª
        log_dir = Path("data/cleanup_logs")
        log_dir.mkdir(parents=True, exist_ok=True)
        
        # Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù…Ù„Ù
        file_handler = logging.FileHandler(
            log_dir / f"cleanup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log",
            encoding='utf-8'
        )
        file_handler.setLevel(logging.INFO)
        
        # Ù…Ø¹Ø§Ù„Ø¬ ÙˆØ­Ø¯Ø© Ø§Ù„ØªØ­ÙƒÙ…
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        
        # ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø³Ø¬Ù„Ø§Øª
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        logger.addHandler(file_handler)
        logger.addHandler(console_handler)
        
        return logger
    
    def analyze_project_structure(self) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø¨Ù†ÙŠØ© Ø§Ù„Ù…Ø´Ø±ÙˆØ¹"""
        print("ğŸ” ØªØ­Ù„ÙŠÙ„ Ø¨Ù†ÙŠØ© Ø§Ù„Ù…Ø´Ø±ÙˆØ¹...")
        
        analysis = {
            "total_files": 0,
            "total_size": 0,
            "file_types": {},
            "largest_files": [],
            "potential_duplicates": [],
            "deprecated_patterns": [],
            "directory_structure": {}
        }
        
        # Ù…Ø³Ø­ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª
        all_files = []
        for root, dirs, files in os.walk(self.project_root):
            # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ù…Ø­Ù…ÙŠØ©
            dirs[:] = [d for d in dirs if d not in self.protected_dirs]
            
            for file in files:
                file_path = Path(root) / file
                try:
                    file_stat = file_path.stat()
                    file_info = FileInfo(
                        path=str(file_path.relative_to(self.project_root)),
                        size=file_stat.st_size,
                        modified_time=file_stat.st_mtime,
                        hash_md5=self._get_file_hash(file_path),
                        extension=file_path.suffix.lower()
                    )
                    all_files.append(file_info)
                    
                    analysis["total_files"] += 1
                    analysis["total_size"] += file_info.size
                    
                    # ØªØµÙ†ÙŠÙ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ù„ÙØ§Øª
                    ext = file_info.extension
                    analysis["file_types"][ext] = analysis["file_types"].get(ext, 0) + 1
                    
                except (PermissionError, FileNotFoundError) as e:
                    self.logger.warning(f"Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ù„Ù: {file_path} - {e}")
                    self.stats["errors"].append(str(e))
        
        # Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙƒØ¨Ø± Ø§Ù„Ù…Ù„ÙØ§Øª
        all_files.sort(key=lambda x: x.size, reverse=True)
        analysis["largest_files"] = [
            {"path": f.path, "size": f.size, "size_mb": f.size / 1024 / 1024}
            for f in all_files[:10]
        ]
        
        # Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø© Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©
        analysis["potential_duplicates"] = self._find_potential_duplicates(all_files)
        
        # Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…ØªØ±ÙˆÙƒØ©
        analysis["deprecated_patterns"] = self._find_deprecated_patterns(all_files)
        
        self.logger.info(f"ØªÙ… ØªØ­Ù„ÙŠÙ„ {analysis['total_files']} Ù…Ù„Ù Ø¨Ø­Ø¬Ù… Ø¥Ø¬Ù…Ø§Ù„ÙŠ {analysis['total_size']/1024/1024:.2f} MB")
        
        return analysis
    
    def _get_file_hash(self, file_path: Path) -> str:
        """Ø­Ø³Ø§Ø¨ hash Ù„Ù„Ù…Ù„Ù"""
        try:
            if file_path.stat().st_size > 50 * 1024 * 1024:  # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø£ÙƒØ¨Ø± Ù…Ù† 50MB
                return "large_file"
            
            hash_md5 = hashlib.md5()
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except Exception:
            return "error_hash"
    
    def _find_potential_duplicates(self, files: List[FileInfo]) -> List[Dict[str, Any]]:
        """Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø© Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©"""
        hash_groups = {}
        for file_info in files:
            if file_info.hash_md5 not in ["large_file", "error_hash"]:
                if file_info.hash_md5 not in hash_groups:
                    hash_groups[file_info.hash_md5] = []
                hash_groups[file_info.hash_md5].append(file_info)
        
        duplicates = []
        for hash_key, file_group in hash_groups.items():
            if len(file_group) > 1:
                duplicates.append({
                    "hash": hash_key,
                    "count": len(file_group),
                    "files": [{"path": f.path, "size": f.size} for f in file_group],
                    "total_waste": sum(f.size for f in file_group[1:])  # ÙƒÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø¹Ø¯Ø§ Ø§Ù„Ø£ÙˆÙ„
                })
        
        return sorted(duplicates, key=lambda x: x["total_waste"], reverse=True)
    
    def _find_deprecated_patterns(self, files: List[FileInfo]) -> List[Dict[str, Any]]:
        """Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…ØªØ±ÙˆÙƒØ©"""
        import re
        
        deprecated = []
        
        for pattern_type, patterns in self.cleanup_rules.items():
            matching_files = []
            for file_info in files:
                for pattern in patterns:
                    if re.search(pattern, file_info.path):
                        matching_files.append(file_info)
                        break
            
            if matching_files:
                deprecated.append({
                    "pattern_type": pattern_type,
                    "count": len(matching_files),
                    "files": [f.path for f in matching_files],
                    "total_size": sum(f.size for f in matching_files)
                })
        
        return deprecated
    
    def create_backup(self) -> bool:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©"""
        try:
            backup_dir = Path("backup") / f"backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            backup_dir.mkdir(parents=True, exist_ok=True)
            
            print(f"ğŸ“¦ Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© ÙÙŠ: {backup_dir}")
            
            # Ù†Ø³Ø® Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ù‡Ù…Ø© ÙÙ‚Ø·
            important_files = [
                "main.py",
                "main_unified.py",
                "requirements.txt",
                "pyproject.toml",
                ".replit"
            ]
            
            for file_name in important_files:
                src_file = self.project_root / file_name
                if src_file.exists():
                    shutil.copy2(src_file, backup_dir / file_name)
            
            # Ù†Ø³Ø® Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø©
            important_dirs = ["config", "data/sessions"]
            for dir_name in important_dirs:
                src_dir = self.project_root / dir_name
                if src_dir.exists():
                    shutil.copytree(src_dir, backup_dir / dir_name, dirs_exist_ok=True)
            
            self.logger.info(f"ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©: {backup_dir}")
            return True
            
        except Exception as e:
            self.logger.error(f"ÙØ´Ù„ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©: {e}")
            self.stats["errors"].append(f"Backup failed: {e}")
            return False
    
    def clean_duplicates(self, dry_run: bool = True) -> CleanupReport:
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©"""
        start_time = time.time()
        
        print("ğŸ§¹ Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªÙ†Ø¸ÙŠÙ...")
        if dry_run:
            print("âš ï¸ ÙˆØ¶Ø¹ Ø§Ù„Ù…Ø­Ø§ÙƒØ§Ø© - Ù„Ù† ÙŠØªÙ… Ø­Ø°Ù Ø£ÙŠ Ù…Ù„ÙØ§Øª")
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹
        analysis = self.analyze_project_structure()
        
        files_to_remove = []
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©
        for duplicate_group in analysis["potential_duplicates"]:
            files_in_group = duplicate_group["files"]
            if len(files_in_group) > 1:
                # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ù„ÙØ§Øª Ø­Ø³Ø¨ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© (Ø§Ù„Ø£Ø­Ø¯Ø« ÙˆØ§Ù„Ø£Ù‚ØµØ± Ø§Ø³Ù…Ø§Ù‹ ÙŠÙØ­ÙØ¸)
                sorted_files = sorted(
                    files_in_group, 
                    key=lambda x: (len(x["path"]), -os.path.getmtime(self.project_root / x["path"]))
                )
                
                # Ø­ÙØ¸ Ø§Ù„Ø£ÙˆÙ„ØŒ Ø­Ø°Ù Ø§Ù„Ø¨Ø§Ù‚ÙŠ
                for file_info in sorted_files[1:]:
                    if not self._is_protected_file(file_info["path"]):
                        files_to_remove.append({
                            "path": file_info["path"],
                            "reason": "duplicate",
                            "size": file_info["size"]
                        })
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ØªØ±ÙˆÙƒØ©
        for deprecated_group in analysis["deprecated_patterns"]:
            if deprecated_group["pattern_type"] in ["obvious_duplicates", "temp_files"]:
                for file_path in deprecated_group["files"]:
                    if not self._is_protected_file(file_path):
                        files_to_remove.append({
                            "path": file_path,
                            "reason": deprecated_group["pattern_type"],
                            "size": next((f.size for f in analysis["largest_files"] if f["path"] == file_path), 0)
                        })
        
        # ØªÙ†ÙÙŠØ° Ø§Ù„Ø­Ø°Ù
        actually_removed = 0
        space_freed = 0
        
        if not dry_run:
            # Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ø£ÙˆÙ„Ø§Ù‹
            backup_created = self.create_backup()
        else:
            backup_created = False
        
        for file_info in files_to_remove:
            file_path = self.project_root / file_info["path"]
            
            try:
                if file_path.exists():
                    if not dry_run:
                        if file_path.is_file():
                            file_path.unlink()
                        elif file_path.is_dir():
                            shutil.rmtree(file_path)
                        
                        actually_removed += 1
                        space_freed += file_info["size"]
                        self.logger.info(f"ØªÙ… Ø­Ø°Ù: {file_info['path']} (Ø§Ù„Ø³Ø¨Ø¨: {file_info['reason']})")
                    else:
                        print(f"Ø³ÙŠØªÙ… Ø­Ø°Ù: {file_info['path']} (Ø§Ù„Ø³Ø¨Ø¨: {file_info['reason']})")
                        
            except Exception as e:
                error_msg = f"ÙØ´Ù„ Ø­Ø°Ù {file_info['path']}: {e}"
                self.logger.error(error_msg)
                self.stats["errors"].append(error_msg)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        cleanup_time = time.time() - start_time
        
        report = CleanupReport(
            total_files_scanned=analysis["total_files"],
            duplicates_found=len(analysis["potential_duplicates"]),
            files_removed=actually_removed,
            space_freed=space_freed,
            deprecated_files=len(files_to_remove),
            backup_created=backup_created,
            cleanup_time=cleanup_time,
            errors=self.stats["errors"]
        )
        
        # Ø¹Ø±Ø¶ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        self._display_cleanup_report(report, dry_run)
        
        # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        self._save_cleanup_report(report, analysis)
        
        return report
    
    def _is_protected_file(self, file_path: str) -> bool:
        """ÙØ­Øµ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…Ù„Ù Ù…Ø­Ù…ÙŠØ§Ù‹"""
        file_name = Path(file_path).name
        
        # Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­Ù…ÙŠØ© Ø¨Ø§Ù„Ø§Ø³Ù…
        if file_name in self.protected_files:
            return True
        
        # Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ù…Ø­Ù…ÙŠØ©
        path_parts = Path(file_path).parts
        if any(part in self.protected_dirs for part in path_parts):
            return True
        
        # Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø­Ø¯ÙŠØ«Ø© (Ø£Ù‚Ù„ Ù…Ù† ÙŠÙˆÙ…)
        try:
            full_path = self.project_root / file_path
            if full_path.exists():
                modified_time = full_path.stat().st_mtime
                if time.time() - modified_time < 86400:  # 24 Ø³Ø§Ø¹Ø©
                    return True
        except:
            pass
        
        return False
    
    def _display_cleanup_report(self, report: CleanupReport, dry_run: bool):
        """Ø¹Ø±Ø¶ ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªÙ†Ø¸ÙŠÙ"""
        print("\n" + "="*60)
        print("ğŸ¯ ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªÙ†Ø¸ÙŠÙ")
        print("="*60)
        print(f"Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙØ­ÙˆØµØ©: {report.total_files_scanned:,}")
        print(f"Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø© Ø§Ù„Ù…ÙƒØªØ´ÙØ©: {report.duplicates_found}")
        print(f"Ø§Ù„Ù…Ù„ÙØ§Øª {'Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© Ù„Ù„Ø­Ø°Ù' if dry_run else 'Ø§Ù„Ù…Ø­Ø°ÙˆÙØ©'}: {report.files_removed}")
        print(f"Ø§Ù„Ù…Ø³Ø§Ø­Ø© {'Ø§Ù„Ù…Ø­Ø±Ø±Ø© Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©' if dry_run else 'Ø§Ù„Ù…Ø­Ø±Ø±Ø©'}: {report.space_freed/1024/1024:.2f} MB")
        print(f"ÙˆÙ‚Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {report.cleanup_time:.2f} Ø«Ø§Ù†ÙŠØ©")
        
        if report.backup_created:
            print("âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©")
        
        if report.errors:
            print(f"\nâš ï¸ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ ({len(report.errors)}):")
            for error in report.errors[:5]:  # Ø¹Ø±Ø¶ Ø£ÙˆÙ„ 5 Ø£Ø®Ø·Ø§Ø¡
                print(f"   â€¢ {error}")
            if len(report.errors) > 5:
                print(f"   ... Ùˆ {len(report.errors) - 5} Ø£Ø®Ø·Ø§Ø¡ Ø£Ø®Ø±Ù‰")
        
        print("="*60)
    
    def _save_cleanup_report(self, report: CleanupReport, analysis: Dict[str, Any]):
        """Ø­ÙØ¸ ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªÙ†Ø¸ÙŠÙ"""
        try:
            reports_dir = Path("data/cleanup_reports")
            reports_dir.mkdir(parents=True, exist_ok=True)
            
            report_data = {
                "timestamp": time.time(),
                "date": datetime.now().isoformat(),
                "cleanup_report": asdict(report),
                "project_analysis": analysis
            }
            
            report_file = reports_dir / f"cleanup_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±: {report_file}")
            
        except Exception as e:
            self.logger.error(f"ÙØ´Ù„ Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±: {e}")
    
    def reorganize_structure(self):
        """Ø¥Ø¹Ø§Ø¯Ø© ØªÙ†Ø¸ÙŠÙ… Ø¨Ù†ÙŠØ© Ø§Ù„Ù…Ø´Ø±ÙˆØ¹"""
        print("ğŸ“ Ø¥Ø¹Ø§Ø¯Ø© ØªÙ†Ø¸ÙŠÙ… Ø¨Ù†ÙŠØ© Ø§Ù„Ù…Ø´Ø±ÙˆØ¹...")
        
        # Ø®Ø·Ø© Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªÙ†Ø¸ÙŠÙ…
        reorganization_plan = {
            "consolidate_core": [
                "core/assistant.py",  # Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
                "core/main_unified.py"  # Ø§Ù„Ù†Ù‚Ø·Ø© Ø§Ù„Ù…ÙˆØ­Ø¯Ø©
            ],
            "clean_old_versions": [
                "core/assistant_updated*.py",
                "core/assistant_*_features.py"
            ],
            "organize_ai_models": [
                "ai_models/**/*.py"
            ]
        }
        
        # ØªÙ†ÙÙŠØ° Ø§Ù„Ø®Ø·Ø© (ÙÙŠ Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„)
        self.logger.info("Ø®Ø·Ø© Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªÙ†Ø¸ÙŠÙ… Ù…Ø­ÙÙˆØ¸Ø© Ù„Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠ")

def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    print("ğŸ§¹ Ø£Ø¯Ø§Ø© Ø§Ù„ØªÙ†Ø¸ÙŠÙ ÙˆØ§Ù„ØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©")
    print("="*50)
    
    cleaner = AdvancedProjectCleaner()
    
    # ØªØ´ØºÙŠÙ„ Ù…Ø­Ø§ÙƒØ§Ø© Ø£ÙˆÙ„Ø§Ù‹
    print("\nğŸ“‹ ØªØ´ØºÙŠÙ„ ØªØ­Ù„ÙŠÙ„ Ø£ÙˆÙ„ÙŠ...")
    report = cleaner.clean_duplicates(dry_run=True)
    
    if report.files_removed > 0:
        response = input(f"\nâ“ Ù‡Ù„ ØªØ±ÙŠØ¯ Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø© ÙˆØ­Ø°Ù {report.files_removed} Ù…Ù„ÙØŸ (y/N): ")
        if response.lower() in ['y', 'yes', 'Ù†Ø¹Ù…']:
            print("\nğŸ—‘ï¸ ØªÙ†ÙÙŠØ° Ø§Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„ÙØ¹Ù„ÙŠ...")
            final_report = cleaner.clean_duplicates(dry_run=False)
            print("âœ… ØªÙ… Ø§Ù„ØªÙ†Ø¸ÙŠÙ Ø¨Ù†Ø¬Ø§Ø­!")
        else:
            print("âŒ ØªÙ… Ø¥Ù„ØºØ§Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªÙ†Ø¸ÙŠÙ")
    else:
        print("âœ¨ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ù…Ù†Ø¸Ù… Ø¨Ø§Ù„ÙØ¹Ù„!")

if __name__ == "__main__":
    main()
