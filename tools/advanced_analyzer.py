
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ø£Ø¯Ø§Ø© ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
ØªÙ‚ÙˆÙ… Ø¨ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ù„ÙƒÙˆØ¯ ÙˆØªÙ‚Ø¯ÙŠÙ… ØªÙ‚Ø§Ø±ÙŠØ± Ø§Ø­ØªØ±Ø§ÙÙŠØ©
"""

import os
import ast
import json
from pathlib import Path
from typing import Dict, List, Tuple, Set
from dataclasses import dataclass, asdict
from collections import defaultdict, Counter

@dataclass
class FileAnalysis:
    """ØªØ­Ù„ÙŠÙ„ Ù…Ù„Ù ÙˆØ§Ø­Ø¯"""
    path: str
    lines_of_code: int
    functions: List[str]
    classes: List[str]
    imports: List[str]
    complexity_score: float
    duplicates_detected: List[str]
    quality_score: float

@dataclass
class ProjectAnalysis:
    """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø§Ù„ÙƒØ§Ù…Ù„"""
    total_files: int
    total_lines: int
    programming_languages: Dict[str, int]
    duplicate_files: List[Tuple[str, str]]
    architectural_issues: List[str]
    recommendations: List[str]
    advanced_features: List[str]
    quality_metrics: Dict[str, float]

class AdvancedProjectAnalyzer:
    """Ù…Ø­Ù„Ù„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…"""
    
    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root)
        self.file_analyses = []
        self.duplicate_patterns = {}
        self.architecture_map = {}
        
    def analyze_project(self) -> ProjectAnalysis:
        """ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ù„Ù…Ø´Ø±ÙˆØ¹"""
        print("ğŸ” Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù„Ù„Ù…Ø´Ø±ÙˆØ¹...")
        
        # ØªØ­Ù„ÙŠÙ„ ÙƒÙ„ Ù…Ù„Ù
        self._analyze_all_files()
        
        # Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
        duplicates = self._detect_duplicates()
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„Ù…Ø¹Ù…Ø§Ø±ÙŠØ©
        architectural_issues = self._analyze_architecture()
        
        # Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
        advanced_features = self._detect_advanced_features()
        
        # Ø­Ø³Ø§Ø¨ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø¬ÙˆØ¯Ø©
        quality_metrics = self._calculate_quality_metrics()
        
        # ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„ØªÙˆØµÙŠØ§Øª
        recommendations = self._generate_recommendations()
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ©
        languages = self._analyze_languages()
        
        analysis = ProjectAnalysis(
            total_files=len(self.file_analyses),
            total_lines=sum(f.lines_of_code for f in self.file_analyses),
            programming_languages=languages,
            duplicate_files=duplicates,
            architectural_issues=architectural_issues,
            recommendations=recommendations,
            advanced_features=advanced_features,
            quality_metrics=quality_metrics
        )
        
        print("âœ… ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ù…Ù† Ø§Ù„ØªØ­Ù„ÙŠÙ„")
        return analysis
    
    def _analyze_all_files(self):
        """ØªØ­Ù„ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹"""
        python_files = list(self.project_root.rglob("*.py"))
        
        for file_path in python_files:
            if self._should_analyze_file(file_path):
                analysis = self._analyze_file(file_path)
                if analysis:
                    self.file_analyses.append(analysis)
    
    def _should_analyze_file(self, file_path: Path) -> bool:
        """ØªØ­Ø¯ÙŠØ¯ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† ÙŠØ¬Ø¨ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù„Ù"""
        # ØªØ¬Ø§Ù‡Ù„ Ù…Ù„ÙØ§Øª Ù…Ø¹ÙŠÙ†Ø©
        ignore_patterns = [
            '__pycache__',
            '.git',
            'venv',
            'env',
            '.pytest_cache'
        ]
        
        return not any(pattern in str(file_path) for pattern in ignore_patterns)
    
    def _analyze_file(self, file_path: Path) -> FileAnalysis:
        """ØªØ­Ù„ÙŠÙ„ Ù…Ù„Ù Python ÙˆØ§Ø­Ø¯"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ØªØ­Ù„ÙŠÙ„ AST
            tree = ast.parse(content)
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª
            functions = [node.name for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)]
            classes = [node.name for node in ast.walk(tree) if isinstance(node, ast.ClassDef)]
            imports = self._extract_imports(tree)
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³
            lines_of_code = len([line for line in content.split('\n') if line.strip() and not line.strip().startswith('#')])
            complexity_score = self._calculate_complexity(tree)
            quality_score = self._calculate_file_quality(content, tree)
            
            return FileAnalysis(
                path=str(file_path.relative_to(self.project_root)),
                lines_of_code=lines_of_code,
                functions=functions,
                classes=classes,
                imports=imports,
                complexity_score=complexity_score,
                duplicates_detected=[],
                quality_score=quality_score
            )
            
        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù„Ù {file_path}: {e}")
            return None
    
    def _extract_imports(self, tree) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯Ø§Øª Ù…Ù† AST"""
        imports = []
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                imports.extend([alias.name for alias in node.names])
            elif isinstance(node, ast.ImportFrom):
                module = node.module or ""
                imports.extend([f"{module}.{alias.name}" for alias in node.names])
        return imports
    
    def _calculate_complexity(self, tree) -> float:
        """Ø­Ø³Ø§Ø¨ ØªØ¹Ù‚Ø¯ Ø§Ù„ÙƒÙˆØ¯"""
        complexity = 0
        for node in ast.walk(tree):
            if isinstance(node, (ast.If, ast.For, ast.While, ast.Try)):
                complexity += 1
            elif isinstance(node, ast.FunctionDef):
                complexity += len(node.args.args)
        
        return complexity / max(1, len([n for n in ast.walk(tree) if isinstance(n, ast.FunctionDef)]))
    
    def _calculate_file_quality(self, content: str, tree) -> float:
        """Ø­Ø³Ø§Ø¨ Ø¬ÙˆØ¯Ø© Ø§Ù„Ù…Ù„Ù"""
        score = 100.0
        
        # Ø®ØµÙ… Ù†Ù‚Ø§Ø· Ù„Ù„Ù…Ø´Ø§ÙƒÙ„
        lines = content.split('\n')
        
        # Ø®Ø·ÙˆØ· Ø·ÙˆÙŠÙ„Ø© Ø¬Ø¯Ø§Ù‹
        long_lines = [line for line in lines if len(line) > 120]
        score -= len(long_lines) * 2
        
        # Ù†Ù‚Øµ Ø§Ù„ØªÙˆØ«ÙŠÙ‚
        functions = [n for n in ast.walk(tree) if isinstance(n, ast.FunctionDef)]
        documented_functions = 0
        for func in functions:
            if (func.body and isinstance(func.body[0], ast.Expr) 
                and isinstance(func.body[0].value, ast.Str)):
                documented_functions += 1
        
        if functions:
            doc_ratio = documented_functions / len(functions)
            score += doc_ratio * 20
        
        return max(0, min(100, score))
    
    def _detect_duplicates(self) -> List[Tuple[str, str]]:
        """Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©"""
        duplicates = []
        
        # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø­Ø³Ø¨ Ø§Ù„Ø§Ø³Ù…
        name_groups = defaultdict(list)
        for analysis in self.file_analyses:
            filename = Path(analysis.path).name
            name_groups[filename].append(analysis.path)
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
        for filename, paths in name_groups.items():
            if len(paths) > 1:
                for i, path1 in enumerate(paths):
                    for path2 in paths[i+1:]:
                        duplicates.append((path1, path2))
        
        return duplicates
    
    def _analyze_architecture(self) -> List[str]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„Ù…Ø¹Ù…Ø§Ø±ÙŠØ©"""
        issues = []
        
        # ÙØ­Øµ Ø§Ù„ØªÙ†Ø¸ÙŠÙ…
        root_python_files = [f for f in self.file_analyses if '/' not in f.path]
        if len(root_python_files) > 10:
            issues.append("Ø¹Ø¯Ø¯ ÙƒØ¨ÙŠØ± Ù…Ù† Ù…Ù„ÙØ§Øª Python ÙÙŠ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¬Ø°Ø±")
        
        # ÙØ­Øµ Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯Ø§Øª Ø§Ù„Ø¯Ø§Ø¦Ø±ÙŠØ©
        import_graph = self._build_import_graph()
        if self._has_circular_imports(import_graph):
            issues.append("Ø§ÙƒØªÙØ´ÙØª Ø§Ø³ØªÙŠØ±Ø§Ø¯Ø§Øª Ø¯Ø§Ø¦Ø±ÙŠØ©")
        
        # ÙØ­Øµ Ø§Ù„ØªØ³Ù…ÙŠØ© Ø§Ù„Ù…ØªØ³Ù‚Ø©
        naming_issues = self._check_naming_consistency()
        issues.extend(naming_issues)
        
        return issues
    
    def _build_import_graph(self) -> Dict[str, Set[str]]:
        """Ø¨Ù†Ø§Ø¡ Ø±Ø³Ù… Ø¨ÙŠØ§Ù†ÙŠ Ù„Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯Ø§Øª"""
        graph = defaultdict(set)
        
        for analysis in self.file_analyses:
            module_name = analysis.path.replace('/', '.').replace('.py', '')
            for imp in analysis.imports:
                if imp.startswith('.'):  # Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ù†Ø³Ø¨ÙŠ
                    continue
                graph[module_name].add(imp)
        
        return graph
    
    def _has_circular_imports(self, graph: Dict[str, Set[str]]) -> bool:
        """ÙØ­Øµ Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯Ø§Øª Ø§Ù„Ø¯Ø§Ø¦Ø±ÙŠØ©"""
        # Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ø¨Ø³ÙŠØ·Ø© Ù„Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø¯ÙˆØ±Ø§Øª
        visited = set()
        rec_stack = set()
        
        def dfs(node):
            if node in rec_stack:
                return True
            if node in visited:
                return False
            
            visited.add(node)
            rec_stack.add(node)
            
            for neighbor in graph.get(node, []):
                if dfs(neighbor):
                    return True
            
            rec_stack.remove(node)
            return False
        
        return any(dfs(node) for node in graph if node not in visited)
    
    def _check_naming_consistency(self) -> List[str]:
        """ÙØ­Øµ Ø§ØªØ³Ø§Ù‚ Ø§Ù„ØªØ³Ù…ÙŠØ©"""
        issues = []
        
        # ÙØ­Øµ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…Ù„ÙØ§Øª
        file_patterns = Counter()
        for analysis in self.file_analyses:
            filename = Path(analysis.path).name
            if '_' in filename:
                file_patterns['snake_case'] += 1
            elif any(c.isupper() for c in filename):
                file_patterns['camelCase'] += 1
            else:
                file_patterns['lowercase'] += 1
        
        if len(file_patterns) > 1:
            issues.append("Ø¹Ø¯Ù… Ø§ØªØ³Ø§Ù‚ ÙÙŠ ØªØ³Ù…ÙŠØ© Ø§Ù„Ù…Ù„ÙØ§Øª")
        
        return issues
    
    def _detect_advanced_features(self) -> List[str]:
        """Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©"""
        features = []
        
        # ÙØ­Øµ Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
        ai_libraries = ['transformers', 'torch', 'tensorflow', 'openai', 'deepface']
        found_ai = any(
            any(lib in imp for imp in analysis.imports)
            for analysis in self.file_analyses
            for lib in ai_libraries
        )
        if found_ai:
            features.append("Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©")
        
        # ÙØ­Øµ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ
        ml_libraries = ['sklearn', 'pandas', 'numpy', 'scipy']
        found_ml = any(
            any(lib in imp for imp in analysis.imports)
            for analysis in self.file_analyses
            for lib in ml_libraries
        )
        if found_ml:
            features.append("Ø£Ø¯ÙˆØ§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
        
        # ÙØ­Øµ Ø§Ù„ÙˆØ§Ø¬Ù‡Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
        ui_libraries = ['dash', 'streamlit', 'flask', 'fastapi']
        found_ui = any(
            any(lib in imp for imp in analysis.imports)
            for analysis in self.file_analyses
            for lib in ui_libraries
        )
        if found_ui:
            features.append("ÙˆØ§Ø¬Ù‡Ø§Øª Ù…Ø³ØªØ®Ø¯Ù… ØªÙØ§Ø¹Ù„ÙŠØ©")
        
        # ÙØ­Øµ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ± ÙˆØ§Ù„ÙÙŠØ¯ÙŠÙˆ
        vision_libraries = ['cv2', 'PIL', 'skimage']
        found_vision = any(
            any(lib in imp for imp in analysis.imports)
            for analysis in self.file_analyses
            for lib in vision_libraries
        )
        if found_vision:
            features.append("Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ± ÙˆØ§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©")
        
        # ÙØ­Øµ Ø§Ù„Ø¨Ø±Ù…Ø¬Ø© ØºÙŠØ± Ø§Ù„Ù…ØªØ²Ø§Ù…Ù†Ø©
        found_async = any(
            'async' in analysis.path or 'await' in analysis.path
            for analysis in self.file_analyses
        )
        if found_async:
            features.append("Ø§Ù„Ø¨Ø±Ù…Ø¬Ø© ØºÙŠØ± Ø§Ù„Ù…ØªØ²Ø§Ù…Ù†Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©")
        
        return features
    
    def _calculate_quality_metrics(self) -> Dict[str, float]:
        """Ø­Ø³Ø§Ø¨ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø¬ÙˆØ¯Ø©"""
        if not self.file_analyses:
            return {}
        
        quality_scores = [f.quality_score for f in self.file_analyses]
        complexity_scores = [f.complexity_score for f in self.file_analyses]
        
        return {
            "Ù…ØªÙˆØ³Ø·_Ø¬ÙˆØ¯Ø©_Ø§Ù„ÙƒÙˆØ¯": sum(quality_scores) / len(quality_scores),
            "Ù…ØªÙˆØ³Ø·_ØªØ¹Ù‚Ø¯_Ø§Ù„ÙƒÙˆØ¯": sum(complexity_scores) / len(complexity_scores),
            "Ù…Ø¹Ø¯Ù„_Ø§Ù„ØªÙˆØ«ÙŠÙ‚": self._calculate_documentation_rate(),
            "ØªØºØ·ÙŠØ©_Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª": self._estimate_test_coverage(),
        }
    
    def _calculate_documentation_rate(self) -> float:
        """Ø­Ø³Ø§Ø¨ Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªÙˆØ«ÙŠÙ‚"""
        total_functions = sum(len(f.functions) for f in self.file_analyses)
        if total_functions == 0:
            return 0.0
        
        # ØªÙ‚Ø¯ÙŠØ± Ø¨Ø³ÙŠØ· - ÙŠØ­ØªØ§Ø¬ ØªØ­Ø³ÙŠÙ†
        documented_estimate = total_functions * 0.3  # Ø§ÙØªØ±Ø§Ø¶ 30%
        return (documented_estimate / total_functions) * 100
    
    def _estimate_test_coverage(self) -> float:
        """ØªÙ‚Ø¯ÙŠØ± ØªØºØ·ÙŠØ© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª"""
        test_files = [f for f in self.file_analyses if 'test' in f.path.lower()]
        if not test_files:
            return 0.0
        
        # ØªÙ‚Ø¯ÙŠØ± Ø¨Ø³ÙŠØ·
        return min(len(test_files) / len(self.file_analyses) * 100, 100)
    
    def _analyze_languages(self) -> Dict[str, int]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©"""
        languages = defaultdict(int)
        
        for file_path in self.project_root.rglob("*"):
            if file_path.is_file():
                suffix = file_path.suffix.lower()
                if suffix == '.py':
                    languages['Python'] += 1
                elif suffix == '.js':
                    languages['JavaScript'] += 1
                elif suffix == '.json':
                    languages['JSON'] += 1
                elif suffix == '.md':
                    languages['Markdown'] += 1
                elif suffix in ['.yml', '.yaml']:
                    languages['YAML'] += 1
                elif suffix == '.toml':
                    languages['TOML'] += 1
        
        return dict(languages)
    
    def _generate_recommendations(self) -> List[str]:
        """ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙˆØµÙŠØ§Øª Ù„Ù„ØªØ­Ø³ÙŠÙ†"""
        recommendations = []
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
        if len(self._detect_duplicates()) > 5:
            recommendations.append("Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø© ÙˆØªÙˆØ­ÙŠØ¯ Ø§Ù„ÙƒÙˆØ¯")
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨Ù†ÙŠØ©
        root_files = [f for f in self.file_analyses if '/' not in f.path]
        if len(root_files) > 8:
            recommendations.append("Ø¥Ø¹Ø§Ø¯Ø© ØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯Ø§Øª Ù…Ù†Ø§Ø³Ø¨Ø©")
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¬ÙˆØ¯Ø©
        avg_quality = sum(f.quality_score for f in self.file_analyses) / len(self.file_analyses)
        if avg_quality < 70:
            recommendations.append("ØªØ­Ø³ÙŠÙ† Ø¬ÙˆØ¯Ø© Ø§Ù„ÙƒÙˆØ¯ ÙˆØ¥Ø¶Ø§ÙØ© Ø§Ù„ØªÙˆØ«ÙŠÙ‚")
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ¹Ù‚Ø¯
        high_complexity_files = [f for f in self.file_analyses if f.complexity_score > 10]
        if high_complexity_files:
            recommendations.append("ØªØ¨Ø³ÙŠØ· Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø© ÙˆØªÙ‚Ø³ÙŠÙ…Ù‡Ø§ Ù„ÙˆØ­Ø¯Ø§Øª Ø£ØµØºØ±")
        
        # Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª
        test_coverage = self._estimate_test_coverage()
        if test_coverage < 30:
            recommendations.append("Ø¥Ø¶Ø§ÙØ© Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø´Ø§Ù…Ù„Ø© Ù„Ù„ÙƒÙˆØ¯")
        
        # ØªÙˆØ«ÙŠÙ‚
        doc_rate = self._calculate_documentation_rate()
        if doc_rate < 50:
            recommendations.append("ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªÙˆØ«ÙŠÙ‚ ÙˆØ¥Ø¶Ø§ÙØ© Ø´Ø±ÙˆØ­Ø§Øª Ù„Ù„Ø¯ÙˆØ§Ù„")
        
        return recommendations
    
    def generate_report(self, analysis: ProjectAnalysis) -> str:
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ù…ÙØµÙ„"""
        report = f"""
# ğŸ“Š ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù„Ù„Ù…Ø´Ø±ÙˆØ¹

## ğŸ“ˆ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø¹Ø§Ù…Ø©
- Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª: {analysis.total_files}
- Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø£Ø³Ø·Ø±: {analysis.total_lines:,}
- Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©: {', '.join(analysis.programming_languages.keys())}

## ğŸ¯ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© Ø§Ù„Ù…ÙƒØªØ´ÙØ©
"""
        for feature in analysis.advanced_features:
            report += f"âœ… {feature}\n"
        
        report += f"""
## ğŸ“Š Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø¬ÙˆØ¯Ø©
"""
        for metric, value in analysis.quality_metrics.items():
            report += f"- {metric}: {value:.1f}%\n"
        
        report += f"""
## âš ï¸ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ù…ÙƒØªØ´ÙØ©
"""
        for issue in analysis.architectural_issues:
            report += f"âŒ {issue}\n"
        
        if analysis.duplicate_files:
            report += f"\n### ğŸ”„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø© ({len(analysis.duplicate_files)})\n"
            for dup1, dup2 in analysis.duplicate_files[:5]:  # Ø£ÙˆÙ„ 5 ÙÙ‚Ø·
                report += f"- {dup1} â†”ï¸ {dup2}\n"
        
        report += f"""
## ğŸ’¡ Ø§Ù„ØªÙˆØµÙŠØ§Øª Ù„Ù„ØªØ­Ø³ÙŠÙ†
"""
        for recommendation in analysis.recommendations:
            report += f"ğŸ“ {recommendation}\n"
        
        return report
    
    def save_analysis(self, analysis: ProjectAnalysis, filename: str = "project_analysis.json"):
        """Ø­ÙØ¸ Ø§Ù„ØªØ­Ù„ÙŠÙ„ ÙÙŠ Ù…Ù„Ù JSON"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(asdict(analysis), f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªØ­Ù„ÙŠÙ„ ÙÙŠ: {filename}")

def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    analyzer = AdvancedProjectAnalyzer()
    analysis = analyzer.analyze_project()
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
    report = analyzer.generate_report(analysis)
    print(report)
    
    # Ø­ÙØ¸ Ø§Ù„ØªØ­Ù„ÙŠÙ„
    analyzer.save_analysis(analysis)
    
    # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
    with open("project_analysis_report.md", 'w', encoding='utf-8') as f:
        f.write(report)
    print("ğŸ“„ ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ± ÙÙŠ: project_analysis_report.md")

if __name__ == "__main__":
    main()
