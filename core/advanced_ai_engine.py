
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ù…Ø­Ø±Ùƒ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…ØªØ·ÙˆØ±
ÙŠØ¯Ù…Ø¬ Ø£Ø­Ø¯Ø« ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆØ§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚
"""

import asyncio
import logging
import json
import time
import numpy as np
import pickle
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple, Union
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
import threading
import queue
import sys

# Ø¥Ø¶Ø§ÙØ© Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ø´Ø±ÙˆØ¹
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© (Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡)
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from transformers import (
        AutoTokenizer, AutoModel, AutoModelForSequenceClassification,
        pipeline, GPT2LMHeadModel, GPT2Tokenizer
    )
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False

try:
    import cv2
    import face_recognition
    CV2_AVAILABLE = True
except ImportError:
    CV2_AVAILABLE = False

try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

try:
    from config.advanced_config import get_config
    CONFIG_AVAILABLE = True
except ImportError:
    CONFIG_AVAILABLE = False

@dataclass
class AIResponse:
    """Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ù…Ø­Ø±Ùƒ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"""
    text: str
    confidence: float
    context: Dict[str, Any]
    emotions: Dict[str, float]
    entities: List[Dict[str, Any]]
    intent: str
    suggestions: List[str]
    metadata: Dict[str, Any]
    processing_time: float = 0.0
    model_used: str = "unknown"

@dataclass
class UserProfile:
    """Ù…Ù„Ù Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø´Ø®ØµÙŠ"""
    user_id: str
    preferences: Dict[str, Any]
    interaction_history: List[Dict[str, Any]]
    emotional_state: Dict[str, float]
    learning_progress: Dict[str, Any]
    goals: List[str]
    last_updated: datetime
    
    def __post_init__(self):
        if isinstance(self.last_updated, str):
            self.last_updated = datetime.fromisoformat(self.last_updated)

class NeuralMemoryNetwork(nn.Module):
    """Ø´Ø¨ÙƒØ© Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©"""
    
    def __init__(self, input_size: int = 768, hidden_size: int = 512, memory_size: int = 1000):
        super().__init__()
        
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.memory_size = memory_size
        
        # Ø·Ø¨Ù‚Ø§Øª Ø§Ù„ØªØ´ÙÙŠØ±
        self.encoder = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
        # Ø¢Ù„ÙŠØ© Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡
        self.attention = nn.MultiheadAttention(
            hidden_size, num_heads=8, dropout=0.1, batch_first=True
        )
        
        # Ø°Ø§ÙƒØ±Ø© Ø·ÙˆÙŠÙ„Ø© Ø§Ù„Ù…Ø¯Ù‰
        self.memory_bank = nn.Parameter(
            torch.randn(memory_size, hidden_size), requires_grad=True
        )
        
        # Ø·Ø¨Ù‚Ø© Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬
        self.decoder = nn.Sequential(
            nn.Linear(hidden_size * 2, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_size, input_size)
        )
        
        # Ù…Ø¤Ø´Ø± Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        self.memory_pointer = 0
    
    def forward(self, x: torch.Tensor, update_memory: bool = True) -> Tuple[torch.Tensor, torch.Tensor]:
        """Ø§Ù„Ù…Ø±ÙˆØ± Ø§Ù„Ø£Ù…Ø§Ù…ÙŠ Ø¹Ø¨Ø± Ø§Ù„Ø´Ø¨ÙƒØ©"""
        batch_size = x.size(0)
        
        # ØªØ´ÙÙŠØ± Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„
        encoded = self.encoder(x)
        
        # Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ Ù…Ø¹ Ø¨Ù†Ùƒ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        memory_expanded = self.memory_bank.unsqueeze(0).expand(batch_size, -1, -1)
        attended, attention_weights = self.attention(
            encoded.unsqueeze(1), memory_expanded, memory_expanded
        )
        attended = attended.squeeze(1)
        
        # Ø¯Ù…Ø¬ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª
        combined = torch.cat([encoded, attended], dim=-1)
        output = self.decoder(combined)
        
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        if update_memory and self.training:
            self._update_memory(encoded.detach())
        
        return output, attention_weights
    
    def _update_memory(self, new_memory: torch.Tensor):
        """ØªØ­Ø¯ÙŠØ« Ø¨Ù†Ùƒ Ø§Ù„Ø°Ø§ÙƒØ±Ø©"""
        with torch.no_grad():
            # Ø¥Ø¶Ø§ÙØ© Ø°ÙƒØ±ÙŠØ§Øª Ø¬Ø¯ÙŠØ¯Ø© Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø¯Ø§Ø¦Ø±ÙŠØ©
            for memory in new_memory:
                self.memory_bank[self.memory_pointer] = memory
                self.memory_pointer = (self.memory_pointer + 1) % self.memory_size

class AdvancedAIEngine:
    """Ù…Ø­Ø±Ùƒ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…ØªØ·ÙˆØ±"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
        # Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø­Ø±Ùƒ
        self.is_initialized = False
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # Ø§Ù„ØªÙƒÙˆÙŠÙ†
        self.config = get_config() if CONFIG_AVAILABLE else None
        
        # Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø­Ù…Ù„Ø©
        self.models = {}
        self.tokenizers = {}
        
        # Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ø¹ØµØ¨ÙŠØ©
        self.neural_memory = None
        
        # Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†
        self.user_profiles: Dict[str, UserProfile] = {}
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡
        self.performance_stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "average_response_time": 0.0,
            "cache_hits": 0,
            "model_switches": 0
        }
        
        # Ù…Ø®Ø²Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        self.memory_store = {}
        self.model_dir = Path("data/models")
        self.model_dir.mkdir(parents=True, exist_ok=True)
        
        # Ù‚Ø§Ø¦Ù…Ø© Ø§Ù†ØªØ¸Ø§Ø± Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        self.processing_queue = queue.PriorityQueue()
        self.background_workers = []
    
    async def initialize(self):
        """ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ø±Ùƒ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"""
        self.logger.info("ğŸ§  ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ø±Ùƒ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…ØªØ·ÙˆØ±...")
        
        try:
            # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
            await self._load_base_models()
            
            # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ø¹ØµØ¨ÙŠØ©
            self._initialize_neural_memory()
            
            # ØªØ­Ù…ÙŠÙ„ Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†
            await self._load_user_profiles()
            
            # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¹Ù…Ø§Ù„ Ø§Ù„Ø®Ù„ÙÙŠÙŠÙ†
            self._start_background_workers()
            
            self.is_initialized = True
            self.logger.info("âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ø±Ùƒ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¨Ù†Ø¬Ø§Ø­")
            
        except Exception as e:
            self.logger.error(f"âŒ ÙØ´Ù„ ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ø±Ùƒ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ: {e}")
            # ØªÙ‡ÙŠØ¦Ø© ÙÙŠ Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
            self.is_initialized = True
    
    async def _load_base_models(self):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©"""
        if not TRANSFORMERS_AVAILABLE:
            self.logger.warning("âš ï¸ Transformers ØºÙŠØ± Ù…ØªØ§Ø­ - ØªØ´ØºÙŠÙ„ ÙÙŠ Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ")
            return
        
        try:
            # Ù†Ù…ÙˆØ°Ø¬ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
            self.models['sentiment'] = pipeline(
                'sentiment-analysis',
                model='cardiffnlp/twitter-xlm-roberta-base-sentiment',
                device=0 if torch.cuda.is_available() else -1
            )
            self.logger.info("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±")
            
            # Ù†Ù…ÙˆØ°Ø¬ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª
            self.models['ner'] = pipeline(
                'ner',
                model='CAMeL-Lab/bert-base-arabic-camelbert-mix-ner',
                device=0 if torch.cuda.is_available() else -1,
                aggregation_strategy='simple'
            )
            self.logger.info("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª")
            
            # Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ†
            model_name = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'
            self.tokenizers['embedding'] = AutoTokenizer.from_pretrained(model_name)
            self.models['embedding'] = AutoModel.from_pretrained(model_name)
            self.models['embedding'].to(self.device)
            self.logger.info("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ†")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø¨Ø¹Ø¶ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬: {e}")
    
    def _initialize_neural_memory(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ø¹ØµØ¨ÙŠØ©"""
        try:
            if TRANSFORMERS_AVAILABLE:
                self.neural_memory = NeuralMemoryNetwork()
                self.neural_memory.to(self.device)
                
                # ØªØ­Ù…ÙŠÙ„ Ø­Ø§Ù„Ø© Ù…Ø­ÙÙˆØ¸Ø© Ø¥Ù† ÙˆØ¬Ø¯Øª
                memory_path = self.model_dir / "neural_memory.pth"
                if memory_path.exists():
                    self.neural_memory.load_state_dict(torch.load(memory_path, map_location=self.device))
                    self.logger.info("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø©")
                else:
                    self.logger.info("ğŸ§  ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø°Ø§ÙƒØ±Ø© Ø¹ØµØ¨ÙŠØ© Ø¬Ø¯ÙŠØ¯Ø©")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ø¹ØµØ¨ÙŠØ©: {e}")
    
    async def _load_user_profiles(self):
        """ØªØ­Ù…ÙŠÙ„ Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†"""
        try:
            profiles_path = self.model_dir / "user_profiles.json"
            
            if profiles_path.exists():
                with open(profiles_path, 'r', encoding='utf-8') as f:
                    profiles_data = json.load(f)
                
                for user_id, data in profiles_data.items():
                    self.user_profiles[user_id] = UserProfile(
                        user_id=user_id,
                        preferences=data.get('preferences', {}),
                        interaction_history=data.get('interaction_history', []),
                        emotional_state=data.get('emotional_state', {"neutral": 1.0}),
                        learning_progress=data.get('learning_progress', {}),
                        goals=data.get('goals', []),
                        last_updated=datetime.fromisoformat(data.get('last_updated', datetime.now().isoformat()))
                    )
                
                self.logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(self.user_profiles)} Ù…Ù„Ù Ù…Ø³ØªØ®Ø¯Ù…")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†: {e}")
    
    def _start_background_workers(self):
        """ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¹Ù…Ø§Ù„ Ø§Ù„Ø®Ù„ÙÙŠÙŠÙ†"""
        num_workers = 2
        
        for i in range(num_workers):
            worker = threading.Thread(
                target=self._background_worker,
                name=f"AIWorker-{i}",
                daemon=True
            )
            worker.start()
            self.background_workers.append(worker)
        
        self.logger.info(f"ğŸ”„ ØªÙ… ØªØ´ØºÙŠÙ„ {num_workers} Ø¹Ø§Ù…Ù„ Ø®Ù„ÙÙŠ")
    
    def _background_worker(self):
        """Ø§Ù„Ø¹Ø§Ù…Ù„ Ø§Ù„Ø®Ù„ÙÙŠ Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©"""
        while True:
            try:
                priority, task = self.processing_queue.get(timeout=1)
                
                # ØªÙ†ÙÙŠØ° Ø§Ù„Ù…Ù‡Ù…Ø©
                task_func, args, kwargs = task
                task_func(*args, **kwargs)
                
                self.processing_queue.task_done()
                
            except queue.Empty:
                continue
            except Exception as e:
                self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¹Ø§Ù…Ù„ Ø§Ù„Ø®Ù„ÙÙŠ: {e}")
    
    async def process_natural_language(
        self, 
        text: str, 
        user_id: str = "default",
        context: Optional[Dict[str, Any]] = None
    ) -> AIResponse:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©"""
        
        start_time = time.time()
        
        try:
            self.performance_stats["total_requests"] += 1
            
            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ
            cleaned_text = self._preprocess_text(text)
            
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ù„Ù Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
            user_profile = self._get_or_create_user_profile(user_id)
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
            emotions = await self._analyze_emotions(cleaned_text)
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª
            entities = await self._extract_entities(cleaned_text)
            
            # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù‚ØµØ¯
            intent = await self._classify_intent(cleaned_text, context)
            
            # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©
            response_text = await self._generate_response(
                cleaned_text, intent, emotions, entities, user_profile, context
            )
            
            # Ø­Ø³Ø§Ø¨ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø©
            confidence = self._calculate_confidence(intent, emotions, entities)
            
            # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª
            suggestions = await self._generate_suggestions(intent, context)
            
            # ØªØ­Ø¯ÙŠØ« Ù…Ù„Ù Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
            self._update_user_profile(user_profile, text, response_text, emotions, intent)
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©
            response = AIResponse(
                text=response_text,
                confidence=confidence,
                context=context or {},
                emotions=emotions,
                entities=entities,
                intent=intent,
                suggestions=suggestions,
                metadata={
                    "user_id": user_id,
                    "model_used": "advanced_ai_engine",
                    "processing_steps": ["emotion_analysis", "entity_extraction", "intent_classification", "response_generation"]
                },
                processing_time=time.time() - start_time,
                model_used="advanced_ai_engine"
            )
            
            self.performance_stats["successful_requests"] += 1
            self._update_performance_stats(response.processing_time)
            
            return response
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©: {e}")
            
            # Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©
            return AIResponse(
                text="Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.",
                confidence=0.0,
                context=context or {},
                emotions={"neutral": 1.0},
                entities=[],
                intent="error",
                suggestions=["Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©", "ØªØ¨Ø³ÙŠØ· Ø§Ù„Ø³Ø¤Ø§Ù„"],
                metadata={"error": str(e)},
                processing_time=time.time() - start_time,
                model_used="fallback"
            )
    
    def _preprocess_text(self, text: str) -> str:
        """ØªÙ†Ø¸ÙŠÙ ÙˆØªØ­Ø¶ÙŠØ± Ø§Ù„Ù†Øµ"""
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
        text = ' '.join(text.split())
        
        # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ (ÙŠÙ…ÙƒÙ† Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯)
        text = text.strip()
        
        return text
    
    async def _analyze_emotions(self, text: str) -> Dict[str, float]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ù…ØªÙ‚Ø¯Ù…"""
        try:
            if 'sentiment' in self.models and TRANSFORMERS_AVAILABLE:
                result = self.models['sentiment'](text)
                
                emotions = {"neutral": 0.5}
                
                if result:
                    label = result[0]['label'].lower()
                    score = float(result[0]['score'])
                    
                    if 'positive' in label or 'pos' in label:
                        emotions = {"happy": score, "neutral": 1-score}
                    elif 'negative' in label or 'neg' in label:
                        emotions = {"sad": score, "neutral": 1-score}
                    else:
                        emotions = {"neutral": score}
                
                return emotions
            else:
                return await self._basic_emotion_analysis(text)
                
        except Exception as e:
            self.logger.warning(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±: {e}")
            return {"neutral": 1.0}
    
    async def _basic_emotion_analysis(self, text: str) -> Dict[str, float]:
        """ØªØ­Ù„ÙŠÙ„ Ù…Ø´Ø§Ø¹Ø± Ø£Ø³Ø§Ø³ÙŠ"""
        positive_words = ["Ø³Ø¹ÙŠØ¯", "Ù…Ù…ØªØ§Ø²", "Ø±Ø§Ø¦Ø¹", "Ø¬ÙŠØ¯", "Ù…Ø°Ù‡Ù„", "Ø­Ø¨", "ÙØ±Ø­"]
        negative_words = ["Ø­Ø²ÙŠÙ†", "Ø³ÙŠØ¡", "ÙØ¸ÙŠØ¹", "Ù…Ø´ÙƒÙ„Ø©", "Ø®Ø·Ø£", "ØºØ¶Ø¨", "ÙƒØ±Ù‡"]
        
        text_lower = text.lower()
        
        positive_count = sum(1 for word in positive_words if word in text_lower)
        negative_count = sum(1 for word in negative_words if word in text_lower)
        
        total = positive_count + negative_count
        
        if total == 0:
            return {"neutral": 1.0}
        
        emotions = {
            "happy": positive_count / total if positive_count > negative_count else 0.0,
            "sad": negative_count / total if negative_count > positive_count else 0.0,
            "neutral": 1.0 - max(positive_count, negative_count) / total
        }
        
        return emotions
    
    async def _extract_entities(self, text: str) -> List[Dict[str, Any]]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…"""
        try:
            if 'ner' in self.models and TRANSFORMERS_AVAILABLE:
                entities = self.models['ner'](text)
                
                processed_entities = []
                for entity in entities:
                    processed_entities.append({
                        "text": entity['word'],
                        "label": entity['entity_group'],
                        "confidence": float(entity['score']),
                        "start": int(entity['start']),
                        "end": int(entity['end'])
                    })
                
                return processed_entities
            else:
                return await self._basic_entity_extraction(text)
                
        except Exception as e:
            self.logger.warning(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª: {e}")
            return []
    
    async def _basic_entity_extraction(self, text: str) -> List[Dict[str, Any]]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒÙŠØ§Ù†Ø§Øª Ø£Ø³Ø§Ø³ÙŠ"""
        entities = []
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£Ø±Ù‚Ø§Ù…
        import re
        numbers = re.findall(r'\d+', text)
        for num in numbers:
            entities.append({
                "text": num,
                "label": "NUMBER",
                "confidence": 0.8,
                "start": text.find(num),
                "end": text.find(num) + len(num)
            })
        
        return entities
    
    async def _classify_intent(self, text: str, context: Optional[Dict[str, Any]]) -> str:
        """ØªØµÙ†ÙŠÙ Ø§Ù„Ù‚ØµØ¯ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…"""
        text_lower = text.lower()
        
        # Ù‚ØµÙˆØ¯ Ø£Ø³Ø§Ø³ÙŠØ©
        if any(word in text_lower for word in ["Ù…Ø±Ø­Ø¨Ø§", "Ø£Ù‡Ù„Ø§", "Ø³Ù„Ø§Ù…", "ØµØ¨Ø§Ø­", "Ù…Ø³Ø§Ø¡"]):
            return "greeting"
        elif any(word in text_lower for word in ["Ø´ÙƒØ±Ø§", "Ù…ØªØ´ÙƒØ±", "Ø£Ø´ÙƒØ±Ùƒ"]):
            return "thanks"
        elif any(word in text_lower for word in ["ÙˆØ¯Ø§Ø¹Ø§", "Ù…Ø¹ Ø§Ù„Ø³Ù„Ø§Ù…Ø©", "Ø¥Ù„Ù‰ Ø§Ù„Ù„Ù‚Ø§Ø¡"]):
            return "goodbye"
        elif "ØŸ" in text or any(word in text_lower for word in ["ÙƒÙŠÙ", "Ù…Ø§Ø°Ø§", "Ù…ØªÙ‰", "Ø£ÙŠÙ†", "Ù„Ù…Ø§Ø°Ø§", "Ù…Ù†"]):
            return "question"
        elif any(word in text_lower for word in ["Ø³Ø§Ø¹Ø¯Ù†ÙŠ", "Ø£Ø±ÙŠØ¯", "Ø£Ø­ØªØ§Ø¬", "ÙŠÙ…ÙƒÙ†Ùƒ"]):
            return "request"
        elif any(word in text_lower for word in ["Ù„Ø§", "ØªÙˆÙ‚Ù", "ÙƒÙÙ‰", "Ø¥ÙŠÙ‚Ø§Ù"]):
            return "stop"
        else:
            return "general"
    
    async def _generate_response(
        self,
        text: str,
        intent: str,
        emotions: Dict[str, float],
        entities: List[Dict[str, Any]],
        user_profile: UserProfile,
        context: Optional[Dict[str, Any]]
    ) -> str:
        """ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…"""
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… GPT Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªØ§Ø­Ø§Ù‹
        if OPENAI_AVAILABLE and self.config and self.config.ai_models.openai_api_key:
            try:
                return await self._generate_with_gpt(text, intent, emotions, user_profile, context)
            except Exception as e:
                self.logger.warning(f"Ø®Ø·Ø£ ÙÙŠ GPT: {e}")
        
        # Ø§Ø³ØªØ¬Ø§Ø¨Ø§Øª Ù…Ø­Ù„ÙŠØ© Ù…ØªÙ‚Ø¯Ù…Ø©
        return await self._generate_local_response(text, intent, emotions, entities, user_profile, context)
    
    async def _generate_with_gpt(
        self,
        text: str,
        intent: str,
        emotions: Dict[str, float],
        user_profile: UserProfile,
        context: Optional[Dict[str, Any]]
    ) -> str:
        """ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… GPT"""
        try:
            openai.api_key = self.config.ai_models.openai_api_key
            
            # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
            system_message = self._build_system_message(user_profile, emotions, context)
            
            response = openai.ChatCompletion.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": system_message},
                    {"role": "user", "content": text}
                ],
                temperature=0.7,
                max_tokens=1024
            )
            
            return response['choices'][0]['message']['content']
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ GPT: {e}")
            raise
    
    def _build_system_message(
        self,
        user_profile: UserProfile,
        emotions: Dict[str, float],
        context: Optional[Dict[str, Any]]
    ) -> str:
        """Ø¨Ù†Ø§Ø¡ Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©"""
        
        base_message = "Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ù…ØªØ·ÙˆØ± ÙˆÙˆØ¯ÙˆØ¯. ØªØªØ­Ø¯Ø« Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØªÙÙ‡Ù… Ø§Ù„Ø³ÙŠØ§Ù‚ Ø¬ÙŠØ¯Ø§Ù‹."
        
        # Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
        if user_profile.preferences:
            base_message += f" ØªÙØ¶ÙŠÙ„Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {user_profile.preferences}"
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø¹Ø§Ø·ÙÙŠØ©
        dominant_emotion = max(emotions, key=emotions.get)
        if dominant_emotion != "neutral":
            base_message += f" Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙŠØ¨Ø¯Ùˆ {dominant_emotion}."
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø³ÙŠØ§Ù‚
        if context and "recent_topics" in context:
            base_message += f" Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø§Ù„Ø£Ø®ÙŠØ±Ø©: {context['recent_topics']}"
        
        return base_message
    
    async def _generate_local_response(
        self,
        text: str,
        intent: str,
        emotions: Dict[str, float],
        entities: List[Dict[str, Any]],
        user_profile: UserProfile,
        context: Optional[Dict[str, Any]]
    ) -> str:
        """ØªÙˆÙ„ÙŠØ¯ Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ù…Ø­Ù„ÙŠØ© Ù…ØªÙ‚Ø¯Ù…Ø©"""
        
        responses = {
            "greeting": [
                "Ù…Ø±Ø­Ø¨Ø§Ù‹! ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø§Ù„ÙŠÙˆÙ…ØŸ",
                "Ø£Ù‡Ù„Ø§Ù‹ ÙˆØ³Ù‡Ù„Ø§Ù‹! Ù…Ø§ Ø§Ù„Ø°ÙŠ ØªØ±ÙŠØ¯ Ø£Ù† Ù†ØªØ­Ø¯Ø« Ø¹Ù†Ù‡ØŸ",
                "Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…! Ø£Ù†Ø§ Ù‡Ù†Ø§ Ù„Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©."
            ],
            "thanks": [
                "Ø¹ÙÙˆØ§Ù‹! Ø£Ø³Ø¹Ø¯Ù†ÙŠ Ø£Ù† Ø£Ø³Ø§Ø¹Ø¯Ùƒ.",
                "Ù„Ø§ Ø´ÙƒØ± Ø¹Ù„Ù‰ ÙˆØ§Ø¬Ø¨! Ù‡Ù„ ØªØ­ØªØ§Ø¬ Ø£ÙŠ Ø´ÙŠØ¡ Ø¢Ø®Ø±ØŸ",
                "ÙƒÙ„ Ø§Ù„ØªÙ‚Ø¯ÙŠØ± Ù„Ùƒ! ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø®Ø¯Ù…ØªÙƒ Ø£ÙƒØ«Ø±ØŸ"
            ],
            "goodbye": [
                "ÙˆØ¯Ø§Ø¹Ø§Ù‹! Ø£ØªÙ…Ù†Ù‰ Ù„Ùƒ ÙŠÙˆÙ…Ø§Ù‹ Ø³Ø¹ÙŠØ¯Ø§Ù‹.",
                "Ø¥Ù„Ù‰ Ø§Ù„Ù„Ù‚Ø§Ø¡! Ø£Ø±Ø§Ùƒ Ù‚Ø±ÙŠØ¨Ø§Ù‹.",
                "Ù…Ø¹ Ø§Ù„Ø³Ù„Ø§Ù…Ø©! Ø¹ÙˆØ¯Ø© Ù…ÙŠÙ…ÙˆÙ†Ø©."
            ],
            "question": [
                "Ø³Ø¤Ø§Ù„ Ù…Ù…ØªØ§Ø²! Ø¯Ø¹Ù†ÙŠ Ø£ÙÙƒØ± ÙÙŠ Ø£ÙØ¶Ù„ Ø¥Ø¬Ø§Ø¨Ø©...",
                "Ù‡Ø°Ø§ Ø³Ø¤Ø§Ù„ Ù…Ù‡Ù…. Ø³Ø£Ø­Ø§ÙˆÙ„ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø¨Ø£ÙØ¶Ù„ Ø·Ø±ÙŠÙ‚Ø© Ù…Ù…ÙƒÙ†Ø©.",
                "Ø£Ù‚Ø¯Ø± ÙØ¶ÙˆÙ„Ùƒ! Ø¥Ù„ÙŠÙƒ Ù…Ø§ Ø£Ø¹Ø±ÙÙ‡..."
            ],
            "request": [
                "Ø¨Ø§Ù„Ø·Ø¨Ø¹! Ø³Ø£ÙØ¹Ù„ Ù…Ø§ Ø¨ÙˆØ³Ø¹ÙŠ Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ.",
                "Ø³Ø£ÙƒÙˆÙ† Ø³Ø¹ÙŠØ¯Ø§Ù‹ Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ÙÙŠ Ø°Ù„Ùƒ.",
                "Ø¯Ø¹Ù†ÙŠ Ø£Ø±Ù‰ ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ ØªØ­Ù‚ÙŠÙ‚ Ø·Ù„Ø¨Ùƒ."
            ]
        }
        
        # Ø§Ø®ØªÙŠØ§Ø± Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø£Ø³Ø§Ø³ÙŠØ©
        intent_responses = responses.get(intent, ["Ø£ÙÙ‡Ù… Ù…Ø§ ØªÙ‚ÙˆÙ„Ù‡. ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒØŸ"])
        
        # ØªØ®ØµÙŠØµ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø­Ø³Ø¨ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
        dominant_emotion = max(emotions, key=emotions.get)
        
        if dominant_emotion == "sad" and emotions[dominant_emotion] > 0.6:
            response = "Ø£Ø´Ø¹Ø± Ø£Ù†Ùƒ Ù‚Ø¯ ØªÙ…Ø± Ø¨ÙˆÙ‚Øª ØµØ¹Ø¨. " + intent_responses[0]
        elif dominant_emotion == "happy" and emotions[dominant_emotion] > 0.6:
            response = "ÙŠØ³Ø¹Ø¯Ù†ÙŠ Ø£Ù† Ø£Ø±Ø§Ùƒ ÙÙŠ Ù…Ø²Ø§Ø¬ Ø¬ÙŠØ¯! " + intent_responses[0]
        else:
            import random
            response = random.choice(intent_responses)
        
        # Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù† Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©
        if entities:
            entity_texts = [entity['text'] for entity in entities]
            response += f" Ù„Ø§Ø­Ø¸Øª Ø£Ù†Ùƒ Ø°ÙƒØ±Øª: {', '.join(entity_texts[:3])}."
        
        return response
    
    def _calculate_confidence(
        self,
        intent: str,
        emotions: Dict[str, float],
        entities: List[Dict[str, Any]]
    ) -> float:
        """Ø­Ø³Ø§Ø¨ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø©"""
        
        base_confidence = 0.5
        
        # Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø«Ù‚Ø© Ù„Ù„Ù‚ØµÙˆØ¯ Ø§Ù„ÙˆØ§Ø¶Ø­Ø©
        clear_intents = ["greeting", "thanks", "goodbye"]
        if intent in clear_intents:
            base_confidence += 0.3
        
        # Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø«Ù‚Ø© Ù„Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„ÙˆØ§Ø¶Ø­Ø©
        max_emotion_score = max(emotions.values()) if emotions else 0
        base_confidence += max_emotion_score * 0.2
        
        # Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø«Ù‚Ø© Ù„Ù„ÙƒÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©
        if entities:
            avg_entity_confidence = sum(e['confidence'] for e in entities) / len(entities)
            base_confidence += avg_entity_confidence * 0.1
        
        return min(base_confidence, 1.0)
    
    async def _generate_suggestions(
        self,
        intent: str,
        context: Optional[Dict[str, Any]]
    ) -> List[str]:
        """ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª"""
        
        suggestions_map = {
            "greeting": ["ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒØŸ", "Ù‡Ù„ ØªØ±ÙŠØ¯ Ø£Ù† Ù†ØªØ­Ø¯Ø« Ø¹Ù† Ø´ÙŠØ¡ Ù…Ø¹ÙŠÙ†ØŸ"],
            "thanks": ["Ù‡Ù„ ØªØ­ØªØ§Ø¬ Ù…Ø³Ø§Ø¹Ø¯Ø© Ø£Ø®Ø±Ù‰ØŸ", "Ù…Ø§ Ø±Ø£ÙŠÙƒ ÙÙŠ Ù…ÙˆØ¶ÙˆØ¹ Ø¬Ø¯ÙŠØ¯ØŸ"],
            "goodbye": ["Ù†Ø±Ø§Ùƒ Ù‚Ø±ÙŠØ¨Ø§Ù‹!", "Ø§Ø³ØªÙ…ØªØ¹ Ø¨Ø¨Ø§Ù‚ÙŠ ÙŠÙˆÙ…Ùƒ!"],
            "question": ["Ù‡Ù„ ØªØ±ÙŠØ¯ Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„ØŸ", "Ù‡Ù„ Ù„Ø¯ÙŠÙƒ Ø£Ø³Ø¦Ù„Ø© Ø£Ø®Ø±Ù‰ØŸ"],
            "request": ["Ù‡Ù„ Ù‡Ø°Ø§ Ù…Ø§ ÙƒÙ†Øª ØªØ¨Ø­Ø« Ø¹Ù†Ù‡ØŸ", "Ù‡Ù„ ØªØ­ØªØ§Ø¬ Ø´ÙŠØ¡ Ø¢Ø®Ø±ØŸ"]
        }
        
        base_suggestions = suggestions_map.get(intent, ["ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø£ÙƒØ«Ø±ØŸ"])
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚
        if context and "recent_topics" in context:
            base_suggestions.append("Ù‡Ù„ ØªØ±ÙŠØ¯ Ù…ÙˆØ§ØµÙ„Ø© Ù…ÙˆØ¶ÙˆØ¹ Ø³Ø§Ø¨Ù‚ØŸ")
        
        return base_suggestions[:3]  # Ø£Ù‚ØµÙ‰ 3 Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª
    
    def _get_or_create_user_profile(self, user_id: str) -> UserProfile:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ù„Ù Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø£Ùˆ Ø¥Ù†Ø´Ø§Ø¤Ù‡"""
        if user_id not in self.user_profiles:
            self.user_profiles[user_id] = UserProfile(
                user_id=user_id,
                preferences={},
                interaction_history=[],
                emotional_state={"neutral": 1.0},
                learning_progress={},
                goals=[],
                last_updated=datetime.now()
            )
        
        return self.user_profiles[user_id]
    
    def _update_user_profile(
        self,
        user_profile: UserProfile,
        input_text: str,
        response_text: str,
        emotions: Dict[str, float],
        intent: str
    ):
        """ØªØ­Ø¯ÙŠØ« Ù…Ù„Ù Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„ØªÙØ§Ø¹Ù„ Ù„Ù„ØªØ§Ø±ÙŠØ®
        interaction = {
            "timestamp": datetime.now().isoformat(),
            "input": input_text[:100],  # Ø£ÙˆÙ„ 100 Ø­Ø±Ù
            "response": response_text[:100],
            "emotions": emotions,
            "intent": intent
        }
        
        user_profile.interaction_history.append(interaction)
        
        # Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø¢Ø®Ø± 50 ØªÙØ§Ø¹Ù„ ÙÙ‚Ø·
        if len(user_profile.interaction_history) > 50:
            user_profile.interaction_history = user_profile.interaction_history[-50:]
        
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø¹Ø§Ø·ÙÙŠØ©
        for emotion, score in emotions.items():
            if emotion in user_profile.emotional_state:
                user_profile.emotional_state[emotion] = (
                    user_profile.emotional_state[emotion] * 0.8 + score * 0.2
                )
            else:
                user_profile.emotional_state[emotion] = score
        
        user_profile.last_updated = datetime.now()
    
    def _update_performance_stats(self, processing_time: float):
        """ØªØ­Ø¯ÙŠØ« Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        total = self.performance_stats["total_requests"]
        current_avg = self.performance_stats["average_response_time"]
        
        new_avg = (current_avg * (total - 1) + processing_time) / total
        self.performance_stats["average_response_time"] = new_avg
    
    async def analyze_image(self, image_path: str) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…ØªÙ‚Ø¯Ù…"""
        try:
            if not CV2_AVAILABLE:
                return {"error": "Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© ØºÙŠØ± Ù…ØªØ§Ø­Ø©"}
            
            import cv2
            
            image = cv2.imread(image_path)
            if image is None:
                return {"error": "ØªØ¹Ø°Ø± ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø©"}
            
            results = {
                "image_size": image.shape,
                "faces_detected": 0,
                "objects_detected": [],
                "colors_analysis": {},
                "brightness": 0.0
            }
            
            # ÙƒØ´Ù Ø§Ù„ÙˆØ¬ÙˆÙ‡
            try:
                if face_recognition:
                    face_locations = face_recognition.face_locations(image)
                    results["faces_detected"] = len(face_locations)
                else:
                    # Ø§Ø³ØªØ®Ø¯Ø§Ù… OpenCV Ù„Ù„ÙƒØ´Ù Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
                    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
                    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
                    faces = face_cascade.detectMultiScale(gray, 1.1, 4)
                    results["faces_detected"] = len(faces)
                    
            except Exception as e:
                self.logger.warning(f"Ø®Ø·Ø£ ÙÙŠ ÙƒØ´Ù Ø§Ù„ÙˆØ¬ÙˆÙ‡: {e}")
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù„ÙˆØ§Ù†
            try:
                # Ù…ØªÙˆØ³Ø· Ø§Ù„Ø£Ù„ÙˆØ§Ù†
                mean_color = np.mean(image, axis=(0, 1))
                results["colors_analysis"] = {
                    "dominant_blue": float(mean_color[0]),
                    "dominant_green": float(mean_color[1]),
                    "dominant_red": float(mean_color[2])
                }
                
                # Ø§Ù„Ø³Ø·ÙˆØ¹
                gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
                results["brightness"] = float(np.mean(gray))
                
            except Exception as e:
                self.logger.warning(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù„ÙˆØ§Ù†: {e}")
            
            return results
            
        except Exception as e:
            return {"error": str(e)}
    
    async def save_memory(self):
        """Ø­ÙØ¸ Ø§Ù„Ø°Ø§ÙƒØ±Ø© ÙˆØ§Ù„Ù†Ù…Ø§Ø°Ø¬"""
        try:
            # Ø­ÙØ¸ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ø¹ØµØ¨ÙŠØ©
            if self.neural_memory:
                memory_path = self.model_dir / "neural_memory.pth"
                torch.save(self.neural_memory.state_dict(), memory_path)
            
            # Ø­ÙØ¸ Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†
            profiles_data = {}
            for user_id, profile in self.user_profiles.items():
                profiles_data[user_id] = {
                    "preferences": profile.preferences,
                    "interaction_history": profile.interaction_history,
                    "emotional_state": profile.emotional_state,
                    "learning_progress": profile.learning_progress,
                    "goals": profile.goals,
                    "last_updated": profile.last_updated.isoformat()
                }
            
            profiles_path = self.model_dir / "user_profiles.json"
            with open(profiles_path, 'w', encoding='utf-8') as f:
                json.dump(profiles_data, f, ensure_ascii=False, indent=2)
            
            self.logger.info("âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ø°Ø§ÙƒØ±Ø© ÙˆØ§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø´Ø®ØµÙŠØ©")
            
        except Exception as e:
            self.logger.error(f"âŒ ÙØ´Ù„ Ø­ÙØ¸ Ø§Ù„Ø°Ø§ÙƒØ±Ø©: {e}")
    
    def get_performance_report(self) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        success_rate = (
            self.performance_stats["successful_requests"] / 
            max(self.performance_stats["total_requests"], 1)
        ) * 100
        
        return {
            "total_requests": self.performance_stats["total_requests"],
            "success_rate": f"{success_rate:.1f}%",
            "average_response_time": f"{self.performance_stats['average_response_time']:.3f}s",
            "models_loaded": len(self.models),
            "users_registered": len(self.user_profiles),
            "memory_initialized": self.neural_memory is not None,
            "device": str(self.device)
        }

# Ù…Ø«ÙŠÙ„ Ø¹Ø§Ù… Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
ai_engine = AdvancedAIEngine()

async def get_ai_engine() -> AdvancedAIEngine:
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø­Ø±Ùƒ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"""
    if not ai_engine.is_initialized:
        await ai_engine.initialize()
    return ai_engine

if __name__ == "__main__":
    async def main():
        """Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø­Ø±Ùƒ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"""
        print("ğŸ§  Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø­Ø±Ùƒ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…ØªØ·ÙˆØ±")
        print("=" * 50)
        
        engine = await get_ai_engine()
        
        # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        test_inputs = [
            "Ù…Ø±Ø­Ø¨Ø§Ù‹ØŒ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸ",
            "Ø£Ø´Ø¹Ø± Ø¨Ø§Ù„Ø­Ø²Ù† Ø§Ù„ÙŠÙˆÙ…",
            "Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø­Ù„ Ù…Ø´ÙƒÙ„Ø©ØŸ",
            "Ø´ÙƒØ±Ø§Ù‹ Ù„Ùƒ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©"
        ]
        
        for text in test_inputs:
            print(f"\nğŸ“ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„: {text}")
            response = await engine.process_natural_language(text)
            print(f"ğŸ¤– Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©: {response.text}")
            print(f"ğŸ“Š Ø§Ù„Ø«Ù‚Ø©: {response.confidence:.1%}")
            print(f"ğŸ­ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±: {response.emotions}")
            print(f"ğŸ¯ Ø§Ù„Ù‚ØµØ¯: {response.intent}")
        
        # Ø¹Ø±Ø¶ ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø£Ø¯Ø§Ø¡
        print(f"\nğŸ“ˆ ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø£Ø¯Ø§Ø¡:")
        report = engine.get_performance_report()
        for key, value in report.items():
            print(f"   â€¢ {key}: {value}")
    
    asyncio.run(main())
